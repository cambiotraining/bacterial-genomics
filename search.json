[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Working with Bacterial Genomes",
    "section": "",
    "text": "Overview\nThis comprehensive course equips you with essential skills and knowledge in bacterial genomics analysis, primarily using Illumina-sequenced samples. You’ll gain an understanding of how to select the most appropriate analysis workflow, tailored to the genome diversity of a given bacterial species. Through hands-on training, you’ll apply both de novo assembly and reference-based mapping approaches to obtain bacterial genomes for your isolates. You will apply standardised workflows for genome assembly and annotation, including quality assessment criteria to ensure the reliability of your results. Along with typing bacteria using methods such as MLST, you’ll learn how to construct phylogenetic trees using whole genome and core genome alignments, enabling you to explore the evolutionary relationships among bacterial isolates. You’ll extend this to estimate a time-scaled phylogeny using a starting phylogenetic tree. Lastly, you’ll apply methods to detect antimicrobial resistance genes. As examples we will use Mycobacterium tuberculosis, Staphylococcus aureus and Streptococcus pneumoniae, allowing you to become well-equipped to conduct bacterial genomics analyses on a range of species.",
    "crumbs": [
      "Slides",
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Working with Bacterial Genomes",
    "section": "",
    "text": "TipLearning Objectives\n\n\n\nBy the end of this course, you will be able to:\n\nChoose the most suitable analysis workflow based on the genome diversity of a given bacterial species.\nDifferentiate between “de novo assembly” and “reference-based mapping” approaches for reconstructing bacterial genomes.\nApply standardised workflows to assemble and annotate genomes using both approaches.\nEvaluate the quality of assembled genomes and determine their suitability for downstream analysis.\nDetect and remove recombinant regions.\nConstruct phylogenetic trees using both whole genome and core genome alignments.\nEstimate a time-scaled phylogeny using and initial maximum likelihood phylogenetic tree and sample dates.\nConduct genomic epidemiology and strain typing.\nDetect the presence of antimicrobial resistance genes in your isolates.\n\n\n\n\nTarget Audience\nThe course is aimed at biologists interested in microbiology, prokaryotic genomics and antimicrobial resistance.\n\n\nPrerequisites\n\nEssential\n\nBasic understanding of high-throughput sequencing technologies.\n\nWatch this iBiology video for an excellent overview.\n\nA working knowledge of the UNIX command line (course registration page).\n\nIf you are not able to attend this prerequisite course, please work through our Unix command line materials ahead of the course (up to section 7).\n\nA working knowledge of R (course registration page).\n\nIf you are not able to attend this prerequisite course, please work through our R materials ahead of the course.\n\n\n\n\nDesirable\n\nA basic knowledge of phylogenetics inference methods (course registration page).\nA working knowledge of running analysis on High Performance Computing (HPC) clusters (course registration page).",
    "crumbs": [
      "Slides",
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#citation-authors",
    "href": "index.html#citation-authors",
    "title": "Working with Bacterial Genomes",
    "section": "Citation & Authors",
    "text": "Citation & Authors\nPlease cite these materials if:\n\nYou adapted or used any of them in your own teaching.\nThese materials were useful for your research work. For example, you can cite us in the methods section of your paper: “We carried our analyses based on the recommendations in YourReferenceHere”.\n\n\nYou can cite these materials as:\n\nvan Tonder, A., Tavares, H., Salehe, B. (2024). Working with Bacterial Genomes. https://cambiotraining.github.io/bacterial-genomics/\n\nOr in BibTeX format:\n@misc{YourReferenceHere,\n  author = {van Tonder, Andries and Tavares, Hugo and Salehe, Bajuna},\n  month = {7},\n  title = {Working with Bacterial Genomes},\n  url = {https://cambiotraining.github.io/bacterial-genomics/},\n  year = {2024}\n}\nAbout the authors:\nAndries van Tonder  \nAffiliation: Department of Veterinary Medicine, University of Cambridge Roles: conceptualisation; primary author; data curation; coding; software\n\nHugo Tavares  \nAffiliation: Cambridge Centre for Research Informatics Training Roles: editor; software; data curation\n\nBajuna Salehe  \nAffiliation: Cambridge Centre for Research Informatics Training Roles: software",
    "crumbs": [
      "Slides",
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Working with Bacterial Genomes",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n\n\nList any other sources of materials that were used.\nOr other people that may have advised during the material development (but are not authors).",
    "crumbs": [
      "Slides",
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Data & Setup",
    "section": "",
    "text": "Software",
    "crumbs": [
      "Slides",
      "Welcome",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data & Setup</span>"
    ]
  },
  {
    "objectID": "setup.html#software",
    "href": "setup.html#software",
    "title": "Data & Setup",
    "section": "",
    "text": "Linux\nMost of the analyses demonstrated in these materials are more suited to be run on a High Performance Computing (HPC) cluster. If you already have access to a HPC in your institution, you can skip this step of the setup.\nOtherwise, we provide instructions to setup Linux on a local computer.\n\nUbuntuWindows WSLVirtual machine\n\n\nThe recommendation for bioinformatic analysis is to have a dedicated computer running a Linux distribution. The kind of distribution you choose is not critical, but we recommend Ubuntu if you are unsure.\nYou can follow the installation tutorial on the Ubuntu webpage.\n\n\n\n\n\n\nWarning\n\n\n\nInstalling Ubuntu on the computer will remove any other operating system you had previously installed, and can lead to data loss.\n\n\n\n\nThe Windows Subsystem for Linux (WSL2) runs a compiled version of Ubuntu natively on Windows.\nThere are detailed instructions on how to install WSL on the Microsoft documentation page. But briefly:\n\nClick the Windows key and search for Windows PowerShell, right-click on the app and choose Run as administrator.\nAnswer “Yes” when it asks if you want the App to make changes on your computer.\nA terminal will open; run the command: wsl --install.\n\nThis should start installing “ubuntu”.\nIt may ask for you to restart your computer.\n\nAfter restart, click the Windows key and search for Ubuntu, click on the App and it should open a new terminal.\nFollow the instructions to create a username and password (you can use the same username and password that you have on Windows, or a different one - it’s your choice).\nYou should now have access to a Ubuntu Linux terminal. This (mostly) behaves like a regular Ubuntu terminal, and you can install apps using the sudo apt install command as usual.\n\nAfter WSL is installed, it is useful to create shortcuts to your files on Windows. Your C:\\ drive is located in /mnt/c/ (equally, other drives will be available based on their letter). For example, your desktop will be located in: /mnt/c/Users/&lt;WINDOWS USERNAME&gt;/Desktop/. It may be convenient to set shortcuts to commonly-used directories, which you can do using symbolic links, for example:\n\nDocuments: ln -s /mnt/c/Users/&lt;WINDOWS USERNAME&gt;/Documents/ ~/Documents\n\nIf you use OneDrive to save your documents, use: ln -s /mnt/c/Users/&lt;WINDOWS USERNAME&gt;/OneDrive/Documents/ ~/Documents\n\nDesktop: ln -s /mnt/c/Users/&lt;WINDOWS USERNAME&gt;/Desktop/ ~/Desktop\nDownloads: ln -s /mnt/c/Users/&lt;WINDOWS USERNAME&gt;/Downloads/ ~/Downloads\n\n\n\nAnother way to run Linux within Windows (or macOS) is to install a Virtual Machine. However, this is mostly suitable for practicing and not suitable for real data analysis.\nDetailed instructions to install an Ubuntu VM using Oracle’s Virtual Box is available from the Ubuntu documentation page.\nNote: In the step configuring “Virtual Hard Disk” make sure to assign a large storage partition (at least 100GB).\n\n\n\n\nUpdate Ubuntu\nAfter installing Ubuntu (through either of the methods above), open a terminal and run the following commands to update your system and install some essential packages:\nsudo apt update && sudo apt upgrade -y && sudo apt autoremove -y\nsudo apt install -y git\nsudo apt install -y default-jre\n\n\n\nConda/Mamba\nWe recommend using the Conda package manager to install your software. In particular, the newest implementation called Mamba.\nTo install Mamba, run the following commands from the terminal:\nwget \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\nbash Miniforge3-$(uname)-$(uname -m).sh -b -p $HOME/miniforge3\nrm Miniforge3-$(uname)-$(uname -m).sh\n$HOME/miniforge3/bin/mamba init\nRestart your terminal (or open a new one) and confirm that your shell now starts with the word (base). Then run the following commands:\nconda config --add channels defaults\nconda config --add channels bioconda\nconda config --add channels conda-forge\nconda config --set remote_read_timeout_secs 1000\n\n\nSoftware environments\nDue to the complexities of the different tools we will use, there are several software dependency incompatibilities between them. Therefore, rather than creating a single software environment with all the tools, we will create separate environments for different applications.\n\nPandas\nFor convenience, we recommend installing the popular Pandas package in the base (default) environment:\nmamba install -n base pandas\n\n\nBakta\nmamba create -n bakta bakta\n\n\nKrona\nmamba create -n krona krona\n\n\nGubbins\nmamba create -n gubbins gubbins\n\n\nIQ-Tree\nmamba create -n iqtree iqtree snp-sites biopython\n\n\nmlst\nmamba create -n mlst mlst\n\n\nNextflow\nmamba create -n nextflow nextflow\nAlso run the following commands to set a basic Nextflow configuration file. Make sure to adjust the resource limits to fit with your workstation or HPC (maximum values for cpus, memory and time).\nmkdir -p $HOME/.nextflow\ncat &lt;&lt;EOF &gt;&gt; $HOME/.nextflow/config\nprocess {\n  resourceLimits = [\n    cpus: 8,\n    memory: 20.GB,\n    time: 12.h\n  ]\n}\nsingularity { \n  pullTimeout = '4 h' \n  cacheDir = '$HOME/.nextflow-singularity-cache/' \n}\nEOF\n\n\npairsnp\nmamba create -n pairsnp pairsnp\n\n\nPanaroo\nmamba create -n panaroo python=3.9 panaroo&gt;=1.3 snp-sites\n\n\nPopPUNK\nmamba create -n poppunk python=3.10 poppunk\n\n\nremove_blocks_from_aln\nmamba create -n remove_blocks python=2.7\n$HOME/miniforge3/envs/remove_blocks/bin/pip install git+https://github.com/sanger-pathogens/remove_blocks_from_aln.git\n\n\nSeqtk\nmamba create -n seqtk seqtk pandas\n\n\nTB-Profiler\nmamba create -n tb-profiler tb-profiler pandas\n\n\nTreeTime\nmamba create -n treetime treetime seqkit biopython\n\n\nMOB-suite & Pling & mashtree\nmamba create -n mob_suite mob_suite\nmamba create -n pling pling\nmamba create -n mashtree mashtree\n\n\nReverse vaccinology\nmamba create -n reverse-vaccinology bakta diamond cd-hit pandas\n\n\nPSORTb\nRunning PSORTb requires Apptainer and a wrapper script. The container is available from our Dropbox.\nwget -O psortb.sif \"https://www.dropbox.com/ #add link here\"\nwget https://raw.githubusercontent.com/brinkmanlab/psortb_commandline_docker/master/psortb_app\nchmod +x psortb_app\n\n\nAccessory genome vaccine workflow\nmicromamba create -n accessory-vaccinology -c bioconda -c conda-forge mash pyseer python=3.6 openssl=1.0\n\n\n\nR and RStudio\nR and RStudio are available for all major operating systems.\n\nWindows: download and install all these using default options:\n\nR\nRTools\nRStudio\n\nmacOS: download and install all these using default options:\n\nR\nRStudio\n\nLinux:\n\nGo to the R installation folder and look at the instructions for your distribution.\nDownload the RStudio installer for your distribution and install it using your package manager.\n\n\nAfter installing R, you will need to install a few packages. Open RStudio and on the console type the following commands:\ninstall.packages(\"BiocManager\")\nBiocManager::install(c(\"data.table\", \"ggraph\", \"igraph\", \n                       \"tidygraph\", \"tidyverse\", \"ape\", \n                       \"phytools\", \"ggnewscale\", \"ggtree\", \n                       \"janitor\"))\n\n\nSingularity\nWe recommend that you install Singularity and use the -profile singularity option when running Nextflow pipelines. On Ubuntu/WSL2, you can install Singularity using the following commands:\nsudo apt install -y libfuse2t64 runc fuse2fs uidmap\nwget -O singularity.deb https://github.com/sylabs/singularity/releases/download/v4.3.0/singularity-ce_4.3.0-$(lsb_release -cs)_amd64.deb\nsudo dpkg -i singularity.deb\nrm singularity.deb\nIf you have a different Linux distribution, you can find more detailed instructions on the Singularity documentation page.\nIf you have issues running Nextflow pipelines with Singularity, then you can follow the instructions below for Docker instead.\n\n\nDocker\nAn alternative for software management when running Nextflow pipelines is to use Docker.\n\nUbuntuWindows WSLVirtual machine\n\n\nFor Ubuntu Linux, here are the installation instructions:\nsudo apt install curl\ncurl -fsSL https://get.docker.com -o get-docker.sh\nsudo sh ./get-docker.sh\nsudo groupadd docker\nsudo usermod -aG docker $USER\nAfter the last step, you will need to restart your computer. From now on, you can use -profile docker when you run Nextflow.\n\n\nWhen using WSL2 on Windows, running Nextflow pipelines with -profile singularity sometimes doesn’t work.\nAs an alternative you can instead use Docker, which is another software containerisation solution. To set this up, you can follow the full instructions given on the Microsoft Documentation: Get started with Docker remote containers on WSL 2.\nWe briefly summarise the instructions here (but check that page for details and images):\n\nDownload Docker for Windows.\nRun the installer and install accepting default options.\nRestart the computer.\nOpen Docker and go to Settings &gt; General to tick “Use the WSL 2 based engine”.\nGo to Settings &gt; Resources &gt; WSL Integration to enable your Ubuntu WSL installation.\n\nOnce you have Docker set and installed, you can use -profile docker when running your Nextflow command.\n\n\nYou can follow the same instructions as for “Ubuntu”.",
    "crumbs": [
      "Slides",
      "Welcome",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data & Setup</span>"
    ]
  },
  {
    "objectID": "setup.html#data",
    "href": "setup.html#data",
    "title": "Data & Setup",
    "section": "Data",
    "text": "Data\nThe data used in these materials are provided as archive files:\n\nbact-data.tar contains the main course data.\nbact-outbreak.tar contains the data for the final capstone exercise.\nbact-databases.tar contains a copy of the databases used by some of the programs. Note: we do not recommend that you use this copy in your own work, always download the latest database versions following the instructions given below.\n\nYou can download these files from the link below and extract the files from the archive into a directory of your choice.\n  Download \nYou can also download them using the command line:\n# directory for saving the data - change this to suit your needs\ndatadir=\"$HOME/Desktop/bacterial_genomics\"\nmkdir $datadir\n\n# download and extract to directory\nwget -O $datadir/bact-data.tar \"https://www.dropbox.com/scl/fi/s88w1cdiqtygnepbff858/bact-data.tar?rlkey=xifz132zgjt7hj8oj38ef9o00&st=izvooc62&dl=1\"\ntar -xvf $datadir/bact-data.tar -C $datadir\nrm $datadir/bact-data.tar\n\nwget -O $datadir/bact-outbreak.tar \"https://www.dropbox.com/scl/fi/tio9qtcuwvv86nwfckezl/bact-outbreak.tar?rlkey=khcb6nsvj3mpsvfbsv97q467e&st=r9vj0dm9&dl=1\"\ntar -xvf $datadir/bact-outbreak.tar -C $datadir\nrm $datadir/bact-outbreak.tar\n\nwget -O $datadir/bact-databases.tar \"https://www.dropbox.com/scl/fi/ljwypmwetfu6o6pe3fwff/bact-databases.tar?rlkey=yyg3q7w0s47ildzad5sfftr1x&st=y381n5nj&dl=1\"\ntar -xvf $datadir/bact-databases.tar -C $datadir\nrm $datadir/bact-databases.tar\n\n\n\n\n\n\nTipNote for training facility\n\n\n\nWe also need to include preprocessed data for the outbreak exercise. See the download script in the repo for details.\n\n\n\nDatabases\nWe include a copy of public databases used in the exercises in the dropbox link above. However, for your analyses you should always download the most up-to-date databases.\nIn the code below we download these databases into a directory called databases. This is optional, you can download the databases where it is most convenient for you. If you work in a research group, it’s a good idea to have a shared storage where everyone can access the same copy of the databases.\n# create directory for public DBs\nmkdir databases\ncd databases\n\nKraken2\nWe use a small version of the database for teaching purposes, whereas you may want to use the full version in your work. Look at the Kraken2 indexes page for the latest versions available.\nwget https://genome-idx.s3.amazonaws.com/kraken/k2_standard_08gb_20240605.tar.gz\nmkdir k2_standard_08gb_20240605\ntar -xzvf k2_standard_08gb_20240605.tar.gz -C k2_standard_08gb_20240605\nrm k2_standard_08gb_20240605.tar.gz\n\n\nBakta\nWe use the “light” version of the database for teaching purposes, whereas you may want to use the full version in your work. Look at the Bakta Zenodo repository for the latest versions available.\nwget https://zenodo.org/records/10522951/files/db-light.tar.gz\ntar -xzvf db-light.tar.gz\nmv db-light  bakta_light_20240119\nrm db-light.tar.gz\n\n# make sure to activate bakta environment\nmamba activate bakta\namrfinder_update --force_update --database bakta_light_20240119/amrfinderplus-db/\n\n\nCheckM2\nCheckM2 also provides a command checkm2 database --download to download the latest version of the database from Zenodo.\nwget https://zenodo.org/records/5571251/files/checkm2_database.tar.gz\ntar -xzvf checkm2_database.tar.gz\nmv CheckM2_database checkm2_v2_20210323\nrm checkm2_database.tar.gz CONTENTS.json\n\n\nGPSCs\nwget https://gps-project.cog.sanger.ac.uk/GPS_v8_ref.tar.gz\nmkdir poppunk\ntar -xzvf GPS_v8_ref.tar.gz -C poppunk\nrm GPS_v8_ref.tar.gz\n\nwget -O poppunk/GPS_v8_external_clusters.csv https://gps-project.cog.sanger.ac.uk/GPS_v8_external_clusters.csv\n\n\nKrona\n# make sure to activate krona environment\nmamba activate krona\nktUpdateTaxonomy.sh krona/\n\n\nMOB-suite\nMOB-suite has a generic database available, which can be downloaded using:\nmamba activate mob_suite\nmob_init -d mob_suite -v\nThe MOB-suite developers also provide a collection of Enterobacteriacea genomes for organisms such as E. coli. These can be downloaded separately from Zenodo, like so:\nwget -O mobsuite.zip https://zenodo.org/api/records/3785351/files-archive\nunzip mobsuite.zip -d mob_suite\nrm mobsuite.zip\n\n\nSWISS-PROT and Human proteome\n# download Swiss-Prot and Human Proteome from UniProt\nwget \"https://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.fasta.gz\"\nwget \"https://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/reference_proteomes/Eukaryota/UP000005640/UP000005640_9606.fasta.gz\"\ngunzip *.gz\n# make sure to activate reverse-vaccinology environment\nmamba activate reverse-vaccinology\n# create DIAMOND-formatted databases\ndiamond makedb --in uniprot_sprot.fasta -d swissprot\ndiamond makedb --in UP000005640_9606.fasta -d human_proteome\n\n\nCARD\nThis database is used by the Nextflow workflow nf-core/funcscan. The database is downloaded by the workflow itself, but if you run this workflow regularly, it might be best to download it once, to save time and bandwidth.\nInstructions for this are given in the workflow documentation page. Here is how we did it for our workshop:\nmkdir card\nwget -O card.tar.bz2 https://card.mcmaster.ca/latest/data\ntar -xjvf card.tar.bz2 -C card\nrm card.tar.bz2",
    "crumbs": [
      "Slides",
      "Welcome",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data & Setup</span>"
    ]
  },
  {
    "objectID": "materials/01-know_your_bug.html",
    "href": "materials/01-know_your_bug.html",
    "title": "3  Know your bug",
    "section": "",
    "text": "3.1 Summary\nBefore starting your analysis, it’s important to understand the characteristics of the species you’re working with as this will determine how best to proceed. In particular, you need to understand how much genetic diversity or plasticity is present in your bug and whether or not it recombines. For instance the approach you take to build a phylogenetic tree of Mycobacterium tuberculosis genomes will differ from the methods you’d use to build a tree with Escherichia coli. To help with making your decision, we’ve provided a flowchart which outlines the best approach to take depending on what your dataset is composed of:\nThis week, we’re going to work with three different bacterial species that each require a different analysis path through the diagram above.",
    "crumbs": [
      "Slides",
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Know your bug</span>"
    ]
  },
  {
    "objectID": "materials/01-know_your_bug.html#summary",
    "href": "materials/01-know_your_bug.html#summary",
    "title": "3  Know your bug",
    "section": "",
    "text": "TipKey Points\n\n\n\n\nThree main things should be considered when choosing an analysis workflow for bacterial sequencing data:\n\nHow diverse is the species? Monoclonal species usually have lower diversity in the population.\nDo your isolates likely come from multiple lineages?\nIs bacterial recombination common in your species (transformation, transduction, conjugation)?\n\nDepending on the answer to these questions, your analyses workflow may involve:\n\nMapping to a reference genome or using a pan-genome approach.\nIncluding a recombination removal step.\n\nThe end goal of most bacterial genomics projects is the generation of a phylogenetic tree representing the relationships and diversity of your isolates.",
    "crumbs": [
      "Slides",
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Know your bug</span>"
    ]
  },
  {
    "objectID": "materials/02-preparing_data.html",
    "href": "materials/02-preparing_data.html",
    "title": "4  Preparing data",
    "section": "",
    "text": "4.1 Preparing Files\nOn our computers, for each of the three species we will be working with on the course, we have a directory in ~/Course_Materials for each species where we will do all our analysis. We already included the following in each species directory:",
    "crumbs": [
      "Slides",
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Preparing data</span>"
    ]
  },
  {
    "objectID": "materials/02-preparing_data.html#preparing-files",
    "href": "materials/02-preparing_data.html#preparing-files",
    "title": "4  Preparing data",
    "section": "",
    "text": "data/ - contains the sequencing files we will be working with.\npreprocessed/ - contains the results for all the analysis we’ll be running during the course (see box below).\nresources/ - where we include other files we will be using such as reference genomes.\ndatabases/ - where we include public databases needed by some tools.\nscripts/ - where we include some scripts that we will use during the course. You will have to edit some of these scripts as part of the exercises.\nsample_info.csv - a table with some metadata for our samples.\n\n\n\n\n\n\n\nWarning\n\n\n\nWe have a lot to fit in this week, and during testing, we found that running all the pipelines and tools on even the genomes we selected from larger datasets was going to take too long. So, what we decided, was to only analyse five samples during the exercises you’ll be doing this week. This will allow you to set up and run the various scripts, see the pipelines and tools running and see the outputs in the time we’ve set aside for each section. The outputs will always be sent to a results directory. However, we’ve provided the results you would have obtained if you’d run the scripts on all of the data in a preprocessed directory. For some of the exercises, you’ll be making use of the outputs in this directory instead of what you create yourselves as this will hopefully give you a more realistic set of results like the ones you may eventually generate when working with larger datasets. We will make it clear in the course materials and exercises when you should be working with the results in the preprocessed directory instead of the results directory.\n\n\n\n4.1.1 Data\nYour analysis starts with FASTQ files (if you need a reminder of what a FASTQ file is, look at the Intro to NGS &gt; FASTQ section of the materials). The Illumina files come as compressed FASTQ files (.fastq.gz format) and there’s two files per sample, corresponding to read 1 and read 2. This is indicated by the file name suffix:\n\n*_1.fastq.gz for read 1\n*_2.fastq.gz for read 2\n\nYou can look at the files you have available in any of the species directories from the command line using:\nls data/reads\n\n\n4.1.2 Metadata\nA critical step in any analysis is to make sure that our samples have all the relevant metadata associated with them. This is important to make sense of our results and produce informative reports at the end. There are many types of information that can be collected from each sample and for effective genomic surveillance, we need at the very minimum three pieces of information:\n\nWhen: date when the sample was collected (not when it was sequenced!).\nWhere: the location where the sample was collected (not where it was sequenced!).\nSource: the source of the the sample e.g. host, environment.\n\nOf course, this is the minimum metadata we need for a useful analysis. The more information you collect about each sample, the more questions you can ask from your data – so always remember to record as much information as possible for each sample.\n\n\n\n\n\n\nWarningDates in Spreadsheet Programs\n\n\n\nNote that programs such as Excel often convert date columns to their own format, and this can cause problems when analysing data later on. The ISO standard for dates is YYYY-MM-DD and this is how we recommend you store your dates. However, by default Excel displays dates as DD/MM/YYYY.\nYou can change how Excel displays dates by highlighting the date column, right-clicking and selecting Format cells, then select “Date” and pick the format that matches YYYY-MM-DD. However, every time you open the CSV file, Excel annoyingly converts it back to its default format!\nTo make sure no date information is lost due to Excel’s behaviour, it’s a good idea to store information about year, month and day in separate columns (stored just as regular numbers).",
    "crumbs": [
      "Slides",
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Preparing data</span>"
    ]
  },
  {
    "objectID": "materials/02-preparing_data.html#summary",
    "href": "materials/02-preparing_data.html#summary",
    "title": "4  Preparing data",
    "section": "4.2 Summary",
    "text": "4.2 Summary\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nProper file and folder organization ensures clarity, reproducibility, and efficiency throughout your bioinformatic analysis.\nOrganising files by project, and creating directories for data, scripts and results helps prevent data mix-ups and confusion.\nFASTQ files containing the raw sequencing data, can be quickly investigated using standard command line tools, for example to count how many reads are available.\nMetadata provides context to biological data, including sample information, experimental conditions, and data sources.\nIn pathogen surveillance, metadata helps trace the origin and characteristics of samples, aiding outbreak investigation.",
    "crumbs": [
      "Slides",
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Preparing data</span>"
    ]
  },
  {
    "objectID": "materials/03-intro_ngs.html",
    "href": "materials/03-intro_ngs.html",
    "title": "5  Introduction to NGS",
    "section": "",
    "text": "5.1 Next Generation Sequencing\nThe sequencing of genomes has become more routine due to the rapid drop in DNA sequencing costs seen since the development of Next Generation Sequencing (NGS) technologies in 2007. One main feature of these technologies is that they are high-throughput, allowing one to more fully characterise the genetic material in a sample of interest.\nThere are three main technologies in use nowadays, often referred to as 2nd and 3rd generation sequencing:\nThe video below from the iBiology team gives a great overview of these technologies.",
    "crumbs": [
      "Slides",
      "Introduction",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to NGS</span>"
    ]
  },
  {
    "objectID": "materials/03-intro_ngs.html#next-generation-sequencing",
    "href": "materials/03-intro_ngs.html#next-generation-sequencing",
    "title": "5  Introduction to NGS",
    "section": "",
    "text": "Illumina’s sequencing by synthesis (2nd generation)\nOxford Nanopore, shortened ONT (3rd generation)\nPacific Biosciences, shortened PacBio (3rd generation)\n\n\n\n\n5.1.1 Illumina Sequencing\nIllumina’s technology has become a widely popular method, with many applications to study transcriptomes (RNA-seq), epigenomes (ATAC-seq, BS-seq), DNA-protein interactions (ChIP-seq), chromatin conformation (Hi-C/3C-Seq), population and quantitative genetics (variant detection, GWAS), de-novo genome assembly, amongst many others.\nAn overview of the sequencing procedure is shown in the animation video below. Generally, samples are processed to generate so-called sequencing libraries, where the genetic material (DNA or RNA) is processed to generate fragments of DNA with attached oligo adapters necessary for the sequencing procedure (if the starting material is RNA, it can be converted to DNA by a step of reverse transcription). Each of these DNA molecule is then sequenced from both ends, generating pairs of sequences from each molecule, i.e. paired-end sequencing (single-end sequencing, where the molecule is only sequenced from one end is also possible, although much less common nowadays).\nThis technology is a type of short-read sequencing, because we only obtain short sequences from the original DNA molecules. Typical protocols will generate 2x50bp to 2x250bp sequences (the 2x denotes that we sequence from each end of the molecule).\n\n\n\n\nThe main advantage of Illumina sequencing is that it produces very high-quality sequence reads (current protocols generate reads with an error rate of less than &lt;1%) at a low cost. However, the fact that we only get relatively short sequences means that there are limitations when it comes to resolving particular problems such as long sequence repeats (e.g. around centromeres or transposon-rich areas of the genome), distinguishing gene isoforms (in RNA-seq), or resolving haplotypes (combinations of variants in each copy of an individual’s diploid genome).\nIn summary, Illumina:\n\nUtilizes sequencing-by-synthesis chemistry.\nOffers short read lengths.\nKnown for high accuracy with low error rates (&lt;1%).\nWell-suited for applications like DNA resequencing and variant detection.\nScalable and cost-effective for large-scale projects.\nLimited in sequencing long DNA fragments.\nExpensive to set up.\n\n\n\n5.1.2 Nanopore Sequencing\nNanopore sequencing is a type of long-read sequencing technology. The main advantage of this technology is that it can sequence very long DNA molecules (up to megabase-sized), thus overcoming the main shortcoming of short-read sequencing mentioned above. Another big advantage of this technology is its portability, with some of its devices designed to work via USB plugged to a standard laptop. This makes it an ideal technology to use in situations where it is not possible to equip a dedicated sequencing facility/laboratory (for example, when doing field work).\n\n\n\nOverview of Nanopore sequencing showing the highly-portable MinION device. The device contains thousands of nanopores embedded in a membrane where current is applied. As individual DNA molecules pass through these nanopores they cause changes in this current, which is detected by sensors and read by a dedicated computer program. Each DNA base causes different changes in the current, allowing the software to convert this signal into base calls.\n\n\nHowever, optimising this technology presents some challenges, notably in the production of sequencing libraries containing high molecular weight and intact DNA. It’s important to note that nanopore sequencing historically exhibited higher error rates, approximately 5% for older chemistries, compared to Illumina sequencing. However, significant advancements have emerged, enhancing the accuracy of nanopore sequencing technology, now achieving accuracy rates exceeding 99%.\nIn summary, ONT:\n\nOperates on the principle of nanopore technology.\nProvides long read lengths, ranging from thousands to tens of thousands of base pairs.\nIdeal for applications requiring long-range information, such as de novo genome assembly and structural variant analysis.\nPortable, enabling fieldwork and real-time sequencing.\nExhibits higher error rates (around 5%), with improvements in recent versions.\nCosts can be higher per base, compared to Illumina for certain projects.\n\n\n\n\n\n\n\nNoteWhich technology to choose?\n\n\n\nBoth of these platforms have been widely popular for bacterial sequencing. They can both generate data with high-enough quality for the assembly and analysis for most of the pathogen genomic surveillance. Mostly, which one you use will depend on what sequencing facilities you have access to.\nWhile Illumina provides the cheapest option per sample of the two, it has a higher setup cost, requiring access to the expensive sequencing machines. On the other hand, Nanopore is a very flexible platform, especially its portable MinION devices. They require less up-front cost allowing getting started with sequencing very quickly in a standard molecular biology lab.",
    "crumbs": [
      "Slides",
      "Introduction",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to NGS</span>"
    ]
  },
  {
    "objectID": "materials/03-intro_ngs.html#sec-file-formats",
    "href": "materials/03-intro_ngs.html#sec-file-formats",
    "title": "5  Introduction to NGS",
    "section": "5.2 Bioinformatics file formats",
    "text": "5.2 Bioinformatics file formats\nBioinformatics relies on various standard file formats for storing diverse types of data. In this section, we’ll discuss some of the key ones we’ll encounter, although there are numerous others. You can refer to the “Common file formats” appendix for a more comprehensive list.\n\n5.2.1 FAST5\nFAST5 is a proprietary format developed by ONT and serves as the standard format generated by its sequencing devices. It is based on the hierarchical data format HDF5, designed for storing extensive and intricate data. Unlike text-based formats like FASTA and FASTQ, FAST5 files are binary, necessitating specialized software for opening and reading.\nWithin these files, you’ll find a Raw/ field containing the original raw current signal measurements. Additionally, tools like basecallers can add Analyses/ fields, converting signals into standard FASTQ data (e.g., Guppy basecaller).\nTypically, manual inspection of these files is unnecessary, as specialized software is used for processing them. For more in-depth information about this format, you can refer to this resource.\n\n\n5.2.2 FASTQ\nFASTQ files are used to store nucleotide sequences along with a quality score for each nucleotide of the sequence. These files are the typical format obtained from NGS sequencing platforms such as Illumina and Nanopore (after basecalling). Common file extensions used for this format include .fastq and .fq.\nThe file format is as follows:\n@SEQ_ID                   &lt;-- SEQUENCE NAME\nAGCGTGTACTGTGCATGTCGATG   &lt;-- SEQUENCE\n+                         &lt;-- SEPARATOR\n%%).1***-+*''))**55CCFF   &lt;-- QUALITY SCORES\nIn FASTQ files each sequence is always represented across 4 lines. The quality scores are encoded in a compact form, using a single character. They represent a score that can vary between 0 and 40 (see Illumina’s Quality Score Encoding). The reason single characters are used to encode the quality scores is that it saves space when storing these large files. Software that work on FASTQ files automatically convert these characters into their score, so we don’t have to worry about doing this conversion ourselves.\nThe quality value in common use is called a Phred score and it represents the probability that the base is an error. For example, a base with quality 20 has a probability \\(10^{-2} = 0.01 = 1\\%\\) of being an error. A base with quality 30 has \\(10^{-3} = 0.001 = 0.1\\%\\) chance of being an error. Typically, a Phred score threshold of &gt;20 or &gt;30 is used when applying quality filters to sequencing reads.\nBecause FASTQ files tend to be quite large, they are often compressed to save space. The most common compression format is called gzip and uses the extension .gz. To look at a gzip file, we can use the command zcat, which decompresses the file and prints the output as text.\nFor example, we can use the following command to count the number of lines in a compressed FASTQ file:\nzcat sequences.fq.gz | wc -l\nIf we want to know how many sequences there are in the file, we can divide the result by 4 (since each sequence is always represented across four lines).\n\n\n5.2.3 FASTA\nFASTA files are used to store nucleotide or amino acid sequences. Common file extensions used for this format include .fasta, .fa, .fas and .fna.\nThe general structure of a FASTA file is illustrated below:\n&gt;sample01                 &lt;-- NAME OF THE SEQUENCE\nAGCGTGTACTGTGCATGTCGATG   &lt;-- SEQUENCE ITSELF\nEach sequence is represented by a name, which always starts with the character &gt;, followed by the actual sequence.\nA FASTA file can contain several sequences, for example:\n&gt;sample01\nAGCGTGTACTGTGCATGTCGATG\n&gt;sample02\nAGCGTGTACTGTGCATGTCGATG\nEach sequence can sometimes span multiple lines, and separate sequences can always be identified by the &gt; character. For example, this contains the same sequences as above:\n&gt;sample01      &lt;-- FIRST SEQUENCE STARTS HERE\nAGCGTGTACTGT\nGCATGTCGATG\n&gt;sample02      &lt;-- SECOND SEQUENCE STARTS HERE\nAGCGTGTACTGT\nGCATGTCGATG\nTo count how many sequences there are in a FASTA file, we can use the following command:\ngrep \"&gt;\" sequences.fa | wc -l\nIn two steps:\n\nfind the lines containing the character “&gt;”, and then\ncount the number of lines of the result.\n\nFASTA files are commonly used to store genome sequences, after they have been assembled. We will see FASTA files several times throughout these materials, so it’s important to be familiar with them.\n\n\n5.2.4 GFF3\nThe GFF3 (Generic Feature Format version 3) is a standardized file format used in bioinformatics to describe genomic features and annotations. It primarily serves as a structured and human-readable way to represent information about genes, transcripts, and other biological elements within a genome. Common file extensions used for this format include .gff and .gff3.\nKey characteristics of the GFF3 format include:\n\nTab-delimited columns: GFF3 files consist of tab-delimited columns, making them easy to read and parse.\nHierarchical structure: the format supports a hierarchical structure, allowing the description of complex relationships between features. For instance, it can represent genes containing multiple transcripts, exons, and other elements.\nNine standard columns: this includes information such as the sequence identifier (e.g. chromosome), feature type (e.g. gene, exon), start and end coordinates, strand and several attributes.\nAttributes field: the ninth column, known as the “attributes” field, contains additional information in a key-value format. This field is often used to store details like gene names, IDs, and functional annotations.\nComments: GFF3 files can include comment lines starting with a “#” symbol to provide context or documentation.\n\nGFF3 is widely supported by various bioinformatics tools and databases, making it a versatile format for storing and sharing genomic annotations.\n\n\n5.2.5 CSV/TSV\nComma-separated values (CSV) and tab-separated values (TSV) files are text-based formats commonly used to store tabular data. While strictly not specific to bioinformatics, they are commonly used as the output of bioinformatic software. CSV files usually have .csv extension, while TSV files often have .tsv or the more generic .txt extension.\nIn both cases, the data is organized into rows and columns. Rows are represented across different lines of the file, while the columns are separated using a delimiting character: a command , in the case of CSV files and a tab space (tab ↹) for TSV files.\nFor example, for this table:\n\n\n\nsample\ndate\nstrain\n\n\n\n\nVCH001\n2023-08-01\nO1 El Tor\n\n\nVCH002\n2023-08-02\nO1 Classical\n\n\nVCH003\n2023-08-03\nO139\n\n\nVCH004\n2023-08-04\nNon-O1 Non-O139\n\n\n\nThis would be its representation as a CSV file:\nsample,date,strain\nVCH001,2023-08-01,O1 El Tor\nVCH002,2023-08-02,O1 Classical\nVCH003,2023-08-03,O139\nVCH004,2023-08-04,Non-O1 Non-O139\nAnd this is its representation as a TSV file (the space between columns is a tab ↹):\nsample    date        strain\nVCH001    2023-08-01  O1 El Tor\nVCH002    2023-08-02  O1 Classical\nVCH003    2023-08-03  O139\nVCH004    2023-08-04  Non-O1 Non-O139\nCSV and TSV files are human-readable and can be opened and edited using basic text editors or spreadsheet software like Microsoft Excel.",
    "crumbs": [
      "Slides",
      "Introduction",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to NGS</span>"
    ]
  },
  {
    "objectID": "materials/03-intro_ngs.html#summary",
    "href": "materials/03-intro_ngs.html#summary",
    "title": "5  Introduction to NGS",
    "section": "5.3 Summary",
    "text": "5.3 Summary\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nHigh-throughput sequencing technologies, often called next-generation sequencing (NGS), enable rapid and cost-effective genome sequencing.\nProminent NGS platforms include Illumina, Oxford Nanopore Technologies (ONT) and Pacific Biosciences (PacBio).\nEach platform employs distinct mechanisms for DNA sequencing, leading to variations in read length, error rates, and applications.\nIllumina sequencing:\n\nUses sequencing-by-synthesis chemistry, produces short read lenghts and has high accuracy with low error rates (&lt;1%).\nWhile it is scalable and cost-effective for large-scale projects, it is expensive to set up and limited in sequencing long DNA fragments.\n\nNanopore sequencing:\n\nUses nanopore technology, provides long read lengths, making it ideal for applications such as de novo genome assembly.\nAlthough the costs can be higher per base, it is cheaper to set up.\nExhibits higher error rates (around 5%), but with significant improvements in recent versions (1%).\n\nCommon file formats in bioinformatics include FASTQ, FASTA and GFF. These are all text-based formats.\nFASTQ format (.fastq or .fq):\n\nDesigned to store sequences along with quality scores.\nContains a sequence identifier, sequence data, a separator line and quality scores.\nWidely used for storing sequence reads generated by NGS platforms.\n\nFASTA format (.fasta, .fa, .fas, .fna):\n\nIs used for storing biological sequences, including DNA, RNA, and protein.\nIt Comprises a sequence identifier (often preceded by “&gt;”) and the sequence data.\nCommonly used for sequence storage and exchange of genome sequences.\n\nGFF format (.gff or .gff3):\n\nA structured, tab-delimited format for describing genomic features and annotations.\nConsists of nine standard columns, including sequence identifier, feature type, start and end coordinates, strand information, and attributes.\nFacilitates the representation of genes, transcripts, and other genomic elements, supporting hierarchical structures and metadata.\nCommonly used for storing and sharing genomic annotation data in bioinformatics.\n\nCSV (.csv) and TSV (.tsv):\n\nPlain text formats to store tables.\nThe columns in the CSV format are delimited by comma, whereas in the TSV format by a tab.\nThese files can be opened in standard spreadsheet software such as Excel.",
    "crumbs": [
      "Slides",
      "Introduction",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to NGS</span>"
    ]
  },
  {
    "objectID": "materials/04-nextflow.html",
    "href": "materials/04-nextflow.html",
    "title": "6  Workflow management",
    "section": "",
    "text": "6.1 Workflows\nAnalysing data involves a sequence of tasks, including gathering, cleaning, and processing data. These sequence of tasks are called a workflow or a pipeline. These workflows typically require executing multiple software packages, sometimes running on different computing environments, such as a desktop or a compute cluster. Traditionally these workflows have been joined together in scripts using general purpose programming languages such as Bash or Python.\nHowever, as workflows become larger and more complex, the management of the programming logic and software becomes difficult.",
    "crumbs": [
      "Slides",
      "Introduction",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Workflow management</span>"
    ]
  },
  {
    "objectID": "materials/04-nextflow.html#workflows",
    "href": "materials/04-nextflow.html#workflows",
    "title": "6  Workflow management",
    "section": "",
    "text": "Example bioinformatics variant calling workflow/pipeline diagram from nf-core (bactmap).",
    "crumbs": [
      "Slides",
      "Introduction",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Workflow management</span>"
    ]
  },
  {
    "objectID": "materials/04-nextflow.html#workflow-management-systems",
    "href": "materials/04-nextflow.html#workflow-management-systems",
    "title": "6  Workflow management",
    "section": "6.2 Workflow management systems",
    "text": "6.2 Workflow management systems\nWorkflow Management Systems (WfMS), such as Snakemake, Galaxy, and Nextflow have been developed specifically to manage computational data-analysis workflows in fields such as Bioinformatics, Imaging, Physics, and Chemistry.\nWfMS contain multiple features that simplify the development, monitoring, execution and sharing of pipelines.\nKey features include;\n\nRun time management: Management of program execution on the operating system and splitting tasks and data to run at the same time in a process called parallelisation.\nSoftware management: Use of technology like containers, such as Docker or Singularity, that packages up code and all its dependencies so the application runs reliably from one computing environment to another.\nPortability & Interoperability: Workflows written on one system can be run on another computing infrastructure e.g., local computer, compute cluster, or cloud infrastructure.\nReproducibility: The use of software management systems and a pipeline specification means that the workflow will produce the same results when re-run, including on different computing platforms.\nReentrancy: Continuous checkpoints allow workflows to resume from the last successfully executed steps.",
    "crumbs": [
      "Slides",
      "Introduction",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Workflow management</span>"
    ]
  },
  {
    "objectID": "materials/04-nextflow.html#nextflow-basic-concepts",
    "href": "materials/04-nextflow.html#nextflow-basic-concepts",
    "title": "6  Workflow management",
    "section": "6.3 Nextflow basic concepts",
    "text": "6.3 Nextflow basic concepts\nNextflow is a workflow management system that combines a runtime environment, software that is designed to run other software, and a programming domain specific language (DSL) that eases the writing of computational pipelines.\nNextflow is built around the idea that Linux is the lingua franca of data science. Nextflow follows Linux’s “small pieces loosely joined” philosophy: in which many simple but powerful command-line and scripting tools, when chained together, facilitate more complex data manipulations.\nNextflow extends this approach, adding the ability to define complex program interactions and an accessible (high-level) parallel computational environment based on the dataflow programming model, whereby processes are connected via their outputs and inputs to other processes, and run as soon as they receive an input. The diagram below illustrates the differences between a dataflow model and a simple linear program .\n\n\n\nA simple program (a) and its dataflow equivalent (b). Adapted from Johnston, Hanna and Millar 2004.\n\n\nIn a simple program (a), these statements would be executed sequentially. Thus, the program would execute in three units of time. In the dataflow programming model (b), this program takes only two units of time. This is because the read quantitation and QC steps have no dependencies on each other and therefore can execute simultaneously in parallel.\n\n6.3.1 Nextflow core features\n\nFast prototyping: A simple syntax for writing pipelines that enables you to reuse existing scripts and tools for fast prototyping.\nReproducibility: Nextflow supports several container technologies, such as Docker and Singularity, as well as the package manager Conda. This, along with the integration of the GitHub code sharing platform, allows you to write self-contained pipelines, manage versions and to reproduce any former configuration.\nPortability: Nextflow’s syntax separates the functional logic (the steps of the workflow) from the execution settings (how the workflow is executed). This allows the pipeline to be run on multiple platforms, e.g. local compute vs. a university compute cluster or a cloud service like AWS, without changing the steps of the workflow.\nSimple parallelism: Nextflow is based on the dataflow programming model which greatly simplifies the splitting of tasks that can be run at the same time (parallelisation).\nContinuous checkpoints: All the intermediate results produced during the pipeline execution are automatically tracked. This allows you to resume its execution from the last successfully executed step, no matter what the reason was for it stopping.\n\n\n\n6.3.2 Scripting language\nNextflow scripts are written using a language intended to simplify the writing of workflows. Languages written for a specific field are called Domain Specific Languages (DSL), e.g., SQL is used to work with databases, and AWK is designed for text processing.\nIn practical terms the Nextflow scripting language is an extension of the Groovy programming language, which in turn is a super-set of the Java programming language. Groovy simplifies the writing of code and is more approachable than Java. Groovy semantics (syntax, control structures, etc) are documented here.\nThe approach of having a simple DSL built on top of a more powerful general purpose programming language makes Nextflow very flexible. The Nextflow syntax can handle most workflow use cases with ease, and then Groovy can be used to handle corner cases which may be difficult to implement using the DSL.\n\n\n6.3.3 DSL2 syntax\nNextflow (version &gt; 20.07.1) provides a revised syntax to the original DSL, known as DSL2. The DSL2 syntax introduces several improvements such as modularity (separating components to provide flexibility and enable reuse), and improved data flow manipulation. This further simplifies the writing of complex data analysis pipelines, and enhances workflow readability, and reusability.\n\n\n6.3.4 Processes, channels, and workflows\nNextflow workflows have three main parts; processes, channels, and workflows. Processes describe a task to be run. A process script can be written in any scripting language that can be executed by the Linux platform (Bash, Perl, Ruby, Python, etc.). Processes spawn a task for each complete input set. Each task is executed independently, and cannot interact with another task. The only way data can be passed between process tasks is via asynchronous queues, called channels.\nProcesses define inputs and outputs for a task. Channels are then used to manipulate the flow of data from one process to the next. The interaction between processes, and ultimately the pipeline execution flow itself, is then explicitly defined in a workflow section.\nIn the following example we have a channel containing three elements, e.g., 3 data files. We have a process that takes the channel as input. Since the channel has three elements, three independent instances (tasks) of that process are run in parallel. Each task generates an output, which is passed to another channel and used as input for the next process.\n\n\n\nProcesses and channels.\n\n\n\n\n6.3.5 Workflow execution\nWhile a process defines what command or script has to be executed, the executor determines how that script is actually run in the target system.\nIf not otherwise specified, processes are executed on the local computer. The local executor is very useful for pipeline development, testing, and small scale workflows, but for large scale computational pipelines, a High Performance Cluster (HPC) or Cloud platform is often required.\n\n\n\nProcesses and channels.\n\n\nNextflow provides a separation between the pipeline’s functional logic and the underlying execution platform. This makes it possible to write a pipeline once, and then run it on your computer, compute cluster, or the cloud, without modifying the workflow, by defining the target execution platform in a configuration file.\nNextflow provides out-of-the-box support for major batch schedulers and cloud platforms such as Sun Grid Engine, SLURM job scheduler, AWS Batch service and Kubernetes. A full list can be found here.",
    "crumbs": [
      "Slides",
      "Introduction",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Workflow management</span>"
    ]
  },
  {
    "objectID": "materials/04-nextflow.html#snakemake",
    "href": "materials/04-nextflow.html#snakemake",
    "title": "6  Workflow management",
    "section": "6.4 Snakemake",
    "text": "6.4 Snakemake\nIn this section we’ve focused on Nextflow but many people in the bioinformatics community use Snakemake. Similar to Nextflow, the Snakemake workflow management system is a tool for creating reproducible and scalable data analyses and it supports all the same features mentioned above. Perhaps the most noticeable difference for users is that Snakemake is based on the Python programming language. This makes it more approachable for those already familiar with this language.",
    "crumbs": [
      "Slides",
      "Introduction",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Workflow management</span>"
    ]
  },
  {
    "objectID": "materials/04-nextflow.html#summary",
    "href": "materials/04-nextflow.html#summary",
    "title": "6  Workflow management",
    "section": "6.5 Summary",
    "text": "6.5 Summary\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nWorkflow management software is designed to simplify the process of orchestrating complex computational pipelines that involve various tasks, inputs and outputs, and parallel processing.\nUsing workfow managent software to manage complex pipelines has several advantages: reproducibility, parallel task execution, automatic software management, scalability (from a local computer to cloud and HPC cluster servers) and “checkpoint and resume” ability.\nNextflow and Snakemake are two of the most popular workflow managers used in bioinformatics, with an active community of developers and several useful features:\n\nFlexible syntax that can be adapted to any task.\nThe ability to reuse and share modules written by the community.\nIntegration with code sharing platforms such as GitHub and GitLab.\nUse of containerisation solutions (Docker and Singularity) and software package managers such as Conda.",
    "crumbs": [
      "Slides",
      "Introduction",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Workflow management</span>"
    ]
  },
  {
    "objectID": "materials/04-nextflow.html#credit",
    "href": "materials/04-nextflow.html#credit",
    "title": "6  Workflow management",
    "section": "6.6 Credit",
    "text": "6.6 Credit\nInformation on this page has been adapted and modified from the following source(s):\n\nGraeme R. Grimes, Evan Floden, Paolo Di Tommaso, Phil Ewels and Maxime Garcia. Introduction to Workflows with Nextflow and nf-core. https://github.com/carpentries-incubator/workflows-nextflow 2021.",
    "crumbs": [
      "Slides",
      "Introduction",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Workflow management</span>"
    ]
  },
  {
    "objectID": "materials/05-nf_core.html",
    "href": "materials/05-nf_core.html",
    "title": "7  The nf-core project",
    "section": "",
    "text": "7.1 What is nf-core?\nnf-core is a community-led project to develop a set of best-practice pipelines built using Nextflow workflow management system. Pipelines are governed by a set of guidelines, enforced by community code reviews and automatic code testing.",
    "crumbs": [
      "Slides",
      "Introduction",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The nf-core project</span>"
    ]
  },
  {
    "objectID": "materials/05-nf_core.html#what-is-nf-core",
    "href": "materials/05-nf_core.html#what-is-nf-core",
    "title": "7  The nf-core project",
    "section": "",
    "text": "nf-core",
    "crumbs": [
      "Slides",
      "Introduction",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The nf-core project</span>"
    ]
  },
  {
    "objectID": "materials/05-nf_core.html#what-are-nf-core-pipelines",
    "href": "materials/05-nf_core.html#what-are-nf-core-pipelines",
    "title": "7  The nf-core project",
    "section": "7.2 What are nf-core pipelines?",
    "text": "7.2 What are nf-core pipelines?\nnf-core pipelines are an organised collection of Nextflow scripts, other non-nextflow scripts (written in any language), configuration files, software specifications, and documentation hosted on GitHub. There is generally a single pipeline for a given data and analysis type, e.g. there is a single pipeline for bulk RNA-Seq. All nf-core pipelines are distributed under the open MIT licence.",
    "crumbs": [
      "Slides",
      "Introduction",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The nf-core project</span>"
    ]
  },
  {
    "objectID": "materials/05-nf_core.html#running-nf-core-pipelines",
    "href": "materials/05-nf_core.html#running-nf-core-pipelines",
    "title": "7  The nf-core project",
    "section": "7.3 Running nf-core pipelines",
    "text": "7.3 Running nf-core pipelines\n\n7.3.1 Software requirements for nf-core pipelines\nnf-core pipeline software dependencies are specified using either Docker, Singularity or Conda. It is Nextflow that handles the downloading of containers and creation of conda environments. In theory it is possible to run the pipelines with software installed by other methods (e.g. environment modules, or manual installation), but this is not recommended.",
    "crumbs": [
      "Slides",
      "Introduction",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The nf-core project</span>"
    ]
  },
  {
    "objectID": "materials/05-nf_core.html#usage-instructions-and-documentation",
    "href": "materials/05-nf_core.html#usage-instructions-and-documentation",
    "title": "7  The nf-core project",
    "section": "7.4 Usage instructions and documentation",
    "text": "7.4 Usage instructions and documentation\nYou can find general documentation and instructions for Nextflow and nf-core on the nf-core website . Pipeline-specific documentation is bundled with each pipeline in the /docs folder. This can be read either locally, on GitHub, or on the nf-core website.\nEach pipeline has its own webpage e.g. nf-co.re/rnaseq.\nIn addition to this documentation, each pipeline comes with basic command line reference. This can be seen by running the pipeline with the parameter --help . It is also recommended to explicitly specify the version of the pipeline you want to run, to ensure reproducibility if you run it again in the future. This can be done with the -r option.\nFor example, the following command prints the help documentation for version 3.4 of the nf-core/rnaseq pipeline:\nnextflow run -r 3.4 nf-core/rnaseq --help\nN E X T F L O W  ~  version 20.10.0\nLaunching `nf-core/rnaseq` [silly_miescher] - revision: 964425e3fd [3.4]\n------------------------------------------------------\n                                        ,--./,-.\n        ___     __   __   __   ___     /,-._.--~'\n  |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n  | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                        `._,._,'\n  nf-core/rnaseq v3.0\n------------------------------------------------------\n\nTypical pipeline command:\n\n    nextflow run nf-core/rnaseq --input samplesheet.csv --genome GRCh37 -profile docker\n\nInput/output options\n    --input                             [string]  Path to comma-separated file containing information about the samples in the experiment.\n    --outdir                            [string]  Path to the output directory where the results will be saved.\n    --public_data_ids                   [string]  File containing SRA/ENA/GEO identifiers one per line in order to download their associated FastQ files.\n    --email                             [string]  Email address for completion summary.\n    --multiqc_title                     [string]  MultiQC report title. Printed as page header, used for filename if not otherwise specified.\n    --skip_sra_fastq_download           [boolean] Only download metadata for public data database ids and don't download the FastQ files.\n    --save_merged_fastq                 [boolean] Save FastQ files after merging re-sequenced libraries in the results directory.\n..truncated..\n\n7.4.1 Config files\nnf-core pipelines make use of Nextflow’s configuration files to specify how the pipelines runs, define custom parameters and what software management system to use e.g. docker, singularity or conda.\nNextflow can load pipeline configurations from multiple locations. nf-core pipelines load configuration in the following order:\n\n\n\nNextflow config loading order\n\n\n\nPipeline: Default ‘base’ config\n\nAlways loaded. Contains pipeline-specific parameters and “sensible defaults” for things like computational requirements.\nDoes not specify any method for software packaging. If nothing else is specified, Nextflow will expect all software to be available on the command line.\n\nCore config profiles\n\nAll nf-core pipelines come with some generic config profiles. The most commonly used ones are for software packaging: docker, singularity and conda.\nOther core profiles are ‘debug’ and two ‘test’ profiles. The two test profiles include: a small version for quick testing, which pulls data from a public repository at nf-core/test-datasets; and a full test profile which provides the path to full-sized data from public repositories.\n\nServer profiles\n\nAt run time, nf-core pipelines fetch configuration profiles from the configs remote repository. The profiles here are specific to clusters at different institutions.\nBecause this is loaded at run time, anyone can add a profile here for their system and it will be immediately available for all nf-core pipelines.\n\nLocal config files given to Nextflow with the -c flag\nnextflow run nf-core/rnaseq -r 3.4 -c mylocal.config\nCommand line configuration: pipeline parameters can be passed on the command line using the --&lt;parameter&gt; syntax. We will see several examples of this use throughout the course.\n\n\n\n7.4.2 Config profiles\nnf-core makes use of Nextflow configuration profiles to make it easy to apply a group of options on the command line.\nConfiguration files can contain the definition of one or more profiles. A profile is a set of configuration attributes that can be activated/chosen when launching a pipeline execution by using the -profile command line option. Common profiles are conda, singularity and docker that specify which software manager to use.\nMultiple profiles are comma-separated. When there are differing configuration settings provided by different profiles, the right-most profile takes priority.\nFor example, the following command runs the test profile (i.e. run the test pipeline) and use Singularity containers as a way to manage the software required by the pipeline:\nnextflow run nf-core/rnaseq -r 3.4 -profile test,singularity\nNote The order in which config profiles are specified matters. Their priority increases from left to right.\n\n\n\n\n\n\nNoteMultiple Nextflow configuration locations\n\n\n\nBe clever with multiple Nextflow configuration locations. For example, use -profile for your cluster configuration, the file $HOME/.nextflow/config for your personal config such as params.email and a working directory nextflow.config file for reproducible run-specific configuration.\n\n\n\n\n7.4.3 Running pipelines with test data\nThe nf-core config profile test is a special profile, which defines a minimal data set and configuration, that runs quickly and tests the workflow from beginning to end. Since the data is minimal, the output is often nonsense. Real world example output are instead linked on the nf-core pipeline web page, where the workflow has been run with a full size data set:\n$ nextflow run nf-core/&lt;pipeline_name&gt; -r \"&lt;pipeline_version&gt;\" -profile test\n\n\n\n\n\n\nTipSoftware configuration profile\n\n\n\nNote that you will typically still need to combine this with a software configuration profile for your system - e.g. -profile test,conda. Running with the test profile is a great way to confirm that you have Nextflow configured properly for your system before attempting to run with real data",
    "crumbs": [
      "Slides",
      "Introduction",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The nf-core project</span>"
    ]
  },
  {
    "objectID": "materials/05-nf_core.html#troubleshooting",
    "href": "materials/05-nf_core.html#troubleshooting",
    "title": "7  The nf-core project",
    "section": "7.5 Troubleshooting",
    "text": "7.5 Troubleshooting\nIf you run into issues running your pipeline you can you the nf-core website to troubleshoot common mistakes and issues https://nf-co.re/usage/troubleshooting .\n\n7.5.1 Extra resources and getting help\nIf you still have an issue with running the pipeline then feel free to contact the nf-core community via the Slack channel . The nf-core Slack workspace has channels dedicated for each pipeline, as well as specific topics (eg. #help, #pipelines, #tools, #configs and much more). To join this workspace you will need an invite, which you can get at https://nf-co.re/join/slack.\nYou can also get help by opening an issue in the respective pipeline repository on GitHub asking for help.\nIf you have problems that are directly related to Nextflow and not our pipelines or the nf-core framework tools then check out the Nextflow gitter channel or the google group.",
    "crumbs": [
      "Slides",
      "Introduction",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The nf-core project</span>"
    ]
  },
  {
    "objectID": "materials/05-nf_core.html#referencing-a-pipeline",
    "href": "materials/05-nf_core.html#referencing-a-pipeline",
    "title": "7  The nf-core project",
    "section": "7.6 Referencing a Pipeline",
    "text": "7.6 Referencing a Pipeline\n\n7.6.1 Publications\nIf you use an nf-core pipeline in your work you should cite the main publication for the main nf-core paper, describing the community and framework, as follows:\n\nThe nf-core framework for community-curated bioinformatics pipelines. Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen. Nat Biotechnol. 2020 Feb 13. doi: 10.1038/s41587-020-0439-x. ReadCube: Full Access Link\n\nMany of the pipelines have a publication listed on the nf-core website that can be found here.\n\n\n7.6.2 DOIs\nIn addition, each release of an nf-core pipeline has a digital object identifiers (DOIs) for easy referencing in literature The DOIs are generated by Zenodo from the pipeline’s github repository.",
    "crumbs": [
      "Slides",
      "Introduction",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The nf-core project</span>"
    ]
  },
  {
    "objectID": "materials/05-nf_core.html#summary",
    "href": "materials/05-nf_core.html#summary",
    "title": "7  The nf-core project",
    "section": "7.7 Summary",
    "text": "7.7 Summary\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nnf-core is a community-led project to develop and curate a set of high-quality bioinformatic pipelines.\nAll pipelines are listed on the nf-co.re website.\nEach pipeline contains both general usage documentation, as well as detailed help for its input parameters and the output files generated by the pipeline.\nSeveral aspects of the pipelines can be configured, using configuration files.\nDefault profiles can be used to load a set of default configurations. Common profiles in nf-core pipelines include:\n\ntest to run a quick test, useful to see if the software is correctly setup on the computer/server being used.\ndocker and singularity to indicate we want to use either Docker or Singularity to automatically manage the software installation during the pipeline run.",
    "crumbs": [
      "Slides",
      "Introduction",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The nf-core project</span>"
    ]
  },
  {
    "objectID": "materials/05-nf_core.html#credit",
    "href": "materials/05-nf_core.html#credit",
    "title": "7  The nf-core project",
    "section": "7.8 Credit",
    "text": "7.8 Credit\nInformation on this page has been adapted and modified from the following source(s):\n\nGraeme R. Grimes, Evan Floden, Paolo Di Tommaso, Phil Ewels and Maxime Garcia. Introduction to Workflows with Nextflow and nf-core. https://github.com/carpentries-incubator/workflows-nextflow 2021.",
    "crumbs": [
      "Slides",
      "Introduction",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The nf-core project</span>"
    ]
  },
  {
    "objectID": "materials/06-downloading.html",
    "href": "materials/06-downloading.html",
    "title": "8  Downloading sequence data",
    "section": "",
    "text": "8.1 Pipeline Overview\nfetchngs is a bioinformatics analysis pipeline written in Nextflow to automatically download and process raw FASTQ files from public databases. Identifiers can be provided in a file and any type of accession ID found in the SRA, ENA, DDBJ and GEO databases are supported. If run accessions (SRR/ERR/DRR) are provided, these will be resolved back to the sample accessions (SRX/ERX/DRX) to allow multiple runs for the same sample to be merged. As well as the FASTQ files, fetchngs will also produce a samplesheet.csv file containing the sample metadata obtained from the ENA. This file can be used as input for other nf-core and Nextflow pipelines like the ones we’ll be using this week.",
    "crumbs": [
      "Slides",
      "Introduction",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Downloading sequence data</span>"
    ]
  },
  {
    "objectID": "materials/06-downloading.html#pipeline-overview",
    "href": "materials/06-downloading.html#pipeline-overview",
    "title": "8  Downloading sequence data",
    "section": "",
    "text": "The fetchngs pipeline",
    "crumbs": [
      "Slides",
      "Introduction",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Downloading sequence data</span>"
    ]
  },
  {
    "objectID": "materials/06-downloading.html#prepare-a-samples-file",
    "href": "materials/06-downloading.html#prepare-a-samples-file",
    "title": "8  Downloading sequence data",
    "section": "8.2 Prepare a samples file",
    "text": "8.2 Prepare a samples file\nfetchngs requires a samples file with the accessions you would like to download. The file requires the suffix .csv but does not need to be in CSV format. Each line needs to represent a database id:\nERR9907668\nERR9907669\nERR9907670\nERR9907671\nERR9907672\n\n\n\n\n\n\nExerciseExercise 1 - Preparing a samples file\n\n\n\n\n\n\nYour first task is to create a samples.csv file to be used as input for fetchngs. Use the following accessions:\nERR9907668\nERR9907669\nERR9907670\nERR9907671\nERR9907672\nMake sure you save the file in the ~/Course_Materials/M_tuberculosis directory.\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nWe opened a text editor (e.g. nano. vim), copied and pasted the accessions and saved the file as samples.csv in the ~/Course_Materials/M_tuberculosis directory.",
    "crumbs": [
      "Slides",
      "Introduction",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Downloading sequence data</span>"
    ]
  },
  {
    "objectID": "materials/06-downloading.html#running-fetchngs",
    "href": "materials/06-downloading.html#running-fetchngs",
    "title": "8  Downloading sequence data",
    "section": "8.3 Running fetchngs",
    "text": "8.3 Running fetchngs\nNow that we have the samples.csv file, we can run the fetchngs pipeline. First, let’s activate the nextflow software environment:\nmamba activate nextflow\nThere are many options that can be used to customise the pipeline but a typical command is shown below:\nnextflow run nf-core/fetchngs \\\n  -r \"1.12.0\" \\\n  -profile singularity \\\n  --input SAMPLES \\\n  --outdir results/fetchngs \\\n  --nf_core_pipeline viralrecon \\\n  --download_method sratools \\\n  -resume\nThe options we used are:\n\n-profile singularity - indicates we want to use the Singularity program to manage all the software required by the pipeline (another option is to use docker). See Data & Setup for details about their installation.\n--input - the samples file with the accessions to be downloaded, as explained above.\n--nf_core_pipeline - Name of supported nf-core pipeline e.g. ‘viralrecon’. A samplesheet for direct use with the pipeline will be created with the appropriate columns.\n--download_method - forces the pipeline to use sratools instead of a direct FTP download.\n-resume - all Nextflow pipelines can be resumed. It isn’t necessary for the force run of the pipeline but it’s good practice to include it in the command.\n\n\n\n\n\n\n\nExerciseExercise 2 - Running fetchngs\n\n\n\n\n\n\nYour next task is to download sequence data with the fetchngs. In the folder scripts (in the M_tuberculosis analysis directory) you will find a script named 01-run_fetchngs.sh. This script contains the code to run fetchngs.\n\nEdit this script, adjusting it to fit your input file.\nRun the script using bash scripts/01-run_fetchngs.sh.\nIf the script is running successfully it should start printing the progress of each job in the fetchngs pipeline.\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\n\nThe fixed script is:\n\n#!/bin/bash\n\nnextflow run nf-core/fetchngs \\\n  -r \"1.12.0\" \\\n  -profile singularity \\\n  --input samples.csv \\\n  --outdir results/fetchngs \\\n  --nf_core_pipeline viralrecon \\\n  --download_method sratools \\\n  -resume\n\nWe ran the script as instructed using:\n\nbash scripts/01-run_fetchngs.sh\n\nWhile it was running it printed a message on the screen:\n\nexecutor &gt;  local (20)\n[5d/682f49] process &gt; NFCORE_FETCHNGS:SRA:SRA_IDS_TO_RUNINFO (ERR9907668)                                                          [100%] 5 of 5 ✔\n[9d/ab8a7b] process &gt; NFCORE_FETCHNGS:SRA:SRA_RUNINFO_TO_FTP (5)                                                                   [100%] 5 of 5 ✔\n[f9/39e27c] process &gt; NFCORE_FETCHNGS:SRA:SRA_FASTQ_FTP (ERX9450498_ERR9907670)                                                    [ 80%] 4 of 5\n[82/c5f847] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_DOWNLOAD_PREFETCH_FASTERQDUMP_SRATOOLS:CUSTOM_SRATOOLSNCBISETTINGS (ncbi-settings) [100%] 1 of 1 ✔\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_DOWNLOAD_PREFETCH_FASTERQDUMP_SRATOOLS:SRATOOLS_PREFETCH                           -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:FASTQ_DOWNLOAD_PREFETCH_FASTERQDUMP_SRATOOLS:SRATOOLS_FASTERQDUMP                        -\n[2e/4fd490] process &gt; NFCORE_FETCHNGS:SRA:SRA_TO_SAMPLESHEET (ERX9450498_ERR9907670)                                               [100%] 4 of 4\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:SRA_MERGE_SAMPLESHEET                                                                    -\n[-        ] process &gt; NFCORE_FETCHNGS:SRA:MULTIQC_MAPPINGS_CONFIG                                                                  -",
    "crumbs": [
      "Slides",
      "Introduction",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Downloading sequence data</span>"
    ]
  },
  {
    "objectID": "materials/06-downloading.html#fetchngs-results",
    "href": "materials/06-downloading.html#fetchngs-results",
    "title": "8  Downloading sequence data",
    "section": "8.4 fetchngs results",
    "text": "8.4 fetchngs results\nOnce fetchngs has run, we can look at the various directories it created in results/fetchngs:\n\n\n\n\n\n\n\nDirectory\nDescription\n\n\n\n\ncustom\nContains settings to help the pipeline run\n\n\nfastq\nPaired-end/single-end reads downloaded from the SRA/ENA/DDBJ/GEO for each accession in the samples.csv file\n\n\nmetadata\nContains the re-formatted ENA metadata for each sample\n\n\nsamplesheet\nContains the samplesheet with collated metadata and paths to downloaded FASTQ files\n\n\npipeline_info\nContains information about the pipeline run\n\n\n\n\n\n\n\n\n\nWarningThe Nextflow work directory\n\n\n\nEach step of the pipeline produces one or more files that are not saved to the results directory but are kept in the work directory. This means that if, for whatever reason, the pipeline doesn’t finish successfully you can resume it. However, once the pipeline has completed successfully, you no longer need this directory (it can take up a lot of space) so you can delete it:\nrm -rf work",
    "crumbs": [
      "Slides",
      "Introduction",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Downloading sequence data</span>"
    ]
  },
  {
    "objectID": "materials/06-downloading.html#summary",
    "href": "materials/06-downloading.html#summary",
    "title": "8  Downloading sequence data",
    "section": "8.5 Summary",
    "text": "8.5 Summary\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nThe nf-core fetchngs pipeline can be used to quickly download sequence data from public databases such as the ENA and GEO.\nThe pipeline also produces a samplesheet.csv file that can be used as input for other nf-core and Nextflow pipelines.",
    "crumbs": [
      "Slides",
      "Introduction",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Downloading sequence data</span>"
    ]
  },
  {
    "objectID": "materials/07-intro_tb.html",
    "href": "materials/07-intro_tb.html",
    "title": "9  Introduction to Mycobacterium tuberculosis",
    "section": "",
    "text": "9.1 Mycobacterium tuberculosis\nMycobacterium tuberculosis, the bacterium that causes tuberculosis (TB) in humans, is a significant pathogen with a considerable global impact:\nIn 1998, the first complete genome sequence of a M. tuberculosis strain, the virulent laboratory reference strain H37Rv, was published (Cole 1998). The genome of M. tuberculosis is a single circular chromosome that is approximately 4.4 million base pairs in size and contains around 4000 genes. M. tuberculosis is a member of the Mycobacterium tuberculosis complex (MTBC), which includes different lineages, some referred to as M. tuberculosis sensu stricto (lineage 1 to lineage 4 and lineage 7), others as M. africanum (lineage 5 and lineage 6), two recently discovered lineages (lineage 8 and lineage 9), and several animal-associated ecotypes such as M. bovis and M. caprae. Some lineages are geographically widespread, while others like L5 and L6 (mainly found in West Africa), are more restricted. A simplified phylogeny showing the relationship of the various MTBC lineages is shown below.\nIncreasingly, M. tuberculosis is resistant to many of the frontline antimicrobials used to treat TB, such as isoniazid and rifampicin. This poses an enormous clinical, financial, and public health challenge across the world. Traditionally, susceptibility of TB isolates to different antimicrobials was conducted in the laboratory but in recent years, antimicrobial profiling using genomic sequencing has been shown to be nearly as accurate as lab methods, especially for the most commonly used drugs. Catalogues of genetic variants that are known to confer resistance to particular antimicrobials are used to type TB genomes, potentially saving time and money.",
    "crumbs": [
      "Slides",
      "Sequence quality control",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to _Mycobacterium tuberculosis_</span>"
    ]
  },
  {
    "objectID": "materials/07-intro_tb.html#mycobacterium-tuberculosis",
    "href": "materials/07-intro_tb.html#mycobacterium-tuberculosis",
    "title": "9  Introduction to Mycobacterium tuberculosis",
    "section": "",
    "text": "In 2020, the World Health Organization estimated that TB was responsible for 10.6 million active cases and 1.6 million deaths across the globe. This means that M .tuberculosis causes greater mortality than any other single pathogen.\nM. tuberculosis is a small, aerobic, nonmotile bacillus. The high lipid content of its cell wall makes the cell impervious to Gram staining, so it is classified as an acid-fast bacillus.\nThe bacterium is able to survive and multiply within macrophages, which are cells that usually kill bacteria. This ability to evade the immune system contributes to its virulence.\nM. tuberculosis is transmitted from person to person via droplets from the throat and lungs of people with active respiratory disease. In healthy individuals, the immune system can often fight off the bacteria and prevent them from spreading within the body. However, in immunocompromised individuals, such as those with HIV, the bacteria can spread and cause active disease.\n\n\n\n\n\nPhylogeny of M. tuberculosis lineage strains. Simplified maximum likelihood phylogeny of the 10 human-associaed lineages of M. tuberculosis and the related animal strains. M. canettii was used as a root.",
    "crumbs": [
      "Slides",
      "Sequence quality control",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to _Mycobacterium tuberculosis_</span>"
    ]
  },
  {
    "objectID": "materials/07-intro_tb.html#course-dataset",
    "href": "materials/07-intro_tb.html#course-dataset",
    "title": "9  Introduction to Mycobacterium tuberculosis",
    "section": "9.2 Course dataset",
    "text": "9.2 Course dataset\nWe will be analysing a dataset of Namibian M. tuberculosis genomes that was recently published (Claasens 2022). The original dataset consisted of 136 drug-resistant TB isolates collected from patients between 2016-2018 across Namibia. For the purposes of this course, we’re only going to analyse 50 genomes from the dataset.",
    "crumbs": [
      "Slides",
      "Sequence quality control",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to _Mycobacterium tuberculosis_</span>"
    ]
  },
  {
    "objectID": "materials/07-intro_tb.html#mtbc-ancestral-reference-sequence",
    "href": "materials/07-intro_tb.html#mtbc-ancestral-reference-sequence",
    "title": "9  Introduction to Mycobacterium tuberculosis",
    "section": "9.3 MTBC ancestral reference sequence",
    "text": "9.3 MTBC ancestral reference sequence\nThe most widely used reference genomes when doing reference-based alignment of MTBC short reads are the H37Rv type strain originally sequenced in 1998 and the putative MTBC ancestral sequence that was inferred by Comas et al. in 2013. As both of these sequences were based on lineage 4 sequences, they do not capture the complete structural variation likely to be found in the MTBC. To improve this ancestral sequence, Harrison et al. compared closed (i.e. complete with no gaps) genomes from across the MTBC and inferred a new MTBC ancestral sequence, MTBC0 (Harrison 2023). This is the reference sequence we’ll use this week, available on the authors’ repository.",
    "crumbs": [
      "Slides",
      "Sequence quality control",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to _Mycobacterium tuberculosis_</span>"
    ]
  },
  {
    "objectID": "materials/07-intro_tb.html#summary",
    "href": "materials/07-intro_tb.html#summary",
    "title": "9  Introduction to Mycobacterium tuberculosis",
    "section": "9.4 Summary",
    "text": "9.4 Summary\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nThe bacterium Mycobacterium tuberculosis causes tuberculosis (TB) in humans and poses several public health challenges due to its ability to evade the immune system and evolve antimicrobial resistance.\nDescribe the main characteristics of this species’ genome and diversity.\nThis species’ genome consists of a single circular genome of around 4.4 Mbp.\nSeveral lineages have been identified in this species.\nTo account for the diversity in the species a new reference genome, MTBC0, has been recently defined to improve reference-based alignment of short-reads. This genome is available to download from a public repository.",
    "crumbs": [
      "Slides",
      "Sequence quality control",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to _Mycobacterium tuberculosis_</span>"
    ]
  },
  {
    "objectID": "materials/07-intro_tb.html#references",
    "href": "materials/07-intro_tb.html#references",
    "title": "9  Introduction to Mycobacterium tuberculosis",
    "section": "9.5 References",
    "text": "9.5 References\nClaasens M, et al. Whole-Genome Sequencing for Resistance Prediction and Transmission Analysis of Mycobacterium tuberculosis Complex Strains from Namibia. Microbiology Spectrum. 2022. DOI\nCole ST, et al. Deciphering the biology of Mycobacterium tuberculosis from the complete genome sequence. Nature. 1998. DOI\nHarrison L, et al. An imputed ancestral reference genome for the Mycobacterium tuberculosis complex better captures structural genomic diversity for reference-based alignment workflows. bioRxiv. 2023. DOI\nWorld Health Organization. Global Tuberculosis Report 2021. Geneva: World Health Organization; 2021. Link",
    "crumbs": [
      "Slides",
      "Sequence quality control",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to _Mycobacterium tuberculosis_</span>"
    ]
  },
  {
    "objectID": "materials/08-intro_qc.html",
    "href": "materials/08-intro_qc.html",
    "title": "10  Introduction to QC",
    "section": "",
    "text": "10.1 Introduction\nBefore we look at our genomic data, lets take time to explore what to look out for when performing Quality Control (QC) checks on our sequence data. For this course, we will largely focus on next generation sequences obtained from Illumina sequencers. As you may already know from Introduction to NGS, the main output files expected from our Illumina sequencer are FASTQ files.",
    "crumbs": [
      "Slides",
      "Sequence quality control",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Introduction to QC</span>"
    ]
  },
  {
    "objectID": "materials/08-intro_qc.html#qc-assessment-of-ngs-data",
    "href": "materials/08-intro_qc.html#qc-assessment-of-ngs-data",
    "title": "10  Introduction to QC",
    "section": "10.2 QC assessment of NGS data",
    "text": "10.2 QC assessment of NGS data\nQC is an important part of any analysis and, in this section, we’re going to look at some of the metrics and graphs that can be used to assess the QC of NGS data.\n\n10.2.1 Base quality\nIllumina sequencing technology relies on sequencing by synthesis. One of the most common problems with this is dephasing. For each sequencing cycle, there is a possibility that the replication machinery slips and either incorporates more than one nucleotide or perhaps misses to incorporate one at all. The more cycles that are run (i.e. the longer the read length gets), the greater the accumulation of these types of errors gets. This leads to a heterogeneous population in the cluster, and a decreased signal purity, which in turn reduces the precision of the base calling. The figure below shows an example of this.\n\n\n\nBase Quality\n\n\nBecause of dephasing, it is possible to have high-quality data at the beginning of the read but really low-quality data towards the end of the read. In those cases you can decide to trim off the low-quality reads. In this course, we’ll do this using the tool fastp. In addition to trimming and removing low quality reads, fastp will also be used to trim Illumina adapter/primer sequences.\nThe figures below show an example of high-quality read data (left) and poor quality read data (right).\n\n\n\n\n\n\nHigh-quality read data\n\n\n\n\n\n\n\nPoor quality read data\n\n\n\n\n\nIn addition to Phasing noise and signal decay resulting from dephasing issues described above, there are several different reasons for a base to be called incorrectly. You can lookup these later by clicking here.\n\n\n10.2.2 Mismatches per cycle\nAligning reads to a high-quality reference genome can provide insights into the quality of a sequencing run by showing you the mismatches to the reference sequence. In particular, this can help you detect cycle-specific errors. Mismatches can occur due to two main causes: sequencing errors and differences between your sample and the reference genome; this is important to bear in mind when interpreting mismatch graphs. The figures below show an example of a good run and a bad one. In the first figure, the distribution of the number of mismatches is even between the cycles, which is what we would expect from a good run. However, in the second figure, two cycles stand out with a lot of mismatches compared to the other cycles.\n\n\n\n\n\n\nGood run\n\n\n\n\n\n\n\nPoor run\n\n\n\n\n\n\n\n10.2.3 GC bias\nIt is a good idea to compare the GC content of the reads against the expected distribution in a reference sequence. The GC content varies between species, so a shift in GC content like the one seen below (right image) could be an indication of sample contamination. In the left image below, we can see that the GC content of the sample is about the same as for the theoretical reference, at ~65%. However, in the right figure, the GC content of the sample shows two distributions: one is closer to 40% and the other closer to 65%, indicating that there is an issue with this sample, likely contamination.\n\n\n\n\n\n\nSingle GC distribution\n\n\n\n\n\n\n\nDouble GC distribution\n\n\n\n\n\n\n\n10.2.4 GC content by cycle\nLooking at the GC content per cycle can help detect if the adapter sequence was trimmed. For a random library, there is expected to be little to no difference between the different bases of a sequence run, so the lines in this plot should be parallel with each other like in the first of the two figures below. In the second of the figures, the initial spikes are likely due to adapter sequences that have not been removed.\n\n\n\n\n\n\nGood run\n\n\n\n\n\n\n\nPoor run\n\n\n\n\n\n\n\n\n10.2.5 Insert size\nFor paired-end sequencing the size of DNA fragments also matters. In the first of the examples below, the insert size peaks around 440 bp. In the second however, there is also a peak at around 200 bp. This indicates that there was an issue with the fragment size selection during library prep.\n\n\n\n\n\n\nGood run\n\n\n\n\n\n\n\nPoor run\n\n\n\n\n\n\n\n10.2.6 Insertions/Deletions per cycle\nSometimes, air bubbles occur in the flow cell, and this can manifest as false indels. The spike in the second image provides an example of how this can look.\n\n\n\n\n\n\nGood run\n\n\n\n\n\n\n\nPoor run",
    "crumbs": [
      "Slides",
      "Sequence quality control",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Introduction to QC</span>"
    ]
  },
  {
    "objectID": "materials/08-intro_qc.html#assessment-of-species-composition",
    "href": "materials/08-intro_qc.html#assessment-of-species-composition",
    "title": "10  Introduction to QC",
    "section": "10.3 Assessment of species composition",
    "text": "10.3 Assessment of species composition\nUnderstanding the species composition of sequence data is crucial for the accuracy and reliability of bioinformatics analyses, especially in the context of de novo genome assembly and metagenomics. In particular, for de novo genome assembly, knowing the species present in a sample can help identify and filter out contaminant sequences that do not belong to the target organism, improving the quality of the assembly. An abundance of non-target sequences also means fewer reads belonging to the target species leading to lower coverage when mapping these reads to a reference genome.",
    "crumbs": [
      "Slides",
      "Sequence quality control",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Introduction to QC</span>"
    ]
  },
  {
    "objectID": "materials/08-intro_qc.html#summary",
    "href": "materials/08-intro_qc.html#summary",
    "title": "10  Introduction to QC",
    "section": "10.4 Summary",
    "text": "10.4 Summary\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nCommon metrics to assess the quality of raw sequencing data include: base quality, mismatches per cycle, GC bias, GC content per cycle, insert size and indels per cycle.\nContamination of sequencing data with other organisms is problematic for applications such as de novo genome assembly.\nScreening the sequencing data for known species can help to remove potential contaminants.",
    "crumbs": [
      "Slides",
      "Sequence quality control",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Introduction to QC</span>"
    ]
  },
  {
    "objectID": "materials/08-intro_qc.html#references",
    "href": "materials/08-intro_qc.html#references",
    "title": "10  Introduction to QC",
    "section": "10.5 References",
    "text": "10.5 References\nInformation on this page has been adapted and modified from the following sources:\n\nhttps://github.com/sanger-pathogens/QC-training\nhttps://www.bioinformatics.babraham.ac.uk/projects/fastqc/",
    "crumbs": [
      "Slides",
      "Sequence quality control",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Introduction to QC</span>"
    ]
  },
  {
    "objectID": "materials/09-bacqc.html",
    "href": "materials/09-bacqc.html",
    "title": "11  The bacQC pipeline",
    "section": "",
    "text": "11.1 Pipeline Overview\nbacQC is a bioinformatics analysis pipeline written in Nextflow that automates the Quality Control of short read sequencing data. It runs the following tools:\nSee Course Software for a more detailed description of each tool.\nAlong with the outputs produced by the above tools, the pipeline produces the following summaries containing results for all samples run through the pipeline (found in the metadata and multiqc directories):",
    "crumbs": [
      "Slides",
      "Sequence quality control",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>The bacQC pipeline</span>"
    ]
  },
  {
    "objectID": "materials/09-bacqc.html#pipeline-overview",
    "href": "materials/09-bacqc.html#pipeline-overview",
    "title": "11  The bacQC pipeline",
    "section": "",
    "text": "The bacQC pipeline\n\n\n\n\nfastQC - assesses sequencing read quality\nfastq-scan - calculates FASTQ summary statistics\nfastp - performs adapter/quality trimming on sequencing reads\nKraken 2 - assigns taxonomic labels to reads\nBracken - refines Kraken 2 assignments\nKrona - provides visualisations of Bracken outputs\nMultiQC - summarises and creates visualizations for outputs from fastQC, fastp, Kraken 2 and Bracken\n\n\n\n\nraw_fastq-scan_summary.tsv - final summary of FASTQ summary statistics for input files in TSV format\ntrim_fastq-scan_summary.tsv - final summary of FASTQ summary statistics for trimmed FASTQ files\nread_stats_summary.tsv - final summary of pre- and post-trimming sequence statistics in TSV format\nspecies_composition.tsv - final summary of taxonomic assignment in TSV format in TSV format\nmultiqc_report.html - final summary of sequence quality, trimming and species composition for input files in HTML format",
    "crumbs": [
      "Slides",
      "Sequence quality control",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>The bacQC pipeline</span>"
    ]
  },
  {
    "objectID": "materials/09-bacqc.html#prepare-a-samplesheet",
    "href": "materials/09-bacqc.html#prepare-a-samplesheet",
    "title": "11  The bacQC pipeline",
    "section": "11.2 Prepare a samplesheet",
    "text": "11.2 Prepare a samplesheet\nBefore we run bacQC, we need to prepare a CSV file with information about our sequencing files which will be used as an input to the bacQC pipeline (for this exercise we’re going to QC the TB dataset described in Introduction to Mycobacterium tuberculosis). The pipeline’s documentation gives details about the format of this samplesheet.\n\n\n\n\n\n\nExerciseExercise 1 - Prepare a samplesheet\n\n\n\n\n\n\nPrepare the input samplesheet for bacQC. You can do this using Excel, making sure you save it as a CSV file (File → Save As… and choose “CSV” as the file format). Alternatively, you can use the fastq_dir_to_samplesheet.py script that can be found in the scripts directory:\npython scripts/fastq_dir_to_samplesheet.py data/reads \\\n    samplesheet.csv \\\n    -r1 _1.fastq.gz \\\n    -r2 _2.fastq.gz\nBy default the script will add names to each sample based on the name of the FASTQ files. However, you may want to use more meaningful names to each sample. If that were the case, you could open the file in Excel and edit it further.",
    "crumbs": [
      "Slides",
      "Sequence quality control",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>The bacQC pipeline</span>"
    ]
  },
  {
    "objectID": "materials/09-bacqc.html#running-bacqc",
    "href": "materials/09-bacqc.html#running-bacqc",
    "title": "11  The bacQC pipeline",
    "section": "11.3 Running bacQC",
    "text": "11.3 Running bacQC\nNow that we have the samplesheet, we can run the bacQC pipeline. First, let’s activate the nextflow software environment:\nmamba activate nextflow\nThere are many options that can be used to customise the pipeline but a typical command is shown below:\nnextflow run avantonder/bacQC \\\n  -r \"v2.0.1\" \\\n  -profile singularity \\\n  --input SAMPLESHEET \\\n  --outdir results/bacqc \\\n  --kraken2db databases/k2_standard_08gb_20240605 \\\n  --kronadb databases/krona/taxonomy.tab \\\n  --genome_size GENOME_SIZE \nThe options we used are:\n\n-r - tells Nextflow to pull the main version of bacQC from Github\n-profile singularity - indicates we want to use the Singularity program to manage all the software required by the pipeline (another option is to use docker). See Data & Setup for details about their installation.\n--input - the samplesheet with the input files, as explained above.\n--kraken2db - the path to the directory containing the Kraken2 database files.\n--kronadb - the path to Krona’s database file (with the .tab extension).\n--genome_size - the estimated genome size of your samples - fastq-scan uses this to calculate the depth of coverage across the genome.\n\n\n\n\n\n\n\nExerciseExercise 2 - Running bacQC\n\n\n\n\n\n\nYour first task is to run the bacQC pipeline on your data. In the folder scripts (within your analysis directory) you will find a script named 02-run_bacqc.sh. This script contains the code to run bacQC.\n\nEdit this script, adjusting it to fit your input files and the estimated genome size of M. tuberculosis.\nRun the script using bash scripts/02-run_bacqc.sh.\n\nIf the script is running successfully it should start printing the progress of each job in the bacQC pipeline. This will take a little while to finish.  You can continue working through the materials by using preprocessed data detailed in the following sections.\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\n\nThe fixed script is:\n\n#!/bin/bash\n\nnextflow run avantonder/bacQC \\\n  -r \"v2.0.1\" \\\n  -profile singularity \\\n  --input samplesheet.csv \\\n  --outdir results/bacqc \\\n  --kraken2db databases/k2_standard_08gb_20240605 \\\n  --kronadb databases/krona/taxonomy.tab \\\n  --genome_size 4300000\n\nWe ran the script as instructed using:\n\nbash scripts/02-run_bacQC.sh\n\nWhile it was running it printed a message on the screen:\n\nexecutor &gt;  slurm (15)\n[80/fd97e3] AVANTONDER_BACQC:BACQC:FASTQSCAN_RAW (ERX9450496)                      [  0%] 0 of 5\n[-        ] AVANTONDER_BACQC:BACQC:FASTQSCANPARSE_RAW                              -\n[29/21b759] AVANTONDER_BACQC:BACQC:FASTQ_TRIM_FASTP_FASTQC:FASTQC_RAW (ERX9450497) [  0%] 0 of 5\n[5b/7c3de6] AVANTONDER_BACQC:BACQC:FASTQ_TRIM_FASTP_FASTQC:FASTP (ERX9450496)      [  0%] 0 of 5\n[-        ] AVANTONDER_BACQC:BACQC:FASTQ_TRIM_FASTP_FASTQC:FASTQC_TRIM             -\n[-        ] AVANTONDER_BACQC:BACQC:FASTQSCAN_TRIM                                  -\n[-        ] AVANTONDER_BACQC:BACQC:FASTQSCANPARSE_TRIM                             -\n[-        ] AVANTONDER_BACQC:BACQC:READ_STATS                                      -\n[-        ] AVANTONDER_BACQC:BACQC:READSTATS_PARSE                                 -\n[-        ] AVANTONDER_BACQC:BACQC:KRAKEN2_KRAKEN2                                 -\n[-        ] AVANTONDER_BACQC:BACQC:BRACKEN_BRACKEN                                 -\n[-        ] AVANTONDER_BACQC:BACQC:KRAKENPARSE                                     -\n[-        ] AVANTONDER_BACQC:BACQC:KRONA_KTIMPORTTAXONOMY                          -\n[-        ] AVANTONDER_BACQC:BACQC:MULTIQC",
    "crumbs": [
      "Slides",
      "Sequence quality control",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>The bacQC pipeline</span>"
    ]
  },
  {
    "objectID": "materials/09-bacqc.html#bacqc-results",
    "href": "materials/09-bacqc.html#bacqc-results",
    "title": "11  The bacQC pipeline",
    "section": "11.4 bacQC results",
    "text": "11.4 bacQC results\nIn the previous exercise, we left bacQC running. While it runs, we can look at the preprocessed output (preprocessed/bacqc) to see the various directories containing output files created by bacQC:\n\n\n\n\n\n\n\nDirectory\nDescription\n\n\n\n\nbracken\nContains the results of the re-estimation of taxonomic abundance by Bracken\n\n\nfastp\nContains the results of the trimming and adapter removal performed by fastp\n\n\nfastqc\nContains QC metrics for the FASTQ files generated with fastQC\n\n\nfastqscan\nContains summary statistics for the FASTQ files generated with fastq-scan\n\n\nkraken2\nContains the results of taxonomic assignment with Kraken 2\n\n\nkrona\nContains HTML files with visual representations of taxonomic assignments with Bracken\n\n\nmetadata\nContains summary files for outputs from fastq-scan and Kraken 2\n\n\nmultiqc\nContains a HTML file containing summaries of the various outputs\n\n\npipeline_info\nContains information about the pipeline run\n\n\n\nNow that the bacQC pipeline has run, we can assess the quality of our sequence data. At this stage, we want to identify issues such as:\n\nAny samples that have very low read coverage i.e. less than 10x.\nAny samples where the majority of reads were removed during the trimming process.\nAny samples that are contaminated with species other than the target. Typically the threshold for reads assigned to other species will vary depending on what you want to do with the data, e.g. for a Mycobacterium tuberculosis dataset, we might aim for a maximum of 20% reads that map to other species.\n\n\n11.4.1 The MultiQC summary report\nThe first thing we’ll check is the HTML report file created by MultiQC. Go to the File Explorer aplication , navigate to preprocessed/bacqc/multiqc/ and double click on multiqc_report.html. This will open the file in your web browser:\n\n\nGeneral statistics\nLet’s go through each section starting with the “General Statistics”:\n\n\n\nbacQC MultiQC General Statistics\n\n\nThis is a compilation of statistics collected from the outputs of fastp and fastQC. Sequencing metrics such as the % of duplicated reads and GC content of the reads are shown. This is a useful way of quickly identifying samples that are of lower quality due to poor sequencing.\n\n\nfastQC\nThese plots will resemble some of the plots we showed you in Introduction to QC with the main difference being that they contain the results for all samples in summary plots generated by MultiQC. The first plot shows the number of sequences in each sample as a barplot (there should be the same number of forward and reverse reads if it’s paired-end sequencing). An estimate of duplicated reads - i.e. reads that are exactly the same - is also shown.\n\n\n\nBarplot of sequence counts.\n\n\nThe next plot shows the mean Phred score across each base in all the reads for all samples. You’ll notice that the sequence quality tends to be lower at the beginning and end of reads - this is why we tend to trim the ends of reads to improve the overall quality.\n\n\n\nSequence quality histograms.\n\n\nThe third plot shows the frequency distribution of per sequence Phred scores for each sample. Samples with a larger number of lower-quality reads will be shifted to the left.\n\n\n\nPer sequence quality scores.\n\n\nNext we have a plot showing the proportion of each base position for which each of the four normal DNA bases has been called. If you click on a line in the plot, it’ll bring the result for one of your samples, which will make a bit more sense than the MultiQC summary plot.\n\n\n\nPer base sequence content.\n\n\nThe fifth plot shows the percentage of bases called G or C in each sample. The peak should match the approximate %GC content of your organism. In this case the peak is around 65-66% which is what we expect from M. tuberculosis. Contaminated samples will show up as the %GC content is likely to be different from the target species.\n\n\n\nPer sequence GC content.\n\n\nThe sixth plot shows the proportion of N’s across the sequences. There may be a higher proportion of Ns at the beginning and end of reads but we don’t want to see N’s in the middle of reads as this implies something has gone wrong during sequencing.\n\n\n\nPer base N content.\n\n\nNow, we have the distribution of sequence lengths. Our dataset was sequenced in the same facility so the most common sequence length for all samples is 150 bp. If you’re analysing sequences from different studies you may see different sequence lengths.\n\n\n\nSequence length distribution.\n\n\nThe eighth plot shows the number of duplicated reads in each sample. Ideally, you want to see that the majority of your reads are only found once.\n\n\n\nSequence duplication levels.\n\n\nThis plot shows the percentage of overrepresented sequences in each sample. Samples with a higher percentage of overrepresented sequences may suggest an issue with library construction like over-amplification of particular DNA fragments.\n\n\n\nBarplot of overrepresented sequences.\n\n\nNow we have a plot showing the cumulative percentage count of the proportion of your sequences which has seen adapter sequences at each position. Typically we should only see adapter sequences at the beginning and end of reads. If there is a higher proportion of adapter sequences in the middle of reads, then something seriously wrong has occurred during sequencing!\n\n\n\nAdapter content.\n\n\nThe final plot summarises the previous plots and highlights which samples may be worth investigating further or discarding altogether.\n\n\n\nStatus checks.\n\n\n\n\nfastp\nThere are a number of plots showing the results of the fastp step in the pipeline. The first shows the results of the read filtering step where reads are trimmed, adapters removed and low quality reads are thrown out. The reads that passed this step are highlighted in blue.\n\n\n\nBarplot of filtered read counts.\n\n\nThe second plot shows the distribution of insert sizes for each set of sequence files.\n\n\n\n\nInsert sizes.\n\n\nThe next plot shows the average sequence quality across the reads in each sample. You can see we have drop offs in quality at the beginning and end of reads; this is pretty typical and is an artefact of the sequencing process.\n\n\n\nSequence quality.\n\n\nThe fourth plot shows the average GC content across the reads in each sample. As you might expect, the average GC content is conserved across all the samples as they are all from the same organism (M. tuberculosis).\n\n\n\nGC content across the reads.\n\n\nThe final fastp plot shows the average N content across the reads in each sample. Similar to what we see in the sequence quality plot, the number of Ns tends to increase towards the end of reads.\n\n\n\nAverage missing bases (N’s) across the reads.\n\n\n\n\nKraken 2\nThe next results section of the MultiQC report is a summary of the outputs from Kraken 2. It’s important to note that these results are generated before Bracken is run to refine the species assignment of the reads thus the proportion of reads assigned to the target species may be much lower than the results found in the species_composition.tsv file you’ll mostly be working with.\n\n\n\nbacQC MultiQC Kraken Top Taxa\n\n\n\n\nBracken\nThe final results section of the MultiQC report is a summary of the outputs from Bracken. You will see that there are much fewer reads assigned to Other and more reads assigned to Mycobacterium tuberculosis as Bracken has probabilistically re-distributed the Kraken 2 read assignments.\n\n\n\nbacQC MultiQC Bracken Top Taxa\n\n\n\n\nSoftware versions\nThis section of the report shows the software run as part of bacQC and the versions used. This is particularly important when reproducing the analysis on a different system or when writing the methods section of a paper.\n\n\n\nbacQC MultiQC software versions\n\n\n\n\nMethods description\nA brief description of the methods used in the pipeline is provided with the relevant citations.\n\n\n\nbacQC MultiQC methods descriptions\n\n\n\n\n\n11.4.2 The read_stats_summary.tsv file\nOne of the outputs from running bacQC is a summary file summarising the reads in the FASTQ files pre- and post-trimming with fastp. This file can be found in preprocessed/bacqc/metadata/read_stats_summary.tsv.\nYou can open it with spreadsheet software such as Excel from your file browser :\nSample  raw_total_bp    raw_coverage    num_raw_reads   trim_total_bp   trim_coverage   num_trim_reads  %reads_after_trimmed\nERX9450498_ERR9907670_T1    440043124   102.336 2980520 198599257   46.1859 1372732 46.0567954585106\nERX9450499_ERR9907671_T1    377518535   87.795  2550884 170989867   39.7651 1178588 46.2031201732419\nERX9450502_ERR9907674_T1    480127121   111.65700000000001  3637820 224190567   52.1373 1744694 47.95987706923377\nERX9450504_ERR9907676_T1    484152311   112.594 3557654 230112090   53.5144 1733156 48.716260771845725\nERX9450506_ERR9907678_T1    519381556   120.786 3721974 253035696   58.8455 1852758 49.778907644169465\nERX9450508_ERR9907680_T1    439374426   102.18  3396118 205770558   47.8536 1631206 48.03148771626899\nERX9450513_ERR9907685_T1    456807540   106.234 3460956 213737167   49.7063 1658838 47.930051696698825\nERX9450514_ERR9907686_T1    443077738   103.041 3181726 207637480   48.2878 1528944 48.053917904935865\nERX9450515_ERR9907687_T1    475127148   110.495 3486940 216999744   50.4651 1634618 46.878294435809046\nERX9450518_ERR9907690_T1    382086897   88.8574 3540978 201474928   46.8546 1844208 52.081882462980566\nThe columns are:\n\nSample - our sample ID.\nraw_total_bp - the combined total number of base pairs in the read 1 and read 2 FASTQ files.\nraw_coverage - the expected mean read depth across our genome. This is calculated by multiplying the number of reads by the read length and dividing by the genome size (4300000 bp) which we provided when we ran bacQC.\nnum_raw_reads - the number of reads in our FASTQ files (both read 1 and read 2 FASTQ files should have the same number of reads).\ntrim_total_bp - the total number of base pairs left after trimming the FASTQ files with fastp.\ntrim_coverage - the expected mean read depth across our genome after trimming the FASTQ files with fastp.\nnum_trim_reads - the number of reads in our FASTQ files after trimming with fastp.\n%reads_after_trimmed - the proportion of reads left in our FASTQ files after trimming with fastp.\n\nThe main things to look out for in this file are the trim_coverage and %reads_after_trimmed columns. The first gives us a rough idea of how well our reference genome will be covered during mapping or else how good our assemblies might turn out to be when we do de novo assembly. Ideally, the higher this number, the better: at a minimum we want at least 30X coverage (less may suffice for mapping) whilst at the other end, more than 100X is unnecessary. In fact, both the mapping pipeline bactmap and the assembly pipeline assembleBAC we’ll use this week downsample reads to remove any excess reads. This is mainly to speed up the steps in the pipeline and reduce the overall computational cost. The %reads_after_trimmed column gives an indication of the quality of the sequencing: the more reads that are removed by fastp, the lower the overall quality of the sequencing run. In this case, despite the removal of approximately 50% of the reads, we still have sufficient read coverage to proceed with any downstream analyses.\n\n\n11.4.3 The species_composition.tsv file\nAnother important output from the bacQC pipeline to consider is the species_composition.tsv file which summarises the results from Kraken 2 and Bracken and can be found in preprocessed/bacqc/metadata/species_composition.tsv.\nYou can open it with spreadsheet software such as Excel from your file browser :\nname    Mycobacterium tuberculosis  unclassified    other\nERX9450498_ERR9907670_T1    99.52841943273346   0.16379992509505206 0.3077806421714939\nERX9450499_ERR9907671_T1    99.53642406588855   0.1945287515531746  0.26904718255826765\nERX9450502_ERR9907674_T1    98.60308257545614   1.1705576317307351  0.22635979281312757\nERX9450504_ERR9907676_T1    98.81634881409809   0.7964108418062598  0.3872403440956447\nERX9450506_ERR9907678_T1    99.08368809334354   0.5640171321283549  0.3522947745281044\nERX9450508_ERR9907680_T1    97.85346504536902   1.4355205479116022  0.7110144067193716\nERX9450513_ERR9907685_T1    98.67929488431771   1.0007790134265255  0.31992610225576357\nERX9450514_ERR9907686_T1    98.76471973622233   0.6654629193489298  0.569817344428742\nERX9450515_ERR9907687_T1    98.86965758101583   0.7738752254688651  0.3564671935153001\nERX9450518_ERR9907690_T1    97.50545840279487   2.2561398218819857  0.23840177532314044\nThe columns are:\n\nname - our sample ID.\nMycobacterium tuberculosis - the proportion of raw reads assigned to Mycobacterium tuberculosis.\nunclassified - reads that could not be assigned to an organism in the database. As we used a database consisting of only bacterial, archeal and viral references, unclassified reads likely reflect potential host contamination.\nother - any reads assigned to species that don’t pass a threshold of 5% are assigned to the other category. These may reflect kit or flowcell contaminants.\n\nThis quite a simple output and shows that there was very little contamination or non-target sequencing in this run. This may not always be the case - for some runs you may see a variety of different contaminants.\n\n\n\n\n\n\nExerciseExercise 3 - Check the bacQC results\n\n\n\n\n\n\nTo assess the quality of our sequence data, we can use the outputs generated by bacQC, found in preprocessed/bacqc.\nOpen read_stats_summary.tsv and try to answer the following questions:\n\nWere there any samples with a estimated post-trimming coverage less than 10x?\nWere there any samples with a low percentage of reads left post-trimming?\n\nNow, open species_composition.tsv and answer this question:\n\nDo any of the samples contain more than 20% reads that weren’t assigned to Mycobacterium tuberculosis?\n\nMake a note of any samples you think should be removed from any downstream analyses. Feel free to discuss this with other participants and compare your results/conclusions to see if you reach similar conclusions.\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\n\nThe read_stats_summary.tsv file showed that there were no samples with a post-trimming coverage less than 10X and 1 samples where the majority (&gt;80%) of the reads had been removed as part of the trimming process. The predicted coverage of this sample is still 27X which is still ok but we should keep an eye on the results of the mapping.\nWith respect to potential species contamination, no samples contained more than 20% non-M. tuberculosis reads.",
    "crumbs": [
      "Slides",
      "Sequence quality control",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>The bacQC pipeline</span>"
    ]
  },
  {
    "objectID": "materials/09-bacqc.html#summary",
    "href": "materials/09-bacqc.html#summary",
    "title": "11  The bacQC pipeline",
    "section": "11.5 Summary",
    "text": "11.5 Summary\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nQuality Control of sequencing reads can be automated using a Nextflow pipeline like bacQC\nThis pipeline uses:\n\nFastQC to assess the quality of sequencing reads.\nfastp for quality trimming and adapter removal.\nKraken2 to determine species composition.\n\nThe results from the pipeline are aggregated in an interactive MultiQC report, which can be used to identify problematic samples.",
    "crumbs": [
      "Slides",
      "Sequence quality control",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>The bacQC pipeline</span>"
    ]
  },
  {
    "objectID": "materials/10-mapping.html",
    "href": "materials/10-mapping.html",
    "title": "12  Mapping to a reference",
    "section": "",
    "text": "12.1 Mapping to a reference\nThere are two main approaches that are used for reconstructing bacterial genomes:\nEach approach has its benefits and limitations and will be appropriate to the species and analysis you’d like to perform. We’ll cover de novo assembly later in the week and, for now, will focus on reference-based assembly or mapping. When it comes to mapping, regardless of whether we’re dealing with viruses, bacteria or even much larger genomes, the concepts are essentially the same. Instead of de novo (i.e. without any prior knowledge of the genome structure) assembling sequence reads to reconstruct a genome, it is often easier and faster to map/align the sequence data to a previously constructed reference genome. From there we can easily identify SNPs or INDELs that clearly distinguish closely related populations or individuals and use this information to identify genetic differences that may, for instance, cause drug resistance or increase virulence in pathogens. It is important to remember that for the mapping of sequence data to work, the reference and resequenced target must have the same genome architecture. There are a number of different tools for mapping sequence data to a reference genome (e.g. bwa, bowtie2) and calling variants (e.g. bcftools, freebayes, gatk).",
    "crumbs": [
      "Slides",
      "Reference-based assembly",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Mapping to a reference</span>"
    ]
  },
  {
    "objectID": "materials/10-mapping.html#mapping-to-a-reference",
    "href": "materials/10-mapping.html#mapping-to-a-reference",
    "title": "12  Mapping to a reference",
    "section": "",
    "text": "de novo assembly\nreference-based assembly\n\n\n\n\n\nMapping: Consensus Assembly\n\n\n\n12.1.1 Picking a reference\nPicking the best reference for your dataset is very important as this can have a large effect on the downstream analsyses such as phylogenetic tree construction. For some species with low diversity e.g. M. tuberculosis, it is usual to use the same reference (the lab strain H37Rv) regardless of what your dataset is comprised of. If your dataset is comprised of a single lineage (e.g. ST, CC), the best reference to use is one that is from the same or a closely related lineage. For many of the most commonly sequenced bacteria, there are several different reference sequences available in public databases such as RefSeq, meaning that, in most instances, you can find a suitable reference. However, for more diverse organisms, a single reference may not represent all the samples in your dataset. In this instance, you may want consider comparing the similarity of your samples to a number of different reference sequences and pick the one that is similar to most samples in your dataset.",
    "crumbs": [
      "Slides",
      "Reference-based assembly",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Mapping to a reference</span>"
    ]
  },
  {
    "objectID": "materials/10-mapping.html#steps-involved-in-aligning-reads",
    "href": "materials/10-mapping.html#steps-involved-in-aligning-reads",
    "title": "12  Mapping to a reference",
    "section": "12.2 Steps involved in aligning reads",
    "text": "12.2 Steps involved in aligning reads\n\n12.2.1 Genome Indexing\nAs reference genomes can be quite long, most mapping algorithms require that the sequence is indexed. You can think of a genome index in the same way as an index at the end of a textbook, which tells you in which pages of the book you can find certain keywords. Similarly, a genome index is used by mapping algorithms to quickly search through the reference sequence and find a good match with the reads it is trying to align. Each mapping software requires its own index, but we only have to generate the genome index once.\n\n\n12.2.2 Read mapping\nThis is the actual step of aligning the reads to a reference genome. There are a few popular read mapping programs such as bowtie2 or bwa (for this workshop, we will use bwa). The input to these programs includes the genome index (from the previous step) and the FASTQ file(s) containing the sequence reads. The output is an alignment in a file format called SAM (a text-based format which takes up a lot of space) or BAM (a compressed binary format thus a much smaller file size). For more information on these file formats, see Common file formats.\n\n\n12.2.3 BAM Sorting\nThe mapping programs output the sequencing reads in the order in which they were processed i.e. the order they appear in the FASTQ files. But, for downstream analysis, the reads need to be sorted by the position in the reference genome they mapped to as this makes it faster to process the file.\n\n\n12.2.4 BAM Indexing\nThis is similar to the genome indexing mentioned above, but this time involces creating an index for the alignment file. This index is often required for downstream analysis and for visualising the alignment with programs such as the integrated genome viewer (IGV) or Artemis.",
    "crumbs": [
      "Slides",
      "Reference-based assembly",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Mapping to a reference</span>"
    ]
  },
  {
    "objectID": "materials/10-mapping.html#variant-calling",
    "href": "materials/10-mapping.html#variant-calling",
    "title": "12  Mapping to a reference",
    "section": "12.3 Variant calling",
    "text": "12.3 Variant calling\nOnce we’ve mapped our reads to the reference genome and created sorted bam files, we can identify differences between the sequencing reads of our samples and the reference. These differences or variants typically come in the form of insertions or deletions (INDELs) or Single Nucleotide Polymorphisms/Variants (SNPs/SNVs). As the names suggest INDELs are deletions or insertions in the sequencing reads relative to the reference whilst SNPs are changes in a single position that don’t involve an insertion or deletion but reflect a mutation. Like mapping software, there are a number of different variant callers which use statistical models to determine variants with the most popular being bcftools, GATK and FreeBayes. All produce the same standard output, VCF files (see Common file formats) which contain the information about the position and type of variant and the evidence for that variant being identified. The general steps in variant calling are:\n\n12.3.1 Pileup\nThe likelihood of the genotype (Allele Frequency) at each position in the reference genome in the mapped reads is calculated. For example, if 90/100 reads at a position have the same nucleotide as the reference then the alternative allele frequency at that position is (100-90)/100 which is 0.1. This information is then used in the next step to identify variants.\n\n\n12.3.2 Variant calling\nThe information from the pileup is used to identify variants. At this point, all variants are identified regardless of their likelihood. The variants are then filtered to identify the true variants.\n\n\n12.3.3 Variant filtering\nWhen filtering, variant callers use metrics such as the number of reads mapping to a position that call a particular nucleotide, the quality of the reads (taken from the FASTQ file) and the number of forward and reverse reads that have the nucleotide to make a decision as to whether the variant is real or not. Most of the time we’re interested in what the majority decision is which we refer to as the consensus. From the example in the pileup paragraph, we would say that there is insufficient evidence to call a variant in that position as 90% of the reads have the reference nucleotide. This may be due to sequence error or else evidence of a second strain in your sample which has a different nucleotide in that position.",
    "crumbs": [
      "Slides",
      "Reference-based assembly",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Mapping to a reference</span>"
    ]
  },
  {
    "objectID": "materials/10-mapping.html#consensus-calling",
    "href": "materials/10-mapping.html#consensus-calling",
    "title": "12  Mapping to a reference",
    "section": "12.4 Consensus calling",
    "text": "12.4 Consensus calling\nOnce the variants have been identified, we can apply this information to the reference genome to create a consensus ‘pseudogenome’. This is the reference sequence in FASTA format with the variants identified in the sample replacing the nucleotides in the reference sequence. Note, we generally don’t consider the INDELs and only use the SNPs to create our pseudogenomes. This is mainly due to the fact that INDELs are often in homopolymeric regions (runs of the same nucleotide e.g. TTTTTT) which all of the sequencing technologies have trouble resolving and thus the confidence we have in these calls is lower than it is for SNPs. The other step that needs to be taken at this point is to mask low quality or missing regions, where few or no reads mapped to the reference, and these are usually marked as ‘N’ in the pseudogenome.",
    "crumbs": [
      "Slides",
      "Reference-based assembly",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Mapping to a reference</span>"
    ]
  },
  {
    "objectID": "materials/10-mapping.html#summary",
    "href": "materials/10-mapping.html#summary",
    "title": "12  Mapping to a reference",
    "section": "12.5 Summary",
    "text": "12.5 Summary\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nBacterial genomes of low-diversity species can be reconstructed through reference-based alignment.\nThis approach consists of aligning the sequencing reads against a suitable reference genome, identifying potential variants/mutations and creating a consensus sequence by replacing the newly identified variants in the reference genome.",
    "crumbs": [
      "Slides",
      "Reference-based assembly",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Mapping to a reference</span>"
    ]
  },
  {
    "objectID": "materials/10-mapping.html#references",
    "href": "materials/10-mapping.html#references",
    "title": "12  Mapping to a reference",
    "section": "12.6 References",
    "text": "12.6 References\nhttps://github.com/WCSCourses/GenEpiLAC2023/blob/main/Manuals/Mapping_and_Phylogenetics/Mapping%2BPhylo.md",
    "crumbs": [
      "Slides",
      "Reference-based assembly",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Mapping to a reference</span>"
    ]
  },
  {
    "objectID": "materials/11-bactmap.html",
    "href": "materials/11-bactmap.html",
    "title": "13  The nf-core/bactmap pipeline",
    "section": "",
    "text": "13.1 Pipeline Overview\nnf-core/bactmap is a bioinformatics analysis pipeline for mapping short reads from bacterial WGS to a reference sequence, creating filtered VCF files, making pseudogenomes based on high quality positions in the VCF files and optionally creating a phylogeny from an alignment of the pseudogenomes.\nIt runs the following tools:\nSee Course Software for a more detailed description of each tool.\nAlong with the outputs produced by the above tools, the pipeline produces the following summary containing results for all samples run through the pipeline:",
    "crumbs": [
      "Slides",
      "Reference-based assembly",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>The nf-core/bactmap pipeline</span>"
    ]
  },
  {
    "objectID": "materials/11-bactmap.html#pipeline-overview",
    "href": "materials/11-bactmap.html#pipeline-overview",
    "title": "13  The nf-core/bactmap pipeline",
    "section": "",
    "text": "nf-core/bactmap variant calling pipeline diagram from nf-core (https://nf-co.re/bactmap).\n\n\n\n\n\nBWA index - indexes reference fasta file\nfastp - trims reads for quality and adapter sequences (Optional)\nmash sketch - estimates genome size if not provided\nRasusa - downsamples fastq files to 100X by default (Optional)\nBWA mem - maps reads to the reference\nSAMtools - sorts and indexes alignments\nBCFtools - calls and filters variants\nvcf2pseudogenome.py - converts filtered bcf to pseudogenome FASTA\ncalculate_fraction_of_non_GATC_bases.py - creates whole genome alignment from pseudogenomes by concatenating fasta files having first checked that the sample sequences are high quality\nGubbins - identifies recombinant regions (Optional)\nSNP-sites - extracts variant sites from whole genome alignment\nRapidNJ, FastTree2, IQ-TREE, RAxML-NG - construct phylogenetic tree (Optional)\n\n\n\n\nmultiqc_report.html - final summary of trimming and mapping statistics for input files in HTML format",
    "crumbs": [
      "Slides",
      "Reference-based assembly",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>The nf-core/bactmap pipeline</span>"
    ]
  },
  {
    "objectID": "materials/11-bactmap.html#running-nf-corebactmap",
    "href": "materials/11-bactmap.html#running-nf-corebactmap",
    "title": "13  The nf-core/bactmap pipeline",
    "section": "13.2 Running nf-core/bactmap",
    "text": "13.2 Running nf-core/bactmap\nThe bactmap pipeline requires a samplesheet CSV file in the same format as the one we used for bacQC so we can re-use that samplesheet CSV file. If you decided to remove any samples because they didn’t pass the QC, then edit the samplesheet CSV file accordingly. There are many options that can be used to customise the pipeline but a typical command is shown below:\nnextflow run nf-core/bactmap \\\n  -r \"1.0.0\" \\\n  -profile singularity \\\n  --input SAMPLESHEET \\\n  --outdir results/bactmap \\\n  --reference REFERENCE \\\n  --genome_size 4.3M\nThe options we used are:\n\n-profile singularity - indicates we want to use the Singularity program to manage all the software required by the pipeline (another option is to use docker). See Data & Setup for details about their installation.\n--input - the samplesheet with the input files, as explained above.\n--outdir - the output directory for the results.\n--reference - the path and name of the reference genome.\n--genome_size - estimated size of the genome - Rasusa uses this value to calculate the genome coverage.\n\n\n\n\n\n\n\nExerciseExercise 1 - Running nf-core/bactmap\n\n\n\n\n\n\nYour next task is to run the bactmap pipeline on your data. In the folder scripts (within your analysis directory) you will find a script named 03-run_bactmap.sh. This script contains the code to run bactmap.\n\nEdit this script, adjusting it to fit your input files and the name and location of the reference you’re going to map to:\n\nuse the same samplesheet as before with the avantonder/bacQC pipeline.\nthe reference genome is located in resources/reference.\n\nActivate the Nextflow software environment: mamba activate nextflow.\nRun the script using bash scripts/03-run_bactmap.sh.\n\nIf the script is running successfully it should start printing the progress of each job in the bactmap pipeline. The pipeline will take a while to run.  You can continue working through the materials by using preprocessed data detailed in the following sections.\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nThe fixed script is:\n#!/bin/bash\n\nnextflow run nf-core/bactmap \\\n  -r \"1.0.0\" \\\n  -profile singularity \\\n  --input samplesheet.csv \\\n  --outdir results/bactmap \\\n  --reference resources/reference/MTBC0.fasta \\\n  --genome_size 4.3M\nWe ran the script as instructed using:\nbash scripts/03-run_bactmap.sh\nWhile it was running it printed a message on the screen:\nN E X T F L O W  ~  version 23.04.1\nLaunching `https://github.com/nf-core/bactmap` [cranky_swartz] DSL2 - revision: e83f8c5f0e [master]\n\n\n------------------------------------------------------\n                                        ,--./,-.\n        ___     __   __   __   ___     /,-._.--~'\n  |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n  | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                        `._,._,'\n  nf-core/bactmap v1.0.0\n------------------------------------------------------",
    "crumbs": [
      "Slides",
      "Reference-based assembly",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>The nf-core/bactmap pipeline</span>"
    ]
  },
  {
    "objectID": "materials/11-bactmap.html#bactmap-results",
    "href": "materials/11-bactmap.html#bactmap-results",
    "title": "13  The nf-core/bactmap pipeline",
    "section": "13.3 bactmap results",
    "text": "13.3 bactmap results\nAfter running the pipeline, we can look at the output directory in results/bactmap (if your pipeline finished running), or you can also use the preprocessed/bactmap results. There are various directories containing output files:\n\n\n\n\n\n\n\nDirectory\nDescription\n\n\n\n\nbwa/index\nContains the index of the reference sequence\n\n\nfastp\nContains the results of the trimming and adapter removal performed by fastp\n\n\nfastqc\nContains QC metrics for the fastq files generated with fastQC\n\n\nmultiqc\nContains a html file containing summaries of the various outputs\n\n\npipeline_info\nContains information about the pipeline run\n\n\npseudogenomes\nContains consensus fasta files for each sample which have the sample variants compared to the reference included. The alignment we’ll use for the next step can also be found in this directory (aligned_pseudogenomes.fas)\n\n\nrasusa\nContains the subsampled post-trimmed fastq files\n\n\nsamtools\nContains the sorted bam files and indices created by bwa and samtools as part of the mapping process\n\n\nsnpsites\nContains a variant alignment file created from aligned_pseudogenomes.fas with snp-sitesthat can be used as input for tree inference tools\n\n\nvariants\nContains filtered vcf files which contain the variants for each sample\n\n\n\n\n13.3.1 The MultiQC summary report\nThe first thing we’ll check is the HTML report file created by MultiQC. Open your File Explorer , navigate to preprocessed/bactmap/multiqc/ and double click on multiqc_report.html. This will open the file in your web browser of choice:\n\n\n\nconfig\n\n\n\nGeneral statistics\nLet’s go through each section starting with the “General Statistics”:\n\n\n\nnf-core/bactmap MultiQC General Statistics\n\n\nThis is a compilation of statistics collected from the outputs of tools such as fastp, samtools and BCFtools. Sequencing metrics such as the % of duplicated reads and GC content of the reads are shown alongside the results of the mapping (% reads mapped, num). This is a useful way of quickly identifying samples that are of lower quality or perhaps didn’t map very well due to species contamination.\n\n\nfastp\nThere are a number of plots showing the results of the fastp step in the pipeline. These plots are explained in The bacQC pipeline.\n\n\nSamtools\nThe plots in this section are created from the results of running samtool stats on the sorted bam files produce during the mapping process. The first shows the number or percentage of reads that mapped to the reference.\n\n\n\nnf-core/bactmap MultiQC samtools mapping\n\n\nThe second plot shows the overall alignment metrics for each sample. Hover over each dot to see more detailed information.\n\n\n\nnf-core/bactmap MultiQC samtools alignment\n\n\n\n\nBCFtools\nThe plots in this section provide information about the variants called using bcftools. The first plot shows the numbers or percentage of each type of variant in each sample.\n\n\n\nnf-core/bactmap MultiQC bcftools variants\n\n\nThe second plot shows the quality of each variant called by bcftools. The majority of variants in each sample are high quality.\n\n\n\nnf-core/bactmap MultiQC bcftools variant quality\n\n\nThe third plot shows the distribution of lengths of Indels (insertions are positive values and deletions are negative values). This is useful information to have, but in practice we tend to exclude indels when building alignments for phylogenetic tree building.\n\n\n\nnf-core/bactmap MultiQC bcftools indel distribution\n\n\nThe final bcftools plot shows the distribution of the number of reads mapping to each variant position and is one of the metrics used to filter out low quality variants (the fewer the reads mapping to a variant position, the lower the confidence we have that the variant is in fact real).\n\n\n\nnf-core/bactmap MultiQC bcftools variant depth\n\n\n\n\nSoftware versions\nThis section of the report shows the software run as part of nf-core/bactmap and the versions used. This is particularly important when reproducing the analysis on a different system or when writing the methods section of a paper.\n\n\n\nnf-core/bactmap MultiQC software versions",
    "crumbs": [
      "Slides",
      "Reference-based assembly",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>The nf-core/bactmap pipeline</span>"
    ]
  },
  {
    "objectID": "materials/11-bactmap.html#sec-seqtk",
    "href": "materials/11-bactmap.html#sec-seqtk",
    "title": "13  The nf-core/bactmap pipeline",
    "section": "13.4 Check how much of the reference was mapped",
    "text": "13.4 Check how much of the reference was mapped\nIt’s good practice to do an additional check of our mapping results before proceeding to the next step of our analysis, phylogenetic tree inference. We can do this by checking how much of the reference genome was mapped in each sample and use a tool called seqtk to do this. If a position in the reference genome is not found in the sample it is marked as a N and any regions of the reference that were not mapped to sufficient quality are marked as -. So we use seqtk comp to count the number of A’s, C’s, G’s and T’s, sum the totals and then divide by the length of the reference sequence. This gives us a proportion (we can convert to a % by multiplying by 100) of the reference sequence that was mapped in each sample. Ideally, we would like to see more than 90% of the reference mapped but this will depend on the species and how close the reference is to the samples you’re analysing. However, anything more than 75% should be sufficient for most applications. The main impact of less mapping is fewer phylogenetically informative positions for constructing phylogenetic trees.\nWe’ll start by activating the seqtk software environment to make seqtk available:\nmamba activate seqtk\nTo run seqtk comp on a single sample (in this example we’ll analyse ERX9450498_ERR9907670), the following commands can be used:\n# create output directory\nmkdir -p results/bactmap/pseudogenomes_check\n\n# run seqtk comp\nseqtk comp preprocessed/bactmap/pseudogenomes/ERX9450498_ERR9907670.fas &gt; results/bactmap/pseudogenomes_check/ERX9450498_ERR9907670.tsv\nIf you open the output file ERX9450498_ERR9907670.tsv in the terminal with cat:\ncat results/bactmap/pseudogenomes_check/ERX9450498_ERR9907670.tsv\nYou should see output like this:\nERX9450498_ERR9907670   4435783 715550  1348517 1343684 715401  0   0   312631  1040588 0   0   0\nThere are no headers in the output but the important information is contained in the first six columns:\n\nColumn 1 - our sample ID.\nColumn 2 - the total length of the sequence (this is the length of the reference).\nColumn 3 - the total number of ’A’s in the sequence.\nColumn 4 - the total number of ’C’s in the sequence.\nColumn 5 - the total number of ’G’s in the sequence.\nColumn 6 - the total number of ’T’s in the sequence.\n\nTo calculate the percentage of the reference mapped we divide the sum of ’A’s, ’C’s, ’G’s and ’T’s, divide by the total length of the sequence and multiply by 100:\n(715550+1348517+1343684+715401)/4435783 * 100 = 92.95%\nThis is more than 90% so we can proceed with the analysis of this sample.\n\n\n\n\n\n\nExerciseExercise 2 - How much of the reference was mapped?\n\n\n\n\n\n\nWe have calculated the percentage of the reference mapped for a single sample. However, we have five samples that we need to repeat the analysis on. To do this, we’ve provided a script that runs seqtk comp on all the samples in the pseudogenomes directory using a for loop.\n\nIn the folder scripts (inside your analysis directory), you’ll find a script named 04-pseudogenome_check.sh.\nOpen the script, which you will notice is composed of two sections:\n\n#### Settings #### where we define some variables for input and output files names. If you were running this script on your own data, you may want to edit the directories in this section.\n#### Analysis #### this is where seqtk comp is run on each sample as detailed in Section 13.4. You should not change the code in this section, although examining it is a good way to learn about running a for loop.\n\nActivate the software environment: mamba activate seqtk\nRun the script with bash scripts/04-pseudogenome_check.sh. If the script is running successfully it should print a message on the screen as the samples are processed.\nOnce the analysis finishes open the mapping_summary.tsv file in Excel from your file browser .\nSort the results by the %ref mapped column and identify the sample which has the lowest percentage of the reference mapped.\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nWe opened the script 04-pseudogenome_check.sh and these are the settings we used:\n\nfasta_dir=\"preprocessed/bactmap/pseudogenomes\" - the name of the directory with the pseudogenomes produced by bactmap in it.\noutdir=\"results/bactmap/pseudogenomes_check\" - the name of the directory where we want to save our results.\nparser=\"scripts/seqtk_parser.py\" - the path to a python script that takes the seqtk TSV files as input and does the calculation we performed above for all the samples.\n\nWe then ran the script using bash scripts/04-pseudogenome_check.sh. The script prints a message while it’s running:\nProcessing ERX9450498_ERR9907670.fas\nProcessing ERX9450499_ERR9907671.fas\nProcessing ERX9450502_ERR9907674.fas\n...\nWe opened the mapping_summary.tsv file in Excel and sorted the %ref mapped in ascending order to identify which sample had the lowest percentage of the reference mapped.\nsample  ref_length  #A  #C  #G  #T  mapped  %ref mapped\nERX9450498_ERR9907670   4435783 715550  1348517 1343684 715401  4123152 92.95206731258044\nERX9450499_ERR9907671   4435783 711436  1338950 1334043 711328  4095757 92.33447623564994\nERX9450502_ERR9907674   4435783 726446  1373546 1368600 726342  4194934 94.57031599607105\nERX9450504_ERR9907676   4435783 728780  1377603 1372394 728397  4207174 94.84625375046525\nERX9450506_ERR9907678   4435783 726076  1376188 1370635 726568  4199467 94.67250764972046\nWe can see that ERX9450520_ERR9907692 only mapped to 79.9% of the reference. Whilst this is less than the other samples, it’s still above 75% so we’ll include it for now.",
    "crumbs": [
      "Slides",
      "Reference-based assembly",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>The nf-core/bactmap pipeline</span>"
    ]
  },
  {
    "objectID": "materials/11-bactmap.html#create-final-alignment",
    "href": "materials/11-bactmap.html#create-final-alignment",
    "title": "13  The nf-core/bactmap pipeline",
    "section": "13.5 Create final alignment",
    "text": "13.5 Create final alignment\nNow that we’ve mapped our sequence data to the ancestral reference, called and filtered variants and created consensus pseudogenomes that we checked, we can create the final alignment we will use for inferring a phylogenetic tree. As we are not excluding any samples based on the pseudogenome check we did above, we can use the aligned_pseudogenomes.fas file that was created by bactmap (it’s worth remembering that this alignment includes the reference as well as the pseudogenomes). If any of the pseudogenomes contained more than 25% missing data and were removed, we could create our final alignment with cat as described below.\n\n\n\n\n\n\nTipTip: Building a final alignment from pseudogenome FASTA files\n\n\n\n\n\nOne of the advantages of working with pseudogenome FASTA files is that the files are all the same length i.e. the length of the reference. This means that they are effectively already aligned so we don’t need to do any additional aligning like we might do with gene sequences from different isolates. If you need to create a final alignment from the pseudogenome files, it’s as simple as using cat:\nFirst create a tmp directory and move the pseudogenome files you want to include to the tmp directory:\nmkdir tmp\n\nmv *.fas tmp\nNow change to the tmp directory and cat the pseudogenome files to create the alignment:\ncd tmp\n\ncat *.fas &gt; aligned_pseudogenomes.fas",
    "crumbs": [
      "Slides",
      "Reference-based assembly",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>The nf-core/bactmap pipeline</span>"
    ]
  },
  {
    "objectID": "materials/11-bactmap.html#mask-final-alignment",
    "href": "materials/11-bactmap.html#mask-final-alignment",
    "title": "13  The nf-core/bactmap pipeline",
    "section": "13.6 Mask final alignment",
    "text": "13.6 Mask final alignment\nIt’s standard practice to mask certain regions of the TB genome when we build alignments for building phylogenetic trees. In particular, repetitive regions of the genome such as PE/PPE genes and regions with an abundance of SNPs are removed as they are often difficult for the mapping software to accurately align and may cause misalignments leading to errors in variant calling with may lead to inaccurate tree topologies. There are two ways to do the masking:\n\nMask the regions in the VCF files before creating the pseudogenomes.\nCreate the final alignment then apply the masking to all the samples at once.\n\nAs we’ve already created our final alignment, we’re going to take the second approach and apply the mask to the aligned_pseudogenomes.fas file. The co-ordinates we’re going to use come from Goig et al. 2020 and have been transferred to the new ancestral reference sequence. We’ll use a tool called remove_blocks_from_aln.py to do the masking.\nLet’s start by activating the remove_blocks environment:\nmamba activate remove_blocks\nTo run remove_blocks_from_aln.py on aligned_pseudogenomes.fas, the following commands can be used:\n# create output directory\nmkdir -p results/bactmap/masked_alignment\n\n# remove_blocks_from_aln.py\nremove_blocks_from_aln.py -a preprocessed/bactmap/pseudogenomes/aligned_pseudogenomes.fas -t resources/masking/MTBC0_Goigetal_regions_toDiscard.bed -o results/bactmap/masked_alignment/aligned_pseudogenomes_masked.fas\nThe options we used are:\n\n-a - the alignment file we want to mask.\n-t - a BED file containing the coordinates of the regions we wish to mask.\n-o - the name of the masked alignment file.\n\nYou should see output like this:\nFound 1024 regions\nAdjusted 51 sequences\nOriginal alignment length: 4435783  New alignment length:4435783\nDone.\nThe masked final alignment will be saved to the results/bactmap/masked_alignment/ directory.\nAlternatively we’ve provided a script, 05-mask_pseudogenome.sh in the scripts directory which could be used instead with bash:\nbash scripts/05-mask_pseudogenome.sh",
    "crumbs": [
      "Slides",
      "Reference-based assembly",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>The nf-core/bactmap pipeline</span>"
    ]
  },
  {
    "objectID": "materials/11-bactmap.html#summary",
    "href": "materials/11-bactmap.html#summary",
    "title": "13  The nf-core/bactmap pipeline",
    "section": "13.7 Summary",
    "text": "13.7 Summary\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nThe nf-core/bactmap pipeline automates several steps to generate reference-based consensus genomes, such as: indexing the reference genome; quality-trimming reads; mapping the reads to the reference; calling variants; and generating the final consensus genomes.\nSeveral key output files include:\n\nAn interactive quality report located in multiqc/multiqc_report.html.\nFASTA files for each new pseudogenome in pseudogenomes/SAMPLE-NAME.fas.\nA single FASTA file with all the genomes concatenated in pseudogenomes/aligned_pseudogenomes.fas.\n\nKey quality control statistics output by the pipeline include the % of reads mapped to the reference, the number and type of called variants, amongst others.\nAdditional steps not included in the pipeline are:\n\nCalculating the fraction of missing bases in each pseudogenome.\nMasking repetitive regions of the genome.\n\n\n\n\n\nReferences\nGoig G, et al. Contaminant DNA in bacterial sequencing experiments is a major source of false genetic variability. BMC Biology. 2020. DOI",
    "crumbs": [
      "Slides",
      "Reference-based assembly",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>The nf-core/bactmap pipeline</span>"
    ]
  },
  {
    "objectID": "materials/12-phylogenetics.html",
    "href": "materials/12-phylogenetics.html",
    "title": "14  Building phylogenetic trees",
    "section": "",
    "text": "14.1 Phylogenetic tree inference\nA phylogenetic tree is a graph (structure) representing evolutionary history and shared ancestry. It depicts the lines of evolutionary descent of different species, lineages or genes from a common ancestor. A phylogenetic tree is made of nodes and edges, with one edge connecting two nodes.\nA node can represent an extant species, and extinct one, or a sampled pathogen: these are all cases of “terminal” nodes, nodes in the tree connected to only one edge, and usually associated with data, such as a genome sequence.\nA tree also contains “internal” nodes: these usually represent most recent common ancestors (MRCAs) of groups of terminal nodes, and are typically not associated with observed data, although genome sequences and other features of these ancestors can be statistically inferred. An internal node is most often connected to 3 branches (two descendants and one ancestral), but a multifurcation node can have any number &gt;2 of descendant branches.",
    "crumbs": [
      "Slides",
      "Phylogenetics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Building phylogenetic trees</span>"
    ]
  },
  {
    "objectID": "materials/12-phylogenetics.html#phylogenetic-tree-inference",
    "href": "materials/12-phylogenetics.html#phylogenetic-tree-inference",
    "title": "14  Building phylogenetic trees",
    "section": "",
    "text": "Example tree. The terminal nodes of this tree - A, B, C and D - represent sampled organisms. The internal nodes - E and F - are inferred from the data. In this case, there is also a multifurcation: nodes A, B and E all coalesce to the base of the tree. This can happen due to poor resolution in the data.\n\n\n\n14.1.1 Tree topology\nA clade is the set of all terminal nodes descending from the same ancestor. Each branch and internal node in a tree is associated with a clade. If two trees have the same clades, we say that they have the same topology. If they have the same clades and the same branch lengths, the two tree are equivalent, that is, they represent the same evolutionary history.\n\n\n14.1.2 Uses of phylogenetic trees\nIn many cases, the phylogenetic tree represents the end result of an analysis, for example if we are interested in the evolutionary history of a set of species.\nHowever, in many cases a phylogenetic tree represents an intermediate step, and there are many ways in which phylogenetic trees can be used to help understand evolution and the spread of infectious disease.\nIn other cases, we may want to know more about genome evolution, for example about mutational pressures, but more frequently about selective pressures. Selection can affect genome evolution in many ways such as slowing down evolution of a portion of the genome in which changes are deleterious (“purifying selection”). Conversely, “positive selection” can favor changes at certain positions of the genome, effectively accelerating their evolution. Using genome data and phylogenetic trees, molecular evolution methods can infer different types of selection acting in different parts of the genome and different branches of a tree.\n\n\n14.1.3 Newick format\nWe often need to represent trees in text format, for example to communicate them as input or output of phylogenetic inference software. The Newick format is the most common text format for phylogenetic trees.\nThe Newick format encloses each subtree (the part of a tree relating the terminal nodes part of the same clade) with parenthesis, and separates the two child nodes of the same internal node with a “,”. At the end of a Newick tree there is always a “;”.\nFor example, the Newick format of a rooted tree relating two samples “S1” and “S2”, with distances from the root respectively of 0.1 and 0.2, is\n(S1:0.1,S2:0.2);\nIf we add a third sample “S3” as an outgroup, the tree might become\n((S1:0.1,S2:0.2):0.3,S3:0.4);\n\n\n14.1.4 Methods for inferring phylogenetic trees\nA few different methods exist for inferring phylogenetic trees:\n\nDistance-based methods such as Neighbour-Joining and UPGMA\nParsimony-based phylogenetics\nMaximum likelihood methods making use of nuclotide substitution models\n\n\nDistance-based methods\nThese are the simplest and fastest phylogenetic methods we can use and are often a useful way to have a quick look at our data before running more robust phylogenetic methods. Here, we infer evolutionary distances from the multiple sequence alignment. In the example below there is 1 subsitution out of 16 informative columns (we exclude columns with gaps or N’s) so the distance is approximately 1/16:\n\n\n\nEvolutionary distance between two sequences\n\n\nTypically, we have multiple sequences in an alignment so here we would generate a matrix of pairwise distances between all samples (distance matrix) and then use Neighbour-Joining or UPGMA to infer our phylogeny:\n\n\n\nDistance matrix to Neighbour-Joining tree\n\n\n\n\nParsimony methods\nMaximum parsimony methods assume that the best phylogenetic tree requires the fewest number of mutations to explain the data (i.e. the simplest explanation is the most likely one). By reconstructing the ancestral sequences (at each node), maximum parsimony methods evaluate the number of mutations required by a tree then modify the tree a little bit at a time to improve it.\n\n\n\nExample of a maximum parsimony tree. In this case the tree topology on the left only requires one mutation to explain the data, whereas the tree on the right would require two mutations. Therefore, the maximum parsimony tree would be the one on the left.\n\n\nMaximum parsimony is an intuitive and simple method and is reasonably fast to run. However, because the most parsimonius tree is always the shortest tree, compared to the hypothetical “true” tree it will often underestimate the actual evolutionary change that may have occurred.\n\n\nMaximum likelihood methods\nThe most commonly encountered phylogenetic method when working with bacterial genome datasets is maximum likelihood. These methods use probabilistic models of genome evolution to evaluate trees and whilst similar to maximum parsimony, they allow statistical flexibility by permitting varying rates of evolution across different lineages and sites. This additional complexity means that maximum likelihood models are much slower than the previous two models discussed. Maximum likelihood methods make use of substitution models (models of DNA sequence evolution) that describe changes over evolutionary time. Two commonly used substitution models, Jukes-Cantor (JC69; assumes only one mutation rate) and Hasegawa, Kishino and Yano (HKY85; assumes different mutation rates - transitions have different rates) are depicted below:\n\n\n\nTwo commonly-used DNA substitution models\n\n\nIt is also possible to incorporate additional assumptions about your data e.g. assuming that a proportion of the the alignment columns (the invariant or constant sites) cannot mutate or that there is rate variation between the different alignment columns (columns may evolve at different rates). The choice of which is the best model to use is often a tricky one; generally starting with one of the simpler models e.g. General time reversible (GTR) or HKY is the best way to proceed. Accounting for rate variation and invariant sites is an important aspect to consider so using models like HKY+G4+I (G4 = four types of rate variation allowed; I = invariant sites don’t mutate) should also be considered.\nThere are a number of different tools for phylogenetic inference via maximum-likelihood and some of the most popular tools used for phylogenetic inference are FastTree, IQ-TREE and RAxML-NG. For this lesson, we’re going to use IQ-TREE as it is fast and has a large number of substitution models to consider. It also has a model finder option which tells IQ-TREE to pick the best fitting model for your dataset, thus removing the decision of which model to pick entirely.\n\n\n\n14.1.5 Tree uncertainty - bootstrap\nAll the methods for phylogenetic inference that we discussed so far aim at estimating a single realistic tree, but they don’t automatically tell us how confident we should be in the tree, or in individual branches of the tree.\nOne common way to address this limitation is using the phylogenetic bootstrap approach (Felsenstein, 1985). This consists of first sampling a large number (say, 1000) of bootstrap alignments. Each of these alignments has the same size as the original alignment, and is obtained by sampling with replacement the columns of the original alignment; in each bootstrap alignment some of the columns of the original alignment will usually be absent, and some other columns would be represented multiple times. We then infer a bootstrap tree from each bootstrap alignment. Because the bootstrap alignments differ from each other and from the original alignment, the bootstrap trees might different between each other and from the original tree. The bootstrap support of a branch in the original tree is then defined as the proportion of times in which this branch is present in the bootstrap trees.",
    "crumbs": [
      "Slides",
      "Phylogenetics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Building phylogenetic trees</span>"
    ]
  },
  {
    "objectID": "materials/12-phylogenetics.html#multiple-sequence-alignments",
    "href": "materials/12-phylogenetics.html#multiple-sequence-alignments",
    "title": "14  Building phylogenetic trees",
    "section": "14.2 Multiple sequence alignments",
    "text": "14.2 Multiple sequence alignments\nPhylogenetic methods require sequence alignments. These can range from alignments of a single gene from different species to whole genome alignments where a sample’s sequence reads are mapped to a reference genome. Alignments attempt to place nucleotides from the same ancestral nucleotide in the same column. One of the most commonly used alignment formats in phylogenetics is FASTA:\n&gt;Sample_1\nAA-GT-T\n&gt;Sample_2\nAACGTGT\nN and - characters represent missing data and are interpreted by phylogenetic methods as such.\nThe two most commonly used muliple sequence alignments in bacterial genomics are reference-based whole genome alignments and core genome alignments generated by comparing genes between different isolates and identifying the genes found in all or nearly all isolates (the core genome). As a broad rule of thumb, if your species is not genetically diverse and doesn’t recombine (TB, Brucella) then picking a suitable good-quality reference and generating a whole genome alignment is appropriate. However, when you have a lot of diversity or multiple divergent lineages (E. coli) then a single reference may not represent all the diversity in your dataset. Here it would be more typical to create de novo assemblies, annotate them and then use a tool like roary or panaroo to infer the pan-genome and create a core genome alignment. The same phylogenetic methods are then applied to either type of multiple sequence alignment.",
    "crumbs": [
      "Slides",
      "Phylogenetics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Building phylogenetic trees</span>"
    ]
  },
  {
    "objectID": "materials/12-phylogenetics.html#building-a-phylogenetic-tree",
    "href": "materials/12-phylogenetics.html#building-a-phylogenetic-tree",
    "title": "14  Building phylogenetic trees",
    "section": "14.3 Building a phylogenetic tree",
    "text": "14.3 Building a phylogenetic tree\nFollowing that very brief introduction to phylogenetics, we can start building our first phylogenetic tree using the masked alignment file we created in the previous section.\nWe start our analysis by activating our software environment, to make all the necessary tools available:\nmamba activate iqtree\n\n14.3.1 Extracting variable sites with SNP-sites\nAlthough you could use the alignment generated by bactmap directly as input to IQ-TREE, this would be quite computationally intensive, because the whole genome alignments tend to be quite large. Instead, what we can do is extract the variable sites from the alignment, such that we reduce our FASTA file to only include those positions that are variable across samples i.e. the positions that are phylogenetically informative.\nHere is a small example illustrating what we are doing. For example, take the following three sequences, where we see 3 variable sites (indicated with an arrow):\nseq1  C G T A G C T G G T\nseq2  C T T A G C A G G T\nseq3  C T T A G C A G A T\n        ↑         ↑   ↑\nFor the purposes of phylogenetic tree construction, we only use the variable sites to look at the relationship between our sequences, so we can simplify our alignment by extract only the variable sites:\nseq1  G T G\nseq2  T A G\nseq3  T A A\nThis example is very small, but when you have megabase-sized genomes, this can make a big difference. To extract variable sites from an alignment we can use the snp-sites software:\n# create output directory\nmkdir results/snp-sites\n\n# run SNP-sites\nsnp-sites results/bactmap/masked_alignment/aligned_pseudogenomes_masked.fas -o results/snp-sites/aligned_pseudogenomes_masked_snps.fas\nThis command simply takes as input the alignment FASTA file and produces a new file with only the variable sites - which we save (-o) to an output file. This is the file we will use as input to constructing our tree. However, before we move on to that step, we need another piece of information: the number of constant sites in the initial alignment (sites that didn’t vary in our alignment). Phylogenetically, it makes a difference if we have 3 mutations in 10 sites (30% variable sites, as in our small example above) or 3 mutations in 1000 sites (0.3% mutations). The IQ-TREE software we will use for tree inference can accept as input 4 numbers, counting the number of A, C, G and T that were constant in the alignment. For our small example above these would be, respectively: 1, 2, 2, 2.\nFortunately, the snp-sites command can also produce these numbers for us (you can check this in the help page by running snp-sites -h). This is how you would do this:\n# count invariant sites\nsnp-sites -C results/bactmap/masked_alignment/aligned_pseudogenomes_masked.fas &gt; results/snp-sites/constant_sites.txt\nThe key difference is that we use the -C option, which produces these numbers. We redirect (&gt;) the output to a file. We can see what these numbers are by printing the content of the file:\ncat results/snp-sites/constant_sites.txt\n692240,1310839,1306835,691662\nAs we said earlier, these numbers represent the number of A, C, G, T that were constant in our original alignment. We will use these numbers in the tree inference step detailed next.\n\n\n14.3.2 Tree inference with IQ-TREE\nThere are different methods for inferring phylogenetic trees from sequence alignments. Regardless of the method used, the objective is to construct a tree that represents the evolutionary relationships between different species or genetic sequences. Here, we will use the IQ-TREE software, which implements maximum likelihood methods of tree inference. IQ-TREE offers various sequence evolution models, allowing researchers to match their analyses to different types of data and research questions. Conveniently, this software can identify the most fitting substituion model for a dataset (using a tool called ModelFinder), while considering the complexity of each model.\nWe run IQ-TREE on the output from snp-sites, i.e. using the variable sites extracted from the core genome alignment:\n# create output directory\nmkdir results/iqtree\n\n# run iqtree2\niqtree \\\n  -s results/snp-sites/aligned_pseudogenomes_masked_snps.fas \\\n  -fconst 692240,1310839,1306835,691662 \\\n  --prefix results/iqtree/Nam_TB \\\n  -nt AUTO \\\n  -ntmax 8 \\\n  -mem 8G \\\n  -m GTR+F+I \\\n  -bb 1000\nThe options used are:\n\n-s - the input alignment file, in our case using only the variable sites extracted with snp-sites.\n--prefix - the name of the output files. This will be used to name all the files with a “prefix”. In this case we are using the “Nam_TB” prefix, which refers to the data we’re using.\n-fconst - these are the counts of invariant sites we estimated in the previous step with snp-sites (see previous section).\n-nt AUTO - automatically detect how many CPUs are available on the computer for parallel processing (quicker to run).\n-ntmax 8 - set the maximum number of CPUs to use\n-mem 8G - set the maximum amount of RAM to use\n-m - specifies the DNA substitution model we’d like to use. We give more details about this option below.\n-bb 1000 - run 1000 fast bootstraps. See the section on bootstrapping above.\n\nWhen not specifying the -m option, IQ-TREE employs ModelFinder to pinpoint the substitution model that best maximizes the data’s likelihood, as previously mentioned. Nevertheless, this can be time-consuming (as IQ-TREE needs to fit trees numerous times). An alternative approach is utilizing a versatile model, like the one chosen here, “GTR+F+I,” which is a generalized time reversible (GTR) substitution model. This model requires an estimate of the base frequencies within the sample population, determined in this instance by tallying the base frequencies from the alignment (indicated by “+F” in the model name). Lastly, the model accommodates variations in rates across sites, including a portion of invariant sites (noted by “+I” in the model name).\nWe can look at the output folder:\nls results/iqtree\nNam_TB.bionj   Nam_TB.log     Nam_TB.mldist\nNam_TB.ckp.gz  Nam_TB.iqtree  Nam_TB.treefile\nThere are several files with the following extensions:\n\n.iqtree - a text file containing a report of the IQ-Tree run, including a representation of the tree in text format.\n.treefile - the estimated tree in NEWICK format. We can use this file with other programs, such as FigTree, to visualise our tree.\n.log - the log file containing the messages that were also printed on the screen.\n.bionj - the initial tree estimated by neighbour joining (NEWICK format).\n.mldist - the maximum likelihood distances between every pair of sequences.\n.ckp.gz - this is a “checkpoint” file, which IQ-Tree uses to resume a run in case it was interrupted (e.g. if you are estimating very large trees and your job fails half-way through). \n\nThe main files of interest are the report file (.iqtree) and the tree file (.treefile) in standard Newick format.\n\n\n\n\n\n\nExerciseExercise 1 - Tree inference\n\n\n\n\n\n\nProduce a tree from the masked pseudogenome alignment from we created in the previous section.\n\nActivate the software environment: mamba activate iqtree.\nFix the script provided in scripts/06-run_iqtree.sh. See Section 14.3.2 if you need a hint of how to fix the code in the script.\nRun the script using bash scripts/06-run_iqtree.sh. Several messages will be printed on the screen while iqtree runs.\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nThe fixed script is:\n#!/bin/bash\n\n# create output directory\nmkdir -p results/snp-sites/\nmkdir -p results/iqtree/\n\n# extract variable sites\nsnp-sites results/bactmap/masked_alignment/aligned_pseudogenomes_masked.fas &gt; results/snp-sites/aligned_pseudogenomes_masked_snps.fas\n\n# count invariant sites\nsnp-sites -C results/bactmap/masked_alignment/aligned_pseudogenomes_masked.fas &gt; results/snp-sites/constant_sites.txt\n\n# Run iqtree\niqtree \\\n  -fconst $(cat results/snp-sites/constant_sites.txt) \\\n  -s results/snp-sites/aligned_pseudogenomes_masked_snps.fas \\\n  --prefix results/iqtree/Nam_TB \\\n  -nt AUTO \\\n  -ntmax 8 \\\n  -mem 8G \\\n  -m GTR+F+I \\\n  -bb 1000\n\nWe extract the variant sites and count of invariant sites using SNP-sites.\nAs input to both snp-sites steps, we use the aligned_pseudogenomes_masked_snps.fas produced in the previous step of our analysis.\nThe input alignment used in iqtree is the one from the previous step.\nThe number of constant sites was specified in the script as $(cat results/snp-sites/constant_sites.txt). This allows to directly add the contents of the constant_sites.txt file, without having to open the file to obtain these numbers.\nWe use as prefix for our output files “Nam_TB” (since we are using the “Namibian TB” data), so all the output file names will be named as such.\nWe automatically detect the number of threads/CPUs for parallel computation.\nWe specify the maximum amount of memory and threads/CPUs to use for computation.\n\nAfter the analysis runs we get several output files in our directory:\nls results/iqtree/\nNam_TB.bionj   Nam_TB.log     Nam_TB.mldist\nNam_TB.ckp.gz  Nam_TB.iqtree  Nam_TB.treefile\nThe main file of interest is Nam_TB.treefile, which contains our tree in the standard Newick format. We will root and then visualize this tree alongside relevant metadata in Visualising phylogenies.\n\n\n\n\n\n\n\n\n\n\n14.3.3 Rooting a phylogenetic tree\nRooting a phylogenetic tree is essential for making sense of evolutionary relationships and for providing a temporal context to the diversification of species. It transforms an unrooted tree, which simply shows relationships without direction, into a meaningful representation of evolutionary history. The most common way to accurately root a phylogenetic tree is to include an outgroup that is known to be more distantly related to the taxa included as part of the analysis. In our example we mapped our TB sequences to the MTBC0 reference, which is an outgroup to all members of the MTBC, so we’ll use this to root our tree before visualizing it. There are a few different tools that could be used to root a phylogenetic tree but we’ve provided a python script, root_tree.py to do this. You can run the script using the following command (we’re going to root the tree we’ve provided in the preprocessed directory so you don’t need to edit the command):\npython scripts/root_tree.py -i preprocessed/iqtree/Nam_TB.treefile -g MTBC0 -o results/iqtree/Nam_TB_rooted.treefile\nThe options we used are:\n\n-i - the TB phylogenetic tree we inferred with IQ-TREE.\n-g - the name of the outgroup to root the tree with (in this case MTBC0).\n-o - the rooted phylogenetic tree.\n\nThis will create a NEWICK file called Nam_TB_rooted.treefile in your results directory.",
    "crumbs": [
      "Slides",
      "Phylogenetics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Building phylogenetic trees</span>"
    ]
  },
  {
    "objectID": "materials/12-phylogenetics.html#summary",
    "href": "materials/12-phylogenetics.html#summary",
    "title": "14  Building phylogenetic trees",
    "section": "14.4 Summary",
    "text": "14.4 Summary\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nTree inference methods include neighbor-joining, maximum parsimony and maximum likelihood. The first two are simpler and computationally faster, but do not accurately capture relevant features of sequence evolution.\nMaximum likelihood methods are recommended, as they incorporate relevant parameters such as different substitution rates, invariant sites and variable mutation rates across the sequence.\nPhylogenetic tree inference requires a multiple sequence alignment as input, regardless of which method of inference is used.\nTo reduce the computational burden of the analysis when using whole-genome alignments, we can extract variable sites from our alignment using the snp-sites software.\nIQ-Tree is a popular software for maximum likelihood tree inference and can take as input the variable sites from the previous step.",
    "crumbs": [
      "Slides",
      "Phylogenetics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Building phylogenetic trees</span>"
    ]
  },
  {
    "objectID": "materials/13-tb_profiler.html",
    "href": "materials/13-tb_profiler.html",
    "title": "15  TB-Profiler",
    "section": "",
    "text": "15.1 Introduction\nTB-Profiler is a tool used to detect antimicrobial resistance and the lineages of MTBC genomes. It is made up of a pipeline which by default uses Trimmomatic to trim reads, aligns the reads to the H37Rv reference using bowtie2, BWA or minimap2 and then calls variants using bcftools. These variants are then compared to a drug-resistance database and a database of lineage-defining variants.\nThere is an online version of the tool which is very useful for analysing few genomes. You can try it out later at your free time by following this link.",
    "crumbs": [
      "Slides",
      "Phylogenetics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>TB-Profiler</span>"
    ]
  },
  {
    "objectID": "materials/13-tb_profiler.html#introduction",
    "href": "materials/13-tb_profiler.html#introduction",
    "title": "15  TB-Profiler",
    "section": "",
    "text": "TB-Profiler pipeline\n\n\n\n\n\n\n\n\nTipKeeping up to date\n\n\n\nNote that, like many other database-based tools TB-Profiler is under constant rapid development. If you plan to use the program in your work please make sure you are using the most up to date version! Similarly, the database is not static and is continuously being improved so make sure you are using the most latest version. If you use TBProfiler in your work please state the version of both the tool and the database as they are developed independently from each other.\n\n\n\n\n\n\nTB-Profiler online tool",
    "crumbs": [
      "Slides",
      "Phylogenetics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>TB-Profiler</span>"
    ]
  },
  {
    "objectID": "materials/13-tb_profiler.html#sec-tbprofiler",
    "href": "materials/13-tb_profiler.html#sec-tbprofiler",
    "title": "15  TB-Profiler",
    "section": "15.2 Running TB-Profiler on the command line",
    "text": "15.2 Running TB-Profiler on the command line\nGiven we have five TB genomes to analyse, we’re going to run TB-Profiler on the command line instead of uploading the FASTQ files to the web version, starting with a single sample ERX9450498_ERR9907670.\nWe’ll start by activating the software environment:\nmamba activate tb-profiler\nTo run TB-Profiler on ERX9450498_ERR9907670, the following commands can be used:\n# create output directory\nmkdir -p results/tb-profiler\n\n# run TB-Profiler\ntb-profiler profile -1 data/reads/ERX9450498_ERR9907670_1.fastq.gz -2 data/reads/ERX9450498_ERR9907670_2.fastq.gz -p ERX9450498_ERR9907670 -t 8 --csv -d results/tb-profiler 2&gt; results/tb-profiler/ERX9450498_ERR9907670.log\nThe options we used are:\n\n-1 - the read 1 FASTQ file.\n-2 - the read 2 FASTQ file.\n-p - the prefix of the output files, in this case the sample ID.\n-t - the number of CPUs to use.\n--csv - saves the output files in CSV format.\n-d - the name of the directory to save the results to.\n2&gt; - to help with identifying if a sample fails we redirect any error messages to a log file.\n\nWhile it was running it printed a message on the screen:\n[17:24:20] INFO     Using ref file: /home/ajv37/miniconda3/envs/tb-profiler/share/tbprofiler//tbdb.fasta                       db.py:795\n           INFO     Using gff file: /home/ajv37/miniconda3/envs/tb-profiler/share/tbprofiler//tbdb.gff                         db.py:795\n           INFO     Using bed file: /home/ajv37/miniconda3/envs/tb-profiler/share/tbprofiler//tbdb.bed                         db.py:795\n           INFO     Using version file: /home/ajv37/miniconda3/envs/tb-profiler/share/tbprofiler//tbdb.version.json            db.py:795\n           INFO     Using json_db file: /home/ajv37/miniconda3/envs/tb-profiler/share/tbprofiler//tbdb.dr.json                 db.py:795\n           INFO     Using variables file: /home/ajv37/miniconda3/envs/tb-profiler/share/tbprofiler//tbdb.variables.json        db.py:795\n           INFO     Using spoligotype_spacers file:                                                                            db.py:795\n                    /home/ajv37/miniconda3/envs/tb-profiler/share/tbprofiler//tbdb.spoligotype_spacers.txt                              \n           INFO     Using spoligotype_annotations file:                                                                        db.py:795\n                    /home/ajv37/miniconda3/envs/tb-profiler/share/tbprofiler//tbdb.spoligotype_list.csv                                 \n           INFO     Using bedmask file: /home/ajv37/miniconda3/envs/tb-profiler/share/tbprofiler//tbdb.mask.bed                db.py:795\n           INFO     Using barcode file: /home/ajv37/miniconda3/envs/tb-profiler/share/tbprofiler//tbdb.barcode.bed             db.py:795\n           INFO     Trimming reads\nYou can open the results file (results/tb-profiler/results/ERX9450498_ERR9907670.results.csv) with a spreadsheet software such as Excel from your file browser . There is a lot of information in this file but TB-Profiler helpfully provides a summary of the most useful information at the top:\nTBProfiler report   \n=================   \n    \nThe following report has been generated by TBProfiler.  \n    \nSummary \n------- \nID  ERX9450498_ERR9907670\nDate    Thu Nov  9 11:09:46 2023\nStrain  lineage4.3.4.1\nDrug-resistance MDR-TB\nMedian Depth    84\nThese key fields are:\n\nID - our sample ID.\nDate - the date and time TB-Profiler was run on this sample.\nStrain - the lineage of the sample.\nDrug-resistance - the drug resistance profile of the sample.\nMedian Depth - the median read depth coverage across the reference genome.\n\nFrom these results we can see that sample ERX9450498_ERR9907670 belongs to lineage 4.3.4.1 and is multi-drug resistant (MDR) i.e. resistant to more than one drug, in this case Rifampicin, Isoniazid and Ethambutol.",
    "crumbs": [
      "Slides",
      "Phylogenetics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>TB-Profiler</span>"
    ]
  },
  {
    "objectID": "materials/13-tb_profiler.html#tb-profiler-collate",
    "href": "materials/13-tb_profiler.html#tb-profiler-collate",
    "title": "15  TB-Profiler",
    "section": "15.3 TB-Profiler collate",
    "text": "15.3 TB-Profiler collate\nTB-Profiler has an option to collect the outputs from multiple samples and collate these together into a useful summary CSV file. We’re going to use this command in the exercise below.\n\n\n\n\n\n\nWarningTB-Profiler sometimes fails\n\n\n\nHopefully, we’ll be able to successfully run TB-Profiler on all our samples but, unfortunately, sometimes it’ll fail. If this does happen, identify which sample didn’t complete and re-run TB-Profiler. This is the reason we save the error messages to a log file.\n\n\n\n\n\n\n\n\nExerciseExercise 1 - Run TB-Profiler on all samples\n\n\n\n\n\n\nWe have run TB-Profiler on a single sample. However, we have five samples that we need to repeat the analysis on. To do this, we’ve provided a script that runs TB-Profiler on all the FASTQ files for all the samples in the data/reads directory using a for loop.\n\nIn the folder scripts (inside your analysis directory), you’ll find a script named 07-run_tb-profiler.sh.\nOpen the script, which you will notice is composed of two sections:\n\n#### Settings #### where we define some variables for input and output files names. If you were running this script on your own data, you may want to edit the directories in this section.\n#### Analysis #### this is where TB-Profiler is run on each sample as detailed in Section 15.2. You should not change the code in this section.\n\nActivate the software environment: mamba activate tb-profiler\nRun the script with bash scripts/07-run_tb-profiler.sh. If the script is running successfully it should print a message on the screen as the samples are processed.\nOnce the analysis finishes open the Nam_TB.txt file and copy/paste the data into Excel (we’ve provided the results for all 50 samples in the preprocessed directory).\nHow many Lineage 2 isolates are there in the dataset?\nAre any of the isolates antimicrobial-susceptible?\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nWe opened the script 07-run_tb-profiler.sh and these are the settings we used:\n\nfastq_dir=\"data/reads\" - the name of the directory with the read 1 and read 2 FASTQ files in it.\noutdir=\"results/tb-profiler\" - the name of the directory where we want to save our results.\nprefix=\"Nam_TB\" - the prefix for the output files created using tb-profiler collate.\n\nWe then ran the script using bash scripts/07-run_tb-profiler.sh. The script prints a message while it’s running:\nProcessing ERX9450498_ERR9907670.fas\n[17:24:20] INFO     Using ref file: /home/ajv37/miniconda3/envs/tb-profiler/share/tbprofiler//tbdb.fasta                       db.py:795\n           INFO     Using gff file: /home/ajv37/miniconda3/envs/tb-profiler/share/tbprofiler//tbdb.gff                         db.py:795\n           INFO     Using bed file: /home/ajv37/miniconda3/envs/tb-profiler/share/tbprofiler//tbdb.bed                         db.py:795\n           INFO     Using version file: /home/ajv37/miniconda3/envs/tb-profiler/share/tbprofiler//tbdb.version.json            db.py:795\n           INFO     Using json_db file: /home/ajv37/miniconda3/envs/tb-profiler/share/tbprofiler//tbdb.dr.json                 db.py:795\n           INFO     Using variables file: /home/ajv37/miniconda3/envs/tb-profiler/share/tbprofiler//tbdb.variables.json        db.py:795\n           INFO     Using spoligotype_spacers file:                                                                            db.py:795\n                    /home/ajv37/miniconda3/envs/tb-profiler/share/tbprofiler//tbdb.spoligotype_spacers.txt                              \n           INFO     Using spoligotype_annotations file:                                                                        db.py:795\n                    /home/ajv37/miniconda3/envs/tb-profiler/share/tbprofiler//tbdb.spoligotype_list.csv                                 \n           INFO     Using bedmask file: /home/ajv37/miniconda3/envs/tb-profiler/share/tbprofiler//tbdb.mask.bed                db.py:795\n           INFO     Using barcode file: /home/ajv37/miniconda3/envs/tb-profiler/share/tbprofiler//tbdb.barcode.bed             db.py:795\n           INFO     Trimming reads\n...\nFor this part, we opened the Nam_TB.txt file in the preprocessed/tb-profiler directory and copy/pasted the data into Excel (only the first four columns shown for brevity):\nsample  main_lineage    sub_lineage DR_type\nERX9450587_ERR9907759   lineage4    lineage4.3.4.1  RR-TB\nERX9450613_ERR9907785   lineage4    lineage4.3.2.1  RR-TB\nERX9450602_ERR9907774   lineage2    lineage2.2.1    RR-TB\nERX9450530_ERR9907702   lineage4    lineage4.3.2    MDR-TB\nERX9450610_ERR9907782   lineage4    lineage4.3.2    MDR-TB\nERX9450515_ERR9907687   lineage4    lineage4.3.2    MDR-TB\nERX9450606_ERR9907778   lineage4    lineage4.3.4.1  MDR-TB\nERX9450537_ERR9907709   lineage4    lineage4.3.4.1  MDR-TB\nERX9450548_ERR9907720   lineage4    lineage4.3.4.1  MDR-TB\nERX9450623_ERR9907795   lineage4    lineage4.3.4.1  MDR-TB\nWe can see that there were three Lineage 2 isolates in our dataset and none of the isolates are drug-susceptible which is not unexpected as these isolates were sequenced because they were at least resistant to rifampicin, one of the front-line drugs used to treat TB.",
    "crumbs": [
      "Slides",
      "Phylogenetics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>TB-Profiler</span>"
    ]
  },
  {
    "objectID": "materials/13-tb_profiler.html#data-cleaning",
    "href": "materials/13-tb_profiler.html#data-cleaning",
    "title": "15  TB-Profiler",
    "section": "15.4 Data cleaning",
    "text": "15.4 Data cleaning\nThe final thing we need to do before we visualize our TB phylogeny is to combine the metadata contained in sample_info.csv with the TB-profiler so we have some interesting information to annotate our tree with. We provide a python script to do this for you but you could, of course, do this yourself in Excel or using R. You can run the script using the following command (make sure to mamba activate tb-profiler):\npython scripts/merge_tb_data.py -s sample_info.csv -t preprocessed/tb-profiler/Nam_TB.txt\nThe options we used are:\n\n-s - the CSV file containing the metadata extracted from the publication.\n-t - the TB-Profiler summary TSV file (in this case the version in the preprocessed directory).\n\nThis will create a TSV file called TB_metadata.tsv in your analysis directory. We’ll use this file along with the TB phylogenetic tree in the next section.",
    "crumbs": [
      "Slides",
      "Phylogenetics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>TB-Profiler</span>"
    ]
  },
  {
    "objectID": "materials/13-tb_profiler.html#summary",
    "href": "materials/13-tb_profiler.html#summary",
    "title": "15  TB-Profiler",
    "section": "15.5 Summary",
    "text": "15.5 Summary\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nTB-profiler can be used to detect the presence of antimicrobial resistance genes in a genome, as well as assign it to its most likely lineage.\nTB-profiler can be run from the command line to process a single sample. Multiple samples can be conveniently processed using a for loop.\nTB-profiler outputs a CSV file with information about lineages and drug resistance genes.\nMultiple output files from TB-profiler can be combined with metadata of our samples to generate a TSV file to annotate phylogenetic trees.",
    "crumbs": [
      "Slides",
      "Phylogenetics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>TB-Profiler</span>"
    ]
  },
  {
    "objectID": "materials/14-tree_visualization.html",
    "href": "materials/14-tree_visualization.html",
    "title": "16  Visualising phylogenies",
    "section": "",
    "text": "16.1 Uploading tree files and metadata\nThere are many programs that can be used to visualise phylogenetic trees. Some of the popular programs include FigTree, iTOL and the R library ggtree. For this course, we’re going to use the web-based tool Microreact as it allows users to interactively manipulate the tree, add metadata and generate other plots including maps and histograms of metadata variables in a single interface.\nIn order to use this platform you will first need to create an account (or sign-in through your existing Google, Facebook or Twitter).\nOnce you’ve logged into Microreact, you can upload the rooted Namibian TB tree file (Nam_TB_rooted.treefile) and combined metadata TSV file (TB_metadata.tsv) we created earlier.",
    "crumbs": [
      "Slides",
      "Phylogenetics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Visualising phylogenies</span>"
    ]
  },
  {
    "objectID": "materials/14-tree_visualization.html#uploading-tree-files-and-metadata",
    "href": "materials/14-tree_visualization.html#uploading-tree-files-and-metadata",
    "title": "16  Visualising phylogenies",
    "section": "",
    "text": "Click on the UPLOAD link in the top-right corner of the page:\n\n\n\nClick the + button on the bottom-right corner then Browse Files to upload the files:\n\n\n\nThis will open a file browser, where you can select the tree file and metadata from your local machine. Go to the M_tuberculosis directory where you have the results we’ve generated so far this week. Click and select the Nam_TB_rooted.treefile and TB_metadata.tsv files while holding the Ctrl key. Click Open on the dialogue window after you have selected both files.\n\n\n\nA new dialogue box will open with files you’ve uploaded and the File kind which is done automatically by Microreact. As the File kind for both files is correct, go ahead and click CONTINUE:\n\n\n\nMicroreact will load the data and process it. The final step before we can have a look at the tree and annotate it is to confirm to Microreact which column in the metadata corresponds to the tip labels in the tree so it can match them. By default Microreact will use the first column, in this case sample which is correct so click CONTINUE:\n\n\n\nYou should now see three windows in front of you. The top-left has a map with the locations of your isolates based on the longitude and latitude values included in TB_metadata.tsv. The top-right has the phylogenetic tree with a separate colour for each tip (by default Microreact will colour the tips by BorstelID). Across the bottom you have the metadata from TB_metadata.tsv.\n\n\n\nThe first thing we’re going to do is change the colour of the tip nodes to Region. Click on the Eye icon in the top-right hand corner and change Colour Column to Region:\n\n\n\nThis will change the colour of the tip nodes as well as the pie charts on the map - each region has its own colour as you’d expect:\n\n\n\nAt this point, before we proceed any further, let’s save the project to your accounts. Click on the Save button in the top-right corner, change the project name to Namibia TB and add some kind of description so you know what the dataset is. Then click Save as a New Project (another dialogue box will appear asking if you want to share your project; for now, close this box):\n\n\n\nNow, let’s make our phylogenetic tree a bit more informative. First, let’s add the tip labels to the display by clicking on the left-hand of the two buttons in the phylogeny window then the drop down arrow next to Nodes & Labels. Now click the slider next to Leaf Labels and the slider next to Align Leaf Labels. We’ll also make the text a little smaller by moving the slider to 12px:\n\n\n\nThe tip labels are still the European Nuclotide Accessions we used to download the FASTQ files. Let’s change the tip labels to the BorstelID which is what’s used in the paper. Click on the Eye icon again and change Labels Column to BorstelID:\n\n\n\nFrom the rooted tree, we can see we have two distinct clades within the tree. These are the two major lineages we identified in our dataset (Lineage 2 and Lineage 4). To make this clearer, change the colour of the tip nodes to main_lineage and click on Legend on the far right-hand side of the plot. Now we have a tree and map annotated with the two lineages in our dataset:\n\n\n\nThe last thing we’re going to do is add a histogram to show the frequency of lineages across the different regions to our Microreact window. Click on the Pencil icon on the top-right corner and click Create New Chart then move your mouse into the right hand side of the metadata box at the bottom of the window and click when you see the blue box appear. A blank chart should appear. Click Chart Type and select Bar chart and change the X Axis Column to Region. The plot should auto populate with the region on the X-axis and the Number of entries on the Y-axis. The bars are coloured according to main_lineage which is what we’re currently using to colour our plots:\n\n\n\n\n\n\n\n\nTipLatitude and longitude for countries\n\n\n\nIf you have information about which countries your samples come from, you can obtain latitude and longitude coordinates by using the Data-flo web application. Data-flo provides several convenience data transformation applications, one of which is called “Geocoder”.\n\nGo to data-flo.io\nOn the top toolbar click on “Transformations”\nSearch for “Geocoder”\nClick on “Run”\nPaste your country names in the “Inputs” box (you can copy these from your metadata table)\nClick “Run”\nOnce it finishes, scroll all the way down and click on the file link “locations.csv” to download a file containing all the country coordinates\nYou can then open your original metadata file and add these two columns to your metadata\n\nOnce you have this information, it will be possible to display your samples on a map using Microreact.",
    "crumbs": [
      "Slides",
      "Phylogenetics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Visualising phylogenies</span>"
    ]
  },
  {
    "objectID": "materials/14-tree_visualization.html#summary",
    "href": "materials/14-tree_visualization.html#summary",
    "title": "16  Visualising phylogenies",
    "section": "16.2 Summary",
    "text": "16.2 Summary\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nMicroreact is a free web app to visualise phylogenetic trees.\nIt supports tree files in standard NEWICK format, as output by IQ-Tree.\nIt also supports metadata for the samples, which can be used to configure the tree.\nMetadata such as latitude and longitude is also used to display the samples on a map.",
    "crumbs": [
      "Slides",
      "Phylogenetics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Visualising phylogenies</span>"
    ]
  },
  {
    "objectID": "materials/15-group_exercise_1.html",
    "href": "materials/15-group_exercise_1.html",
    "title": "17  Know your Audience",
    "section": "",
    "text": "TipLearning Objectives\n\n\n\n\nDiscuss how the presentation and visualisation of your results should be tailored to your target audience(s).\nUse Microreact to customise visualisations accordingly.\n\n\n\nPresenting your data to different audiences is an important part of being a scientist and you should be able to tailor your research outputs accordingly. In this group exercise, you will use Microreact to design an infographic that displays the data we’ve been working with. Your presentation should be tailored to one of the following audiences:\n\nField epidemiologist\nHead of a public health lab\nMinister of Health\nConcerned citizen\n\nParticipants will be split into groups and assigned one of the audiences. We suggest that one person in each group is responsible for manipulating the Microreact, whilst the other members of the group provide useful input. After 30 minutes we’ll ask you to nominate a member of your group to do a two minute presentation of the data.\nRemember: your presentation should be tailored to your audience, avoiding jargon where appropriate.",
    "crumbs": [
      "Slides",
      "Phylogenetics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Know your Audience</span>"
    ]
  },
  {
    "objectID": "materials/16-dating.html",
    "href": "materials/16-dating.html",
    "title": "18  Estimating time-scaled phylogenies",
    "section": "",
    "text": "18.1 Time-scaled phylogenies\nTime-scaled phylogenetics is an approach in evolutionary biology that integrates temporal data into the construction of phylogenetic trees, providing a framework to estimate the timing of evolutionary events. This method combines molecular sequence data with collection dates and known mutation rates to produce trees where branch lengths represent time, rather than merely genetic change. By doing so, it allows researchers to infer not only the relationships between different species but also the chronological sequence of divergence events. This approach can uncover insights into the rates of evolution, the timing of speciation events, and the impact of historical environmental changes on evolutionary processes.\nThe incorporation of time into phylogenetic analysis enhances our understanding of the evolutionary timeline and facilitates more accurate reconstructions of ancestral states. For example, it enables the estimation of the age of the most recent common ancestor of a group of species, offering a temporal perspective on the diversification of lineages. Time-scaled phylogenetics is crucial for fields such as paleontology, where it can help correlate fossil records with molecular data, and for biogeography, where it aids in understanding the temporal patterns of species distributions. By integrating genetic, paleontological, and geochronological data, this method provides a comprehensive view of evolutionary history that is essential for unraveling the complexities of life’s past.",
    "crumbs": [
      "Slides",
      "Phylogenetics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Estimating time-scaled phylogenies</span>"
    ]
  },
  {
    "objectID": "materials/16-dating.html#time-scaled-phylogenies",
    "href": "materials/16-dating.html#time-scaled-phylogenies",
    "title": "18  Estimating time-scaled phylogenies",
    "section": "",
    "text": "Bayesian maximum clade credibility tree of 261 MTBC genomes with estimated divergence dates shown in years before present (Bos et al. 2014)",
    "crumbs": [
      "Slides",
      "Phylogenetics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Estimating time-scaled phylogenies</span>"
    ]
  },
  {
    "objectID": "materials/16-dating.html#tools-for-estimating-time-scaled-phylogenies",
    "href": "materials/16-dating.html#tools-for-estimating-time-scaled-phylogenies",
    "title": "18  Estimating time-scaled phylogenies",
    "section": "18.2 Tools for estimating time-scaled phylogenies",
    "text": "18.2 Tools for estimating time-scaled phylogenies\nThe most commonly used tools for estimating time-scaled phylogenies include BEAST (Bayesian Evolutionary Analysis Sampling Trees), MrBayes, and RAxML. BEAST is a powerful software package that uses Bayesian inference to estimate phylogenies and divergence times simultaneously, incorporating molecular clock models and various priors on the rates of evolution. It is particularly well-suited for complex datasets and allows for the incorporation of different types of data, such as molecular sequences and fossil calibrations. MrBayes, another Bayesian inference tool, is also widely used for phylogenetic analysis and can estimate time-scaled trees by applying relaxed or strict molecular clocks. RAxML (Randomized Axelerated Maximum Likelihood), although primarily a maximum likelihood-based tool, has features for dating phylogenies when combined with other tools that can handle molecular clock models.\nDespite its widespread use and powerful capabilities, BEAST has several drawbacks when estimating time-scaled phylogenies. One significant challenge is its computational intensity; BEAST requires substantial processing power and time, especially for large datasets or complex models, making it less accessible for researchers without high-performance computing resources. Additionally, BEAST’s reliance on Bayesian inference means that results can be highly sensitive to the choice of priors, which requires careful consideration and can introduce subjective bias. The complexity of the software also presents a steep learning curve for new users, necessitating substantial expertise to correctly implement and interpret analyses. These limitations highlight the need for cautious application and interpretation of BEAST’s results in phylogenetic studies.",
    "crumbs": [
      "Slides",
      "Phylogenetics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Estimating time-scaled phylogenies</span>"
    ]
  },
  {
    "objectID": "materials/16-dating.html#assessing-molecular-clock-signal",
    "href": "materials/16-dating.html#assessing-molecular-clock-signal",
    "title": "18  Estimating time-scaled phylogenies",
    "section": "18.3 Assessing molecular clock signal",
    "text": "18.3 Assessing molecular clock signal\nEstimating a molecular clock signal in genome data involves determining the rate at which genetic mutations accumulate over time, providing a “clock” to date evolutionary events. We do this by extracting the root-to-tip distances for each genome in a phylogenetic tree (“genetic divergence”) and plot this against the collection date for each genome. A positive correlation between genetic divergence and time indicates a molecular clock signal with the slope being the mutation rate over time and the x-intercept the inferred date of the MRCA. It’s worth remembering that this analysis is designed to provide an indication of a molecular clock signal in your data and a shallow slope doesn’t necessarily mean that you may not be able to infer a robust time-scaled phylogenetic tree.\n\n\n\nExample of a root-to-tip distance plot to assess molecular clock signal (van Tonder et al. 2021)",
    "crumbs": [
      "Slides",
      "Phylogenetics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Estimating time-scaled phylogenies</span>"
    ]
  },
  {
    "objectID": "materials/16-dating.html#treetime",
    "href": "materials/16-dating.html#treetime",
    "title": "18  Estimating time-scaled phylogenies",
    "section": "18.4 TreeTime",
    "text": "18.4 TreeTime\nTreeTime is a computational tool designed to estimate time-scaled phylogenies with a focus on efficiency and simplicity. It employs a maximum likelihood framework to integrate molecular sequence data with temporal information, such as sampling dates, to infer the timing of evolutionary events. TreeTime optimizes the placement of mutations along the phylogenetic tree while simultaneously adjusting branch lengths to reflect chronological time. This method leverages a relaxed molecular clock model, allowing for variations in the rate of evolution across different branches. By combining sequence data with temporal constraints, TreeTime rapidly produces time-calibrated phylogenies that are both accurate and computationally efficient, making it particularly useful for analyzing large datasets, such as those encountered in viral and bacterial evolution studies. Its user-friendly interface and robust performance make TreeTime an accessible and valuable tool for researchers aiming to elucidate the temporal dynamics of evolutionary processes.\n\n18.4.1 Running TreeTime\nWe’re going to run TreeTime on the rooted phylogenetic tree of our Namibian TB genomes (see Building phylogenetic trees - Rooting a phylogenetic tree). As well as the tree, we also need the masked alignment we created in The nf-core/bactmap pipeline and, as we’re inferring a time-scaled phylogenetic tree, the sample collection dates that can be found in the TB_metadata.tsv file.\nBefore we run TreeTime, we need to remove the outgroup MTBC0 from both the alignment and the rooted phylogenetic tree (you can find all this code in the script 08-run_treetime.sh):\nmamba activate treetime\n\n# create output directory\nmkdir -p results/treetime/\n\n# Remove outgroup from alignment\nseqkit grep -v -p MTBC0 preprocessed/bactmap/masked_alignment/aligned_pseudogenomes_masked.fas &gt; results/treetime/aligned_pseudogenomes_masked_no_outgroups.fas \n\n# Remove outgroup from rooted tree\npython scripts/remove_outgroup.py -i results/iqtree/Nam_TB_rooted.treefile -g MTBC0 -o results/treetime/Nam_TB_rooted_no_outgroup.treefile\nNow we can run TreeTime:\n# Run TreeTime\ntreetime --tree results/treetime/Nam_TB_rooted_no_outgroup.treefile \\\n        --dates TB_metadata.tsv \\\n        --name-column sample \\\n        --date-column Date.sample.collection \\\n        --aln results/treetime/aligned_pseudogenomes_masked_no_outgroups.fas \\\n        --outdir results/treetime \\\n        --report-ambiguous \\\n        --time-marginal only-final \\\n        --clock-std-dev 0.00003 \\\n        --relax 1.0 0\nThe options used are:\n\n--tree results/treetime/Nam_TB_rooted_no_outgroup.treefile - the rooted phylogenetic tree with the outgroup removed.\n--dates TB_metadata.tsv - a TSV file containing the sample collection dates.\n--name-column sample - the column within the TSV file that contains the sample names (this needs to match the names in the tree).\n--date-column Date.sample.collection - the column within the TSV file that contains the sample collection dates.\n--aln results/treetime/aligned_pseudogenomes_masked_no_outgroups.fas - the masked whole genome alignment with the outgroup removed.\n--outdir results/treetime - directory to save the output files to.\n--report-ambiguous - include transitions involving ambiguous states.\n--time-marginal only-final - assigns nodes to their most likely divergence time after integrating over all possible configurations of the other nodes.\n--clock-std-dev 0.00003 - standard deviation of the provided clock rate estimate.\n--relax 1.0 0 - use an autocorrelated molecular clock. Coupling 0 (–relax 1.0 0) corresponds to an un-correlated clock.\n\nWe can look at the output folder:\nls results/treetime\naligned_pseudogenomes_masked_no_outgroups.fas  dates.tsv                           root_to_tip_regression.pdf    timetree.pdf\nancestral_sequences.fasta                      divergence_tree.nexus               sequence_evolution_model.txt  trace_run.log\nauspice_tree.json                              molecular_clock.txt                 substitution_rates.tsv\nbranch_mutations.txt                           Nam_TB_rooted_no_outgroup.treefile  timetree.nexus\nThere are several files produced by TreeTime but the most useful ones for our purposes are:\n\nroot_to_tip_regression.pdf - contains a plot showing the correlation between root to tip distance and collection date.\ntimetree.pdf - contains a plot showing the time-scaled phylogenetic tree produced by TreeTime.\ntimetree.nexus - contains the time-scaled phylogeny in NEXUS format.\n\n\n\n\n\n\n\nExerciseExercise 1 - Build a time-scaled phylogenetic tree\n\n\n\n\n\n\nWe’ve provided a script, 08-run_treetime.sh, to remove the outgroup from the alignment and tree and then run TreeTime.\n\nActivate the software environment: mamba activate treetime.\nRun the script with bash scripts/08-run_treetime.sh. If the script is running successfully it should print a message on the screen as TreeTime constructs a time-scaled phylogenetic tree.\nAssess the strength of the molecular clock signal in the data.\nExamine the dates estimated by TreeTime. Do these seem reasonable based on what we know about the MTBC?\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nWe ran the script using bash scripts/08-run_treetime.sh. The script prints a message while it’s running:\n0.00    -TreeAnc: set-up\n\n153.00  TreeTime.reroot: with method or node: least-squares\n\n153.00  TreeTime.reroot: rerooting will ignore covariance and shared ancestry.\n\n153.04  TreeTime.reroot: with method or node: least-squares\n\n153.04  TreeTime.reroot: rerooting will ignore covariance and shared ancestry.\n\n164.17  ###TreeTime.run: INITIAL ROUND\n...\nWe opened the root_to_tip_regression.pdf file in the results/treetime directory:\n\n\n\nRoot-to-tip correlation\n\n\nWe can see that the correlation between root-to-tip distance and collection date isn’t particularly strong. However, the inferred mutation rate (the slope) is 1.14e-06 mutations per site per year (~5 mutations per genome per year) which is around 10 times as fast as published estimates for the mutation rate of TB. This is the mutation rate that TreeTime uses as its prior so we should bear that in mind when we consider the dates in the time-scaled phylogenetic tree.\nThen we looked at the timetree.pdf file:\n\n\n\nTime-scaled phylogenetic tree\n\n\nWe can see that the date of the MRCA for the genomes we included in our analysis is around 1906. Given this is the MRCA of the L2 and L4 genomes in our dataset, this likely to be a gross under-estimate for this date (Look at the Bos tree in the introduction which inferred the split between L2 and L4 to have occurred at least 1779 years before present). The likely reasons for this result include the small number of genomes we’ve included with a small range for the collection dates, the high mutation rate taken from the root-to-tip correlation plot and the fact that we’ve included three genomes from L2 in the dataset. The analysis was run this way to make use of the analysis we’ve already done thus far so you’d want to give more careful consideration when deciding which genomes to include when building a time-scaled phylogenetic tree. For instance, it may have been better to have removed the L2 genomes and just calculate the time-scaled evolutionary history of the the Namibian L4 genomes.\n\n\n\n\n\n\n\n\n\n\n18.4.2 Visualizing time-scaled phylogenetic trees\nNow that we’ve generated a time-scaled phylogenetic tree with TreeTime, we can use FigTree to visualize the tree (the PDF produced by TreeTime doesn’t include the sample ids and we can’t edit it). You can open FigTree from the terminal by running the command figtree.\nTo open the tree:\n\nGo to File &gt; Open… and browse to the results/treetime folder with the TreeTime output files.\nSelect the timetree.nexus file and click Open. You will be presented with a visual representation of the tree:\n\n\n\nFirst, let’s add the time scale on the bottom. Click the arrow and Tick box next to Scale Axis. Then click the tick box next to Reverse axis:\n\n\n\nNow we need to correct the scale to account for our sample collection dates. Click the arrow next to Time Scale and input 2017.8 (the date of the most recently sampled genome)in the box next to Offset by:",
    "crumbs": [
      "Slides",
      "Phylogenetics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Estimating time-scaled phylogenies</span>"
    ]
  },
  {
    "objectID": "materials/16-dating.html#summary",
    "href": "materials/16-dating.html#summary",
    "title": "18  Estimating time-scaled phylogenies",
    "section": "18.5 Summary",
    "text": "18.5 Summary\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nTemporal data can be incorporated into the inference of phylogenetic trees to provide estimates for dates of evolutionary events.\nThe most commonly used tools to infer time-scaled phylogenies are Bayesian and often require significant computational resources as well as time to run.\nMolecular clock signal in your data can be assessed by looking for a positive correlation between sampling date and root-to-tip distance.\nTreeTime is a computational tool designed to estimate time-scaled phylogenies with a focus on efficiency and simplicity.\nFigTree can be used to visualize time-scaled phylogenetic trees.",
    "crumbs": [
      "Slides",
      "Phylogenetics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Estimating time-scaled phylogenies</span>"
    ]
  },
  {
    "objectID": "materials/16-dating.html#references",
    "href": "materials/16-dating.html#references",
    "title": "18  Estimating time-scaled phylogenies",
    "section": "18.6 References",
    "text": "18.6 References",
    "crumbs": [
      "Slides",
      "Phylogenetics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Estimating time-scaled phylogenies</span>"
    ]
  },
  {
    "objectID": "materials/17-transmission.html",
    "href": "materials/17-transmission.html",
    "title": "19  Building transmission networks",
    "section": "",
    "text": "19.1 Transmission networks in bacteria\nGenomic data can be utilized to identify and understand the transmission of bacterial pathogens. In particular, SNP thresholds can be used to infer bacterial transmission networks by analyzing the genetic similarity between bacterial isolates from different individuals or sources. The underlying principle is that closely related bacterial strains will share a higher number of SNPs due to their recent common ancestry, while more distantly related strains will have accumulated more genetic differences over time.\nPutative transmission can be inferred by establishing a threshold for the number of SNPs that can be considered to indicate a transmission link. This threshold can be based on epidemiological data, previous studies, or statistical models. These links can then be used to identify clusters and potential transmission network.\nThe SNP threshold for inferring transmission will vary depending on the organism, the population structure, and the specific context of the study. It is often determined empirically by analyzing the distribution of pairwise SNP differences among strains and identifying a natural cutoff point. Alternatively, thresholds can be based on simulation studies that incorporate information about the mutation rate of the organism and likely routes of transmission.",
    "crumbs": [
      "Slides",
      "Transmission",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Building transmission networks</span>"
    ]
  },
  {
    "objectID": "materials/17-transmission.html#transmission-networks-in-bacteria",
    "href": "materials/17-transmission.html#transmission-networks-in-bacteria",
    "title": "19  Building transmission networks",
    "section": "",
    "text": "19.1.1 Identifying transmission networks in TB\nAs mentioned, the SNP threshold used to define a cluster can vary depending on the study and the population being analyzed. In TB, there have been a number of different thresholds applied when trying to identify putative transmission clusters, with the most commonly applied thresholds being 5, 10 and 12 SNPs.\nFor example the 12 SNP threshold is used for inferring likely transmission between a pair of TB cases by the UK Health Security Agency (UKHSA) and is the threshold applied in the publication describing the data we’ve been analysing (Walker 2014, Claasens 2022). Given the slow mutation of M. tuberculosis, a stricter threshold of 5 SNPs is usually used to infer recent transmission. In the exercise to follow, we will use both thresholds to examine the effect they have on the transmission networks we infer.",
    "crumbs": [
      "Slides",
      "Transmission",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Building transmission networks</span>"
    ]
  },
  {
    "objectID": "materials/17-transmission.html#generating-a-pairwise-snp-distance-matrix",
    "href": "materials/17-transmission.html#generating-a-pairwise-snp-distance-matrix",
    "title": "19  Building transmission networks",
    "section": "19.2 Generating a pairwise SNP distance matrix",
    "text": "19.2 Generating a pairwise SNP distance matrix\nThe first step in building putative transmission networks is to calculate the pairwise SNP distances between all the samples in our dataset and we can do this by running a tool call pairsnp on the SNP alignment we used to build our phylogenetic tree.\nWe’ll start by activating the pairsnp software environment:\nmamba activate pairsnp\nTo run pairsnp on aligned_pseudogenomes_masked_snps.fas, the following commands can be used:\n# create output directory\nmkdir -p results/transmission/\n\n# run pairsnp\npairsnp preprocessed/snp-sites/aligned_pseudogenomes_masked_snps.fas -c &gt; results/transmission/aligned_pseudogenomes_masked_snps.csv\nThe option we used is:\n\n-c - saves the pairsnp output in CSV format.\n\nThe pairwise SNP matrix will be saved to the results/transmission/ directory.\nAlternatively we’ve provided a script, 09-run_pairsnp.sh in the scripts directory which could be used instead with bash:\nbash scripts/09-run_pairsnp.sh",
    "crumbs": [
      "Slides",
      "Transmission",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Building transmission networks</span>"
    ]
  },
  {
    "objectID": "materials/17-transmission.html#calculating-and-plotting-transmission-networks-in-r",
    "href": "materials/17-transmission.html#calculating-and-plotting-transmission-networks-in-r",
    "title": "19  Building transmission networks",
    "section": "19.3 Calculating and plotting transmission networks in R",
    "text": "19.3 Calculating and plotting transmission networks in R\nNow that we’ve generated a pairwise SNP distance matrix, we can use R to calculate and plot our transmission network using a pre-determined threshold of 5 SNPs to identify putative transmission events. Open RStudio then open the script 10-transmission.R in the scripts directory. Run the code in the script, going line by line (remember in RStudio you can run code from the script panel using Ctrl + Enter). As you run the code check the tables that are created (in your “Environment” panel on the top-right) and see if the SNP matrix was correctly imported. Once you reach the end of the script, you should have created a plot showing the putative transmission networks identified in the data with the nodes coloured by Sex and the pairwise SNP distances shown along the edges:\n\n\n\nPutative transmission networks generated using a 5 SNP threshold\n\n\n\n\n\n\n\n\nExerciseExercise 1 - Adjust the pairwise SNP threshold\n\n\n\n\n\n\nAs discussed in the introduction above, various SNP thresholds are used when inferring putative transmission networks in TB. We used the most conservative threshold of 5 SNPs. For this exercise:\n\nOn your R script, change the SNP threshold from 5 to 12 SNPs and run the following code again to recalculate the transmission networks.\nIn your final network graph, change the colour of the nodes to show “Region” instead of “Sex”.\nHow many additional networks did we infer compared to using a threshold of 12 SNPs?\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\n\nWe changed the variable holding the threshold value to 12 (threshold &lt;- 12) then re-ran the subsequent code to generate new networks.\n\nIn the command to generate the final plot, we changed geom_node_point(aes(colour = Sex), size = 6) to geom_node_point(aes(colour = Region), size = 6) and changed labs(colour = \"Sex\") to labs(colour = \"Region\").\nWe generated one additional network but identified a much more complex network comprised of 13 isolates when using the higher SNP threshold of 12.\n\n\n\n\nPutative transmission networks generated using a 12 SNP threshold",
    "crumbs": [
      "Slides",
      "Transmission",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Building transmission networks</span>"
    ]
  },
  {
    "objectID": "materials/17-transmission.html#summary",
    "href": "materials/17-transmission.html#summary",
    "title": "19  Building transmission networks",
    "section": "19.4 Summary",
    "text": "19.4 Summary\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nCalculating the number of shared SNPs between genome sequences can be used to infer which isolates come from the same transmission event.\nA threshold can be set for the number of shared SNPs that is used to consider isolates to come from the same transmission event.\nThe precise threshold used varies depending on the study system and population structure.\nThresholds can be empirically determined from the data or from simulation studies.\nGenerally, a lower threshold results in smaller clusters and is suitable for recent transmission events. Higher thresholds are useful for transmission events spread across a longer span of time.\nThe software pairsnp can be used to calculate SNP distance matrices.\nTransmission networks can be visualised as a graph, using data analysis software such as R.",
    "crumbs": [
      "Slides",
      "Transmission",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Building transmission networks</span>"
    ]
  },
  {
    "objectID": "materials/17-transmission.html#references",
    "href": "materials/17-transmission.html#references",
    "title": "19  Building transmission networks",
    "section": "19.5 References",
    "text": "19.5 References\nWalker TM, Lalor MK, Broda A, Ortega LS, Morgan M, Parker L, Churchill S, Bennett K, Golubchik T, Giess AP, Del Ojo Elias C, Jeffery KJ, Bowler ICJW, Laurenson IF, Barrett A, Drobniewski F, McCarthy ND, Anderson LF, Abubakar I, Thomas HL, Monk P, Smith EG, Walker AS, Crook DW, Peto TEA, Conlon CP. Assessment of Mycobacterium tuberculosis transmission in Oxfordshire, UK, 2007-12, with whole pathogen genome sequences: an observational study. Lancet Respir Med. 2014. DOI\nClaasens M, et al. Whole-Genome Sequencing for Resistance Prediction and Transmission Analysis of Mycobacterium tuberculosis Complex Strains from Namibia. Microbiology Spectrum. 2022. DOI",
    "crumbs": [
      "Slides",
      "Transmission",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Building transmission networks</span>"
    ]
  },
  {
    "objectID": "materials/18-intro_staph.html",
    "href": "materials/18-intro_staph.html",
    "title": "20  Introduction to Staphylococcus aureus",
    "section": "",
    "text": "20.1 Staphylococcus aureus\nStaphylococcus aureus is an important human pathogen that colonises approximately 30% of the population. It is a major cause of multiple infections with varying degrees of severity including skin and soft tissue, bone and joint, endocarditis, device and prosthesis, line-related, pneumonia, and bacteraemia. Mortality from bacteraemia is reported at approximately 30% (Bai 2022) with approximately 14,000 cases from 2022 to 2023 (UKHSA 2023). Antimicrobial drug resistance (AMR) is an important factor in increasing mortality resulting from S. aureus infections. In the global burden of disease study from 2019, S. aureus was the second highest contributing pathogen to AMR-related deaths (Lancet 2022). Meticillin resistant S. aureus (MRSA) is a primary concern of resistance amongst S. aureus infections and accounted for the pathogen-drug combination, across all bacteria, with the highest attributable deaths, whilst resulting in a higher mortality when compared to methicillin sensitive S. aureus (MSSA) (UKHSA 2023).\nGenomic epidemiological studies over the last two decades have provided significant insights into the transmission dynamics of S. aureus and aided targeted control strategies. Whole-genome sequences from S. aureus isolates derived from hospitals and the community have provided insights outbreaks related to critical care wards, care homes, intravenous drug user networks and schools (Harris, 2013, Coll 2017, Marks 2021, van Tonder 2023). It has helped detail the emergence of human-adapted clades, such as CC398, facilitated a one-health approach to understanding transmission, and identified key virulence traits (Young 2012, Uhlemann 2017, Larsen 2022).\nThere is an increasing body of evidence that routine use of whole-genome sequencing can aid infection prevention control and public health decision making, and this is being increasingly investigated and implemented (Durand 2018). That said, there is a significant disparity in sequences generated between regions and an under-representation of sequences from MSSA, which can bias our understanding of the population dynamics and evolutionary selection pressures shaping S. aureus populations.",
    "crumbs": [
      "Slides",
      "De-novo assembly and annotation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Introduction to _Staphylococcus aureus_</span>"
    ]
  },
  {
    "objectID": "materials/18-intro_staph.html#course-dataset",
    "href": "materials/18-intro_staph.html#course-dataset",
    "title": "20  Introduction to Staphylococcus aureus",
    "section": "20.2 Course dataset",
    "text": "20.2 Course dataset\nFor this course we’re going to analyse 30 isolates collected as part of a citizen science project that aimed to identify links between self-reported social networks and genome-linked transmission of S. aureus in two Cambridgeshire schools (van Tonder 2023).",
    "crumbs": [
      "Slides",
      "De-novo assembly and annotation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Introduction to _Staphylococcus aureus_</span>"
    ]
  },
  {
    "objectID": "materials/18-intro_staph.html#summary",
    "href": "materials/18-intro_staph.html#summary",
    "title": "20  Introduction to Staphylococcus aureus",
    "section": "20.3 Summary",
    "text": "20.3 Summary\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nStaphylococcus aureus is a bacterium species that is a part of the human microbiota. However, it can also cause a range of infections, from minor skin infections to more serious conditions like pneumonia, endocarditis, and sepsis.\nMethicillin-Resistant Staphylococcus aureus (MRSA) are of primary concern, as they are resistant to common antibiotics such as methicillin.\nGenome sequencing and assembly of this species can aid in identifying clonal clusters and presence of antimicrobial resistance genes.\n\n\n\n\n\nReferences\nBai AD, Lo CKL, Komorowski AS, Suresh M, Guo K, Garg A, et al. Staphylococcus aureus bacteraemia mortality: a systematic review and meta-analysis. Clin Microbiol Infect. 2022. DOI\nUKHSA. Annual epidemiological commentary: Gram-negative, MRSA, MSSA bacteraemia and CDI infections, up to and including financial year 2022 to 2023. 2023. Link\nAntimicrobial Resistance Collaborators. Global burden of bacterial antimicrobial resistance in 2019: a systematic analysis. Lancet. 2022. DOI\nUKHSA. 30 day all-cause mortality following MRSA, MSSA and Gram-negative bacteraemia and C. difficile infections: 2021 to 2022 report. 2023. Link\nColl F, Harrison EM, Toleman MS, Reuter S, Raven KE, Blane B, et al. Longitudinal genomic surveillance of MRSA in the UK reveals transmission patterns in hospitals and the community. Sci Transl Med. 2017. DOI\nHarris SR, Cartwright EJ, Torok ME, Holden MT, Brown NM, Ogilvy-Stuart AL, et al. Whole-genome sequencing for analysis of an outbreak of meticillin-resistant Staphylococcus aureus: a descriptive study. Lancet Infect Dis. 2013. DOI\nMarks LR, Calix JJ, Wildenthal JA, Wallace MA, Sawhney SS, Ransom EM, et al. Staphylococcus aureus injection drug use-associated bloodstream infections are propagated by community outbreaks of diverse lineages. Commun Med (Lond). 2021. DOI\nvan Tonder AJ, McCullagh F, McKeand H, Thaw S, Bellis K, Raisen C, et al. Colonization and transmission of Staphylococcus aureus in schools: a citizen science project. Microb Genom. 2023. DOI\nUhlemann AC, McAdam PR, Sullivan SB, Knox JR, Khiabanian H, Rabadan R, et al. Evolutionary Dynamics of Pandemic Methicillin-Sensitive Staphylococcus aureus ST398 and Its International Spread via Routes of Human Migration. mBio. 2017. DOI\nLarsen J, Raisen CL, Ba X, Sadgrove NJ, Padilla-Gonzalez GF, Simmonds MSJ, et al. Emergence of methicillin resistance predates the clinical use of antibiotics. Nature. 2022. DOI\nYoung BC, Golubchik T, Batty EM, Fung R, Larner-Svensson H, Votintseva AA, et al. Evolutionary dynamics of Staphylococcus aureus during progression from carriage to disease. Proc Natl Acad Sci U S A. 2012. DOI\nDurand G, Javerliat F, Bes M, Veyrieras JB, Guigon G, Mugnier N, et al. Routine Whole-Genome Sequencing for Outbreak Investigations of Staphylococcus aureus in a National Reference Center. Front Microbiol. 2018. DOI",
    "crumbs": [
      "Slides",
      "De-novo assembly and annotation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Introduction to _Staphylococcus aureus_</span>"
    ]
  },
  {
    "objectID": "materials/19-assembly_annotation.html",
    "href": "materials/19-assembly_annotation.html",
    "title": "21  de novo Assembly and Annotation",
    "section": "",
    "text": "21.1 Introduction\nThere are two approaches for genome assembly: reference-based (or comparative) or de novo. In a reference-based assembly, we use a reference genome as a guide to map our sequence data to and thus reassemble our sequence this way (this is what we did in the previous module). Alternatively, we can create a ‘new’ (de novo) assembly that does not rely on a map or reference and more closely reflects the actual genome structure of the isolate that was sequenced. Genome assemblers work by calculating overlaps between reads and (usually) represent these as a graph or network. They then “walk” the graph to determine the original sequence",
    "crumbs": [
      "Slides",
      "De-novo assembly and annotation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>de novo Assembly and Annotation</span>"
    ]
  },
  {
    "objectID": "materials/19-assembly_annotation.html#introduction",
    "href": "materials/19-assembly_annotation.html#introduction",
    "title": "21  de novo Assembly and Annotation",
    "section": "",
    "text": "Genome assembly (http://dx.doi.org/10.1007/s12575-009-9004-1)",
    "crumbs": [
      "Slides",
      "De-novo assembly and annotation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>de novo Assembly and Annotation</span>"
    ]
  },
  {
    "objectID": "materials/19-assembly_annotation.html#genome-assemblers",
    "href": "materials/19-assembly_annotation.html#genome-assemblers",
    "title": "21  de novo Assembly and Annotation",
    "section": "21.2 Genome assemblers",
    "text": "21.2 Genome assemblers\nSeveral tools are available for de novo genome assembly, depending on whether you use short-read sequence data, long reads, or a combination of both. Two of the most commonly used assemblers for short-read Illumina data are Velvet and SPAdes. SPAdes has become the de facto standard de novo genome assembler for Illumina whole genome sequencing data of bacteria and is a major improvement over previous assemblers like Velvet. However, some of its components can be slow and it traditionally did not handle overlapping paired-end reads well. Shovill is a pipeline which uses SPAdes at its core, but alters the steps before and after the primary assembly step to get similar results in less time. Shovill also supports other assemblers like SKESA, Velvet and Megahit.",
    "crumbs": [
      "Slides",
      "De-novo assembly and annotation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>de novo Assembly and Annotation</span>"
    ]
  },
  {
    "objectID": "materials/19-assembly_annotation.html#genome-annotation",
    "href": "materials/19-assembly_annotation.html#genome-annotation",
    "title": "21  de novo Assembly and Annotation",
    "section": "21.3 Genome annotation",
    "text": "21.3 Genome annotation\nGenome annotation is a multi-level process that includes prediction of protein-coding genes (CDSs), as well as other functional genome units such as structural RNAs, tRNAs, small RNAs, pseudogenes, control regions, direct and inverted repeats, insertion sequences, transposons and other mobile elements. The most commonly used tools for annotating bacterial genomes are Prokka and, more recently, Bakta. Both use a tool called prodigal to predict the protein-coding regions along with other tools for predicting other genomic features such as Aragorn for tRNA. Once the genomic regions have been predicted, the tools use a database of existing bacterial genome annotations, normally generated from a large collection of genomes such as UniRef, to add this information to your genome.",
    "crumbs": [
      "Slides",
      "De-novo assembly and annotation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>de novo Assembly and Annotation</span>"
    ]
  },
  {
    "objectID": "materials/19-assembly_annotation.html#summary",
    "href": "materials/19-assembly_annotation.html#summary",
    "title": "21  de novo Assembly and Annotation",
    "section": "21.4 Summary",
    "text": "21.4 Summary\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nde novo genome assembly consists of reconstructing a genome from sequencing reads, without prior information about its structure.\nAssembling genomes de novo requires identifying how the millions of sequencing reads overlap with each other, inferring a “path” through an overlap graph.\nThere are many software packages for de novo assembly, which can be chosen depending on the type of data being used (short read, long read, or both).\nSPades is a popular software for bacterial de novo assembly from short read data. In particular, the Shovill pipeline uses this software in a more efficient way.\nGenome annotation consists of identifying the location of functional elements in the genome, such as genes.\nBakta is a recent genome annotation package for bacteria, which relies in part on known annotated sequences available from public databases such as UniRef.\n\n\n\n\n\nReferences\nComparative Genomics (link) (accessed 2023)\nDe novo genome assembly - T.Seemann - IMB winter school 2016 - brisbane, au - 4 july 2016 (link) (accessed 2023)",
    "crumbs": [
      "Slides",
      "De-novo assembly and annotation",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>de novo Assembly and Annotation</span>"
    ]
  },
  {
    "objectID": "materials/20-assemblebac.html",
    "href": "materials/20-assemblebac.html",
    "title": "22  The assembleBAC pipeline",
    "section": "",
    "text": "22.1 Pipeline Overview\nassembleBAC is a bioinformatics analysis pipeline written in Nextflow for assembling and annotating bacterial genomes. It also predicts the Sequence Type (ST) and provides QC metrics with Quast and CheckM2. It runs the following tools:\nSee Course Software for a more detailed description of each tool.\nAlong with the outputs produced by the above tools, the pipeline produces the following summaries containing results for all samples run through the pipeline (found in the metadata directory):",
    "crumbs": [
      "Slides",
      "De-novo assembly and annotation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>The assembleBAC pipeline</span>"
    ]
  },
  {
    "objectID": "materials/20-assemblebac.html#pipeline-overview",
    "href": "materials/20-assemblebac.html#pipeline-overview",
    "title": "22  The assembleBAC pipeline",
    "section": "",
    "text": "The assembleBAC pipeline\n\n\n\n\nShovill - de novo genome assembly\nmlst - Sequence Type assignment\nBakta - annotation\nQuast - assembly metrics\nCheckM2 - assembly completeness\nMultiQC - assembly metrics summary and pipeline information\n\n\n\n\ncheckm2_summary.tsv - final summary of CheckM2 statistics for input files in TSV format\ntransposed_report.tsv - final summary of Quast summary statistics for input files in TSV format\nMLST_summary.tsv - final summary of Sequence Type assignments in TSV format",
    "crumbs": [
      "Slides",
      "De-novo assembly and annotation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>The assembleBAC pipeline</span>"
    ]
  },
  {
    "objectID": "materials/20-assemblebac.html#prepare-a-samplesheet",
    "href": "materials/20-assemblebac.html#prepare-a-samplesheet",
    "title": "22  The assembleBAC pipeline",
    "section": "22.2 Prepare a samplesheet",
    "text": "22.2 Prepare a samplesheet\nAs with bacQC and bactmap, we need to prepare a CSV file containing the information about our sequencing files, which will be used as an input to the assembleBAC pipeline. Refer back to the bacQC pipeline page for how to do this.",
    "crumbs": [
      "Slides",
      "De-novo assembly and annotation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>The assembleBAC pipeline</span>"
    ]
  },
  {
    "objectID": "materials/20-assemblebac.html#running-assemblebac",
    "href": "materials/20-assemblebac.html#running-assemblebac",
    "title": "22  The assembleBAC pipeline",
    "section": "22.3 Running assembleBAC",
    "text": "22.3 Running assembleBAC\nNow that we have the samplesheet, we can run the assembleBAC pipeline. There are many options that can be used to customise the pipeline, but a typical command is shown below:\nnextflow run avantonder/assembleBAC \\\n  -r \"v2.0.2\" \\\n  -profile singularity \\\n  --input SAMPLESHEET \\\n  --outdir results/assemblebac \\\n  --baktadb databases/bakta_light_20240119 \\\n  --genome_size GENOME_SIZE \\\n  --checkm2db databases/checkme2/uniref100.KO.1.dmnd\n\n-r - tells Nextflow to pull the main version of assembleBAC from Github\n-profile singularity - indicates we want to use the Singularity program to manage all the software required by the pipeline (another option is to use docker). See Data & Setup for details about their installation.\n--input - the samplesheet with the input files, as explained above.\n--outdir - the output directory for the results.\n--baktadb - the path to the directory containing the Bakta database files.\n--genome_size - estimated size of the genome - Shovill uses this value to calculate the genome coverage.\n--checkm2db - the path to the diamond file required by CheckM2.\n\n\n\n\n\n\n\nImportantRemember to QC your sequencing reads\n\n\n\nRemember, the first step of any analysis of a new sequence dataset is to perform Quality Control. For the purposes of time, we’ve run bacQC for you and the results are in preprocessed/bacqc. Before you run assembleBAC, have a look at the read stats and species composition TSV files and make sure that the data looks good before we go ahead and assemble it.\n\n\n\n\n\n\n\n\nExerciseExercise 1 - Running assembleBAC\n\n\n\n\n\n\nYour next task is to run the assembleBAC pipeline on your data. Make sure you start this exercise from the S_aureus directory.\n\nIf you haven’t done so already, make sure to create a samplesheet for your samples. This follows the same format as detailed for the bacQC pipeline, so you can use the same python script as shown in that section.\nIn the folder scripts (within your analysis directory) you will find a script named 01-run_assemblebac.sh. This script contains the code to run this pipeline. Edit this script, adjusting it to fit your input files and the estimated genome size of Staphylococcus aureus.\nActivate the Nextflow software environment (mamba activate nextflow).\nRun the script using bash scripts/01-run_assemblebac.sh.\n\nIf the script is running successfully it should start printing the progress of each job in the assembleBAC pipeline. This will take a little while to finish. \n\n\n\n\n\n\nHintHint\n\n\n\n\n\n\n\nDo a web search to find what the aproximate genome size of S. aureus is.\nLook at the pipeline help (nextflow run avantonder/assembleBAC -r \"v2.0.2\" --help) to find out what the format should be to specify the genome size in megabase pairs.\n\n\n\n\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nThe fixed script is:\n#!/bin/bash\n\nnextflow run avantonder/assembleBAC \\\n  -r \"v2.0.2\" \\\n  -profile singularity \\\n  --input samplesheet.csv \\\n  --outdir results/assemblebac \\\n  --baktadb databases/bakta_light_20240119 \\\n  --genome_size 2M \\\n  --checkm2db databases/checkm2_v2_20210323/uniref100.KO.1.dmnd\nAfter activating the software environment, we ran the script as instructed using:\nbash scripts/01-run_assemblebac.sh\nWhile it was running it printed a message on the screen:\nexecutor &gt;  local (1), slurm (125)\n[5b/76f045] process &gt; ASSEMBLEBAC:INPUT_CHECK:SAMPLESHEET_CHECK (samplesheet.csv) [100%] 1 of 1 ✔\n[e6/2ea670] process &gt; ASSEMBLEBAC:SHOVILL (ERX3876931_ERR3864878_T1)              [100%] 30 of 30 ✔\n[94/bcfb5b] process &gt; ASSEMBLEBAC:MLST (ERX3876931_ERR3864878_T1)                 [100%] 30 of 30 ✔\n[9c/f528af] process &gt; ASSEMBLEBAC:MLST_PARSE                                      [100%] 1 of 1 ✔\n[07/e7d6bf] process &gt; ASSEMBLEBAC:BAKTA (ERX3876931_ERR3864878_T1)                [100%] 30 of 30 ✔\n[7f/6e833a] process &gt; ASSEMBLEBAC:CHECKM2 (ERX3876931_ERR3864878_T1)              [100%] 30 of 30 ✔\n[dc/41e8df] process &gt; ASSEMBLEBAC:CHECKM2_PARSE                                   [100%] 1 of 1 ✔\n[80/9598f1] process &gt; ASSEMBLEBAC:QUAST                                           [100%] 1 of 1 ✔\n[b1/dded73] process &gt; ASSEMBLEBAC:CUSTOM_DUMPSOFTWAREVERSIONS (1)                 [100%] 1 of 1 ✔\n[80/f2d907] process &gt; ASSEMBLEBAC:MULTIQC (1)                                     [100%] 1 of 1 ✔\n-[avantonder/assembleBAC] Pipeline completed successfully-\nCompleted at: 16-Nov-2023 11:45:37\nDuration    : 32m 24s\nCPU hours   : 41.5\nSucceeded   : 126",
    "crumbs": [
      "Slides",
      "De-novo assembly and annotation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>The assembleBAC pipeline</span>"
    ]
  },
  {
    "objectID": "materials/20-assemblebac.html#summary",
    "href": "materials/20-assemblebac.html#summary",
    "title": "22  The assembleBAC pipeline",
    "section": "22.4 Summary",
    "text": "22.4 Summary\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nThe assembleBAC workflow performs de novo assembly of bacterial genomes.\nIn addition, it also:\n\nAnnotates the assembly.\nPerforms sequence typing (using PubMLST schemes).\nDetermines assembly completeness.\nCollects several quality statistics into a report.\n\nRunning the assembleBAC workflow requires:\n\nA samplesheet detailing the sample names and their respective FASTQ files.\nDirectories to databases used for gene annotation (for Bakta) and genome completeness (for CheckM2).\nAn approximate genome size.",
    "crumbs": [
      "Slides",
      "De-novo assembly and annotation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>The assembleBAC pipeline</span>"
    ]
  },
  {
    "objectID": "materials/21-assembly_qc.html",
    "href": "materials/21-assembly_qc.html",
    "title": "23  Assembly Quality",
    "section": "",
    "text": "23.1 Assembly quality\nBefore we do any further analyses, we need to assess the quality of our genome assemblies. The quality of a genome assembly can be influenced by various factors that impact its accuracy and completeness, from sample collection, to sequencing, to the bioinformatic analysis. To assess the quality of an assembly, several key indicators can be examined:\nEvaluating these factors collectively provides insights into the reliability and utility of the genome assembly for further analysis and interpretation.\nFor the purposes of time, we didn’t run bacQC on our Staphylococcus aureus dataset. However, we’ve provided the results for our contamination check using Kraken 2 in preprocessed/bacqc/metadata/species_composition.tsv along with the other summary results produced by bacQC.\nSince our samples are taken from a published dataset, we expected little or no contamination, but this is not always the case. So, it is important still to do quality control of data taken from public databases, to ensure that it is suitable for the analysis you’re running. For instance, contamination with another Staphylococcus species or even another bacterial species altogether would have affected the accuracy of our assemblies.\nLet’s now turn to some of the other metrics to help us assess our assemblies’ quality.",
    "crumbs": [
      "Slides",
      "De-novo assembly and annotation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Assembly Quality</span>"
    ]
  },
  {
    "objectID": "materials/21-assembly_qc.html#assembly-quality",
    "href": "materials/21-assembly_qc.html#assembly-quality",
    "title": "23  Assembly Quality",
    "section": "",
    "text": "Completeness: the extent to which the genome is accurately represented in the assembly, including both core and accessory genes.\nContiguity: refers to how long the sequences are without gaps. A highly contiguous assembly means that longer stretches are assembled without interruptions, the best being chromosome-level assemblies. A less contiguous assembly will be represented in more separate fragments.\nContamination: the presence of DNA from other species or sources in the assembly.\nAccuracy/correctness: how closely the assembled sequence matches the true sequence of the genome.\n\n\n\n\n\n\n\n\nImage source: Bretaudeau et al. (2023) Galaxy Training",
    "crumbs": [
      "Slides",
      "De-novo assembly and annotation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Assembly Quality</span>"
    ]
  },
  {
    "objectID": "materials/21-assembly_qc.html#contiguity",
    "href": "materials/21-assembly_qc.html#contiguity",
    "title": "23  Assembly Quality",
    "section": "23.2 Contiguity",
    "text": "23.2 Contiguity\nOne of the outputs from running assembleBAC is a summary file containing the Quast outputs for each sample. This file can be found in preprocessed/assemblebac/metadata/transposed_report.tsv.\nYou can open it with a spreadsheet software such as Excel from our file browser  (for brevity, we only show the columns of most interest):\nAssembly    # contigs (&gt;= 0 bp) # contigs (&gt;= 1000 bp)  Total length (&gt;= 0 bp)  Largest contig  N50\nERX3876932_ERR3864879_T1_contigs    77  40  2848635 484893  109559\nERX3876949_ERR3864896_T1_contigs    70  18  2683262 493468  251833\nERX3876930_ERR3864877_T1_contigs    84  20  2729135 557153  251805\nERX3876908_ERR3864855_T1_contigs    45  13  2717933 792936  707553\nERX3876945_ERR3864892_T1_contigs    30  9   2670961 1351816 1351816\nThe columns are:\n\nAssembly - our sample ID.\n# contigs (&gt;= 0 bp) - the total number of contigs in each of our assemblies.\n# contigs (&gt;= 1000 bp) - the total number of contigs &gt; 1000 bp in each of our assemblies.\nTotal length (&gt;= 0 bp) - the total length of our assembled fragments.\nLargest contig - the largest assembled fragment.\nN50 - a metric indicating the length of the shortest fragment, from the group of fragments that together represent at least 50% of the total genome. A higher N50 value suggests better contig lengths.\n\nTo interpret these statistics, it helps to compare them with other well-assembled Staphylococcus aureus genomes. For example, let’s take the first MRSA genome that was sequenced, N315, as our reference for comparison. This genome is 2.8 Mb long and is composed of a single chromosome.\nWe can see that all of our assemblies reached a total length of around 2.7 to 2.9 Mb, which matches the expected length from our reference genome. This indicates that we managed to assemble most of the expected genome. However, we can see that there is a variation in the number of fragments in the final assemblies (i.e. their contiguity). For instance, isolates ERX3876936_ERR3864883 and ERX3876945_ERR3864892 were assembled to a small number of fragments each, suggesting good assemblies. For several other isolates our assemblies were more fragmented, in particular ERX3876939_ERR3864886 which had more than 200 fragments. This indicates less contiguous sequences.",
    "crumbs": [
      "Slides",
      "De-novo assembly and annotation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Assembly Quality</span>"
    ]
  },
  {
    "objectID": "materials/21-assembly_qc.html#completeness",
    "href": "materials/21-assembly_qc.html#completeness",
    "title": "23  Assembly Quality",
    "section": "23.3 Completeness",
    "text": "23.3 Completeness\nWe now turn to assessing genome completeness, i.e. whether we managed to recover most of the known Staphylococcus aureus genome, or whether we have large fractions missing. We can assess this by using CheckM2 which was run as part of the assembleBAC pipeline. This tool assesses the completeness of the assembled genomes based on other similar organisms in public databases, in addition to contamination scores. The output file from assembleBAC summarising the CheckM2 results is a tab-delimited file called checkm2_summary.tsv. This file can be found in preprocessed/assemblebac/metadata/ and can be opened in a spreadsheet software such as Excel. Here is an example result (for brevity, we only show the columns of most interest):\nName    Completeness    Contamination   Genome_Size GC_Content  Total_Coding_Sequences\nERX3876905_ERR3864852_T1_contigs    100 0.22    2743298 0.33    2585\nERX3876907_ERR3864854_T1_contigs    100 0.49    2900162 0.33    2765\nERX3876908_ERR3864855_T1_contigs    100 0.07    2717933 0.33    2534\nERX3876909_ERR3864856_T1_contigs    100 0.17    2854371 0.33    2711\nERX3876929_ERR3864876_T1_contigs    100 0.1 2675834 0.33    2464\nThese columns indicate:\n\nName - our sample name.\nCompleteness - how complete our genome was inferred to be as a percentage; this is based on the machine learning models used and the organisms present in the database.\nContamination - the percentage of the assembly estimated to be contaminated with other organisms (indicating our assembly isn’t “pure”).\nGenome_Size - how big the genome is estimated to be, based on other similar genomes present in the database. The N315 is 2.8 Mb in total, so these values make sense.\nGC_Content - the percentage of G’s and C’s in the genome, which is relatively constant within a species. The S. aureus GC content is approximately 33%, so again these values make sense.\nTotal_Coding_Sequences - the total number of coding sequences (genes) that were identified by CheckM2. The N315 indicates annotated genes, so the values obtained could be overestimated.\n\nFrom this analysis, we can assess that our genome assemblies are good quality, with 100% of the genome assembled for all our isolates. It is worth noting that the assessment from CheckM2 is an approximation based on other genomes in the database. In diverse species such as S. aureus the completeness may be underestimated, because each individual strain will only carry part of the pan-genome for that species.",
    "crumbs": [
      "Slides",
      "De-novo assembly and annotation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Assembly Quality</span>"
    ]
  },
  {
    "objectID": "materials/21-assembly_qc.html#accuracy",
    "href": "materials/21-assembly_qc.html#accuracy",
    "title": "23  Assembly Quality",
    "section": "23.4 Accuracy",
    "text": "23.4 Accuracy\nAssessing the accuracy of our genome includes addressing issues such as:\n\nRepeat resolution: the ability of the assembly to accurately distinguish and represent repetitive regions within the genome.\nStructural variations: detecting large-scale changes, such as insertions, deletions, or rearrangements in the genome structure.\nSequencing errors: identifying whether errors from the sequencing reads have persisted in the final assembly, including single-nucleotide errors or minor insertions/deletions.\n\nAssessing these aspects of a genome assembly can be challenging, primarily because the true state of the organism’s genome is often unknown, especially in the case of new genome assemblies.\n\n\n\n\n\n\nExerciseExercise 1 - Assembly contiguity\n\n\n\n\n\n\nTo assess the contiguity of your assemblies, you ran QUAST as part of the assembleBAC pipeline. Open the file transposed_report.tsv in the preprocessed/assemblebac/metadata directory. This should open the file in Excel. - Answer the following questions: - Was the assembly length what you expect for Staphylococcus aureus? - Did your samples have good contiguity (number of fragments)? - Do you think any of the samples are not of sufficient quality to include in further analyses?\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nFor the dataset we are using, we had our file in preprocessed/assembleBAC/transposed_report.tsv. We opened this table in Excel and this is what we obtained:\nAssembly    # contigs (&gt;= 0 bp) # contigs (&gt;= 1000 bp)  # contigs (&gt;= 5000 bp)  # contigs (&gt;= 10000 bp) # contigs (&gt;= 25000 bp) # contigs (&gt;= 50000 bp) Total length (&gt;= 0 bp)  Total length (&gt;= 1000 bp)   Total length (&gt;= 5000 bp)   Total length (&gt;= 10000 bp)  Total length (&gt;= 25000 bp)  Total length (&gt;= 50000 bp)  # contigs   Largest contig  Total length    GC (%)  N50 N90 auN L50 L90 # N's per 100 kbp\nERX3876932_ERR3864879_T1_contigs    77  40  36  29  25  20  2848635 2836870 2827630 2778943 2698417 2521114 47  484893  2841571 32.73   109559  41891   191514.9    6   21  0\nERX3876949_ERR3864896_T1_contigs    70  18  16  15  14  11  2683262 2671100 2666432 2659228 2636490 2507149 21  493468  2673387 32.69   251833  78361   283091.3    4   10  0\nERX3876930_ERR3864877_T1_contigs    84  20  20  18  15  11  2729135 2714469 2714469 2701567 2644913 2501830 25  557153  2717749 32.7    251805  52910   319559.1    4   10  0\nThe assembly lengths obtained were all around 2.7 to 2.8 Mb, which is expected for this species.\nThe contiguity of our assemblies seemed excellent, with only up to 205 fragments. Some samples even had fewer than 50 fragments assembled, suggesting we managed to mostly reassemble our genomes.\nIsolate ERX3876939_ERR3864886_T1_contigs has more than 200 contigs. A this is higher than the number obtained for the rest of our assembled genome, we may want to consider removing this isolate as it may affect our pan-genome calculation in the next section. However, as the rest of the metrics for this genome look alright, let’s leave it in for now.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExerciseExercise 2 - Assembly completeness\n\n\n\n\n\n\nTo assess the completeness of your assembly, we ran the CheckM2 software on your assembly files as part of the assembleBAC pipeline. Go to the preprocessed/assemblebac/metadata/ directory and open the checkm2_summary.tsv file in Excel. Answer the following questions: - Did you manage to achieve &gt;90% completeness for your genomes? - Was there evidence of substantial contamination in your assemblies? - Was the GC content what you expected for this species?\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nWe opened the file preprocessed/assemblebac/metadata/checkm2_summary.tsv in Excel and this is what we obtained:\nName    Completeness    Contamination   Completeness_Model_Used Translation_Table_Used  Coding_Density  Contig_N50  Average_Gene_Length Genome_Size GC_Content  Total_Coding_Sequences  Additional_Notes\nERX3876905_ERR3864852_T1_contigs    100 0.22    Neural Network (Specific Model) 11  0.841   188199  297.9439072 2743298 0.33    2585    None\nERX3876907_ERR3864854_T1_contigs    100 0.49    Neural Network (Specific Model) 11  0.839   174940  293.8538879 2900162 0.33    2765    None\nERX3876908_ERR3864855_T1_contigs    100 0.07    Neural Network (Specific Model) 11  0.841   707553  301.2190213 2717933 0.33    2534    None\nERX3876909_ERR3864856_T1_contigs    100 0.17    Neural Network (Specific Model) 11  0.84    181628  295.3832534 2854371 0.33    2711    None\nERX3876929_ERR3864876_T1_contigs    100 0.1 Neural Network (Specific Model) 11  0.841   605766  304.9176136 2675834 0.33    2464    None\nWe can see from this that:\n\nWe achieved 100% completeness (according to CheckM2’s database) for all our samples.\nThere was no evidence of strong contamination affecting our assemblies (all ~5% or below).\nThe GC content was 33%, which is what is expected for Staphylococcus aureus.",
    "crumbs": [
      "Slides",
      "De-novo assembly and annotation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Assembly Quality</span>"
    ]
  },
  {
    "objectID": "materials/21-assembly_qc.html#summary",
    "href": "materials/21-assembly_qc.html#summary",
    "title": "23  Assembly Quality",
    "section": "23.5 Summary",
    "text": "23.5 Summary\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nKey aspects to evaluate an assembly quality include:\n\nContiguity: how continuous the final assembly is (the best assembly would be chromosome-level).\nCompleteness: whether the entire genome of the species was captured.\n\nCommon indicators for evaluating the contiguity of a genome assembly include metrics like N50, fragment count and total assembly size.\nSpecialised software tools, like QUAST and CheckM2, enable the assessment of genome completeness and contamination by comparing the assembly to known reference genomes and identifying missing or unexpected genes and sequences.",
    "crumbs": [
      "Slides",
      "De-novo assembly and annotation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Assembly Quality</span>"
    ]
  },
  {
    "objectID": "materials/22-strain_typing.html",
    "href": "materials/22-strain_typing.html",
    "title": "24  Typing bacteria using MLST",
    "section": "",
    "text": "24.1 Typing bacteria using MLST\nStrain typing of bacteria is a critical process in microbiology that allows for the identification and differentiation of bacterial strains within a species. Traditional lab-based methods for bacterial strain typing include techniques such as serotyping, phage typing, and antibiograms. Serotyping involves the identification of distinct bacterial strains based on their unique surface antigens, while phage typing differentiates strains by their susceptibility to specific bacteriophages. Antibiograms, on the other hand, classify bacteria based on their patterns of antibiotic resistance. Although these methods are valuable and widely used, they can be labor-intensive, time-consuming, and sometimes lack the resolution needed to distinguish closely related strains.\nOver the past 25 years, molecular techniques like Multilocus Sequence Typing (MLST) have revolutionized bacterial strain typing. MLST involves sequencing internal fragments of multiple housekeeping genes and assigning a unique allelic profile to each strain based on the sequence variation at these loci. Each unique combination of alleles is called a sequence type (ST) and the PubMLST project curates and maintains these sequence types in the form of large, comprehensive databases that facilitates global surveillance of bacterial populations and enhances our understanding of bacterial evolution and diversity. Although this method might seem a bit old-fashioned in the age of genomic analysis (as it focuses on only 7 genes), it offers a uniform and comparable way to categorize strains across different labs and locations.",
    "crumbs": [
      "Slides",
      "De-novo assembly and annotation",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Typing bacteria using MLST</span>"
    ]
  },
  {
    "objectID": "materials/22-strain_typing.html#typing-bacteria-using-mlst",
    "href": "materials/22-strain_typing.html#typing-bacteria-using-mlst",
    "title": "24  Typing bacteria using MLST",
    "section": "",
    "text": "Multilocus Sequence Typing (MLST)\n\n\n\n24.1.1 Clonal complexes\nClonal complexes (CCs) are groups of related STs that share a high degree of genetic similarity, typically differing by only a single allele. By comparing ST profiles using algorithms such as eBURST or using clustering methods, closely related STs are grouped into CCs, reflecting their evolutionary relationships and likely descent from a common ancestor. This assignment of STs to CCs allows researchers to study the population structure, epidemiology, and evolutionary history of bacterial species, facilitating the tracking of disease outbreaks and the identification of major lineages responsible for infections.\n\n\n\nCommon S. aureus Clonal Complexes (CCs) composed of closely related STs (Vivoni 2006)",
    "crumbs": [
      "Slides",
      "De-novo assembly and annotation",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Typing bacteria using MLST</span>"
    ]
  },
  {
    "objectID": "materials/22-strain_typing.html#sec-mlst-cli",
    "href": "materials/22-strain_typing.html#sec-mlst-cli",
    "title": "24  Typing bacteria using MLST",
    "section": "24.2 MLST with command line",
    "text": "24.2 MLST with command line\nWe start our analysis by activating our software environment, to make all the necessary tools available:\nmamba activate mlst\nWe’re going to run a tool called mlst on the assemblies we generated with assembleBAC:\n# create output directory\nmkdir results/mlst\n\n# run mlst\nmlst --scheme saureus results/assemblebac/assemblies/*.fa &gt; results/mlst/mlst_typing.tsv\nThis command outputs a tab-delimited file (TSV), which we can open in a spreadsheet software such as Excel. Here is the result for our samples:\nresults/assemblebac/assemblies/ERX3876905_ERR3864852_T1_contigs.fa  saureus 398 arcC(3) aroE(35)    glpF(19)    gmk(2)  pta(20) tpi(26) yqiL(39)\nresults/assemblebac/assemblies/ERX3876907_ERR3864854_T1_contigs.fa  saureus 34  arcC(8) aroE(2) glpF(2) gmk(2)  pta(6)  tpi(3)  yqiL(2)\nresults/assemblebac/assemblies/ERX3876908_ERR3864855_T1_contigs.fa  saureus 97  arcC(3) aroE(1) glpF(1) gmk(1)  pta(1)  tpi(5)  yqiL(3)\nresults/assemblebac/assemblies/ERX3876909_ERR3864856_T1_contigs.fa  saureus 30  arcC(2) aroE(2) glpF(2) gmk(2)  pta(6)  tpi(3)  yqiL(2)\nresults/assemblebac/assemblies/ERX3876929_ERR3864876_T1_contigs.fa  saureus 45  arcC(10)    aroE(14)    glpF(8) gmk(6)  pta(10) tpi(3)  yqiL(2)\nWe get a column for each of the 7 genes used for S. aureus sequence typing, with the gene name followed by the allele number in parenthesis. The allele number is an identifier used by PubMLST, and it means that allele has a specific sequence with a certain set of variants (search for alleles here). For example, arcC(3) corresponds to allele 3 of the arcC gene.\nThe command line version of mlst also reports when an allele has an inexact match to the allele in the database, with the following notation (copied from the README documentation):\n\n\n\n\n\n\n\n\n\nSymbol\nMeaning\nLength\nIdentity\n\n\n\n\nn\nExact intact allele\n100%\n100%\n\n\n~n\nNovel full length allele similar to n\n100%\n≥ --minid\n\n\nn?\nPartial match to known allele\n≥ --mincov\n≥ --minid\n\n\n-\nAllele missing\n&lt; --mincov\n&lt; --minid\n\n\nn,m\nMultiple alleles\n\n\n\n\n\nThe third column of the output indicates the Sequence Type (ST) of our samples based on the combination of the 7 alleles identified by mlst.\n\n\n\n\n\n\nTipMLST schemes available\n\n\n\nTo check the list of MLST schemes available, you can use the command mlst --list.\n\n\n\n\n\n\n\n\nExerciseExercise 1 - MLST with assemblebac\n\n\n\n\n\n\nThe sharp-eyed amongst you may have noticed that assemblebac runs mlst alongside assembly and annotation.\n\nOpen the mlst_summary.tsv file in the results/assemblebac/metadata directory\nAre the assigned alleles and Sequence Types the same as those you obtained by running mlst yourself?\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\n\nWe opened the mlst_summary.tsv file which contained the following:\n\nERX3876905_ERR3864852_T1_contigs.fa saureus 398 arcC(3) aroE(35)    glpF(19)    gmk(2)  pta(20) tpi(26) yqiL(39)\nERX3876907_ERR3864854_T1_contigs.fa saureus 34  arcC(8) aroE(2) glpF(2) gmk(2)  pta(6)  tpi(3)  yqiL(2)\nERX3876908_ERR3864855_T1_contigs.fa saureus 97  arcC(3) aroE(1) glpF(1) gmk(1)  pta(1)  tpi(5)  yqiL(3)\nERX3876909_ERR3864856_T1_contigs.fa saureus 30  arcC(2) aroE(2) glpF(2) gmk(2)  pta(6)  tpi(3)  yqiL(2)\nERX3876929_ERR3864876_T1_contigs.fa saureus 45  arcC(10)    aroE(14)    glpF(8) gmk(6)  pta(10) tpi(3)  yqiL(2)\n\nThe alleles and STs were the same as those assigned when we ran mlst ourselves",
    "crumbs": [
      "Slides",
      "De-novo assembly and annotation",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Typing bacteria using MLST</span>"
    ]
  },
  {
    "objectID": "materials/22-strain_typing.html#summary",
    "href": "materials/22-strain_typing.html#summary",
    "title": "24  Typing bacteria using MLST",
    "section": "24.3 Summary",
    "text": "24.3 Summary\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nMLST (Multilocus Sequence Typing) is a genotyping method used to identify and categorize bacterial strains based on the sequences of specific housekeeping genes.\nIt aids in tracking and monitoring the spread of bacterial pathogens, understanding their genetic diversity, and detecting outbreaks.\nMLST results reveal the Sequence types (STs) of bacterial strains, which can help in identifying clonal complexes and their relatedness.\nDedicated command-line software such as mlst allows for automation and give a more detailed output.\n\n\n\n\nReferences\nVivoni AM, Diep BA, de Gouveia Magalhães AC, Santos KR, Riley LW, Sensabaugh GF, Moreira BM. Clonal composition of Staphylococcus aureus isolates at a Brazilian university hospital: identification of international circulating lineages. J Clin Microbiol. 2006. DOI",
    "crumbs": [
      "Slides",
      "De-novo assembly and annotation",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Typing bacteria using MLST</span>"
    ]
  },
  {
    "objectID": "materials/23-pan_genomes.html",
    "href": "materials/23-pan_genomes.html",
    "title": "25  Introduction to Pan-genomes",
    "section": "",
    "text": "25.1 Pan-genome analysis\nThe concept of a bacterial pan-genome refers to the full complement of genes in a bacterial species. It is divided into two parts: the core genome and the accessory genome. The core genome consists of genes that are present in all strains of the species, which are typically essential for basic functions and survival. The accessory genome includes genes that are not present in all strains but may be found in one or more strains; these genes often confer specialized functions, such as antibiotic resistance or the ability to metabolise certain substrates.\nThe significance of bacterial pan-genomes in microbial genomics lies in their ability to provide insights into the genetic diversity, evolutionary history, and adaptive capabilities of bacterial species. By studying the pan-genome, researchers can identify genes that are crucial for the survival of a species across different environments, as well as genes that allow certain strains to thrive in specific niches or confer pathogenicity.",
    "crumbs": [
      "Slides",
      "Pan-genome analysis",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Introduction to Pan-genomes</span>"
    ]
  },
  {
    "objectID": "materials/23-pan_genomes.html#core-gene-alignment",
    "href": "materials/23-pan_genomes.html#core-gene-alignment",
    "title": "25  Introduction to Pan-genomes",
    "section": "25.2 Core gene alignment",
    "text": "25.2 Core gene alignment\nWhen you have a very diverse dataset where no single reference accurately reflects the population structure within your dataset, it is challenging to create a sequence alignment using the entire genomes. In such cases, it is more effective to use a reference-independent approach. One way to do this is by constructing a core gene alignment, which includes genes present in nearly all the genomes in the dataset. This core gene alignment is a key part of a pan-genome analysis and is considered the best method for building a multiple sequence alignment for phylogenetic inference in these cases.\nThere are several tools available to do this including roary, Panaroo and panX. It is important to note that because the alignments produced using these tools only contain the genes found in all or nearly all of the samples, the amount of phylogenetically informative sites is reduced. For this reason, core gene-based phylogenies are useful for looking at the broader diversity in a species. To examine the relationship between more closely related genomes, it is preferable to perform clustering and create new sub-trees using reference-based alignment.",
    "crumbs": [
      "Slides",
      "Pan-genome analysis",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Introduction to Pan-genomes</span>"
    ]
  },
  {
    "objectID": "materials/23-pan_genomes.html#summary",
    "href": "materials/23-pan_genomes.html#summary",
    "title": "25  Introduction to Pan-genomes",
    "section": "25.3 Summary",
    "text": "25.3 Summary\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nA pan-genome includes all the genes in a bacterial species, with “core” genes being present in (nearly) all individuals and “accessory” genes being those unique to only some.\nExamining the pan-genomes of a species can reveal shared and unique genetic traits, giving insights into its biology and adaptability. This includes understanding the evolution of antimicrobial resistance, which is of relevance to public health.\nA core gene alignment consists of identifying genes found in almost all the genomes in a dataset and performing a multiple sequence alignment from them.\nCore gene alignments are used to obtain more accurate phylogenetic trees in highly diverse species.",
    "crumbs": [
      "Slides",
      "Pan-genome analysis",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Introduction to Pan-genomes</span>"
    ]
  },
  {
    "objectID": "materials/24-panaroo.html",
    "href": "materials/24-panaroo.html",
    "title": "26  Panaroo",
    "section": "",
    "text": "26.1 Core genome alignment generation with Panaroo\nThe software Panaroo was developed to analyse bacterial pan-genomes. It is able to identify orthologous sequences between a set of sequences, which it uses to produce a multiple sequence alignment of the core genome. The output alignment it produces can then be used to build our phylogenetic tree in the next step.\nAs input to Panaroo we will use the gene annotations for our newly assembled genomes, which were produced by the assembleBAC pipeline using Bakta.\nFirst, let’s activate the panaroo software environment:\nTo run Panaroo on our samples we can use the following commands:\nThe options used are:\nPanaroo can take a long time to run, so be prepared to wait a while for its analysis to finish .\nOnce it finishes, we can see the output it produces:\nThere are several output files generated, which can be generated for more advanced analysis and visualisation (see Panaroo documentation for details). For our purpose of creating a phylogeny from the core genome alignment, we need the file core_gene_alignment_filtered.aln, which is a file in FASTA format. We can take a quick look at this file:\nWe can see this contains a sequence named “ERX3876945_ERR3864892_T1”, which corresponds to one of the genomes we included in our dataset. We can look at all the sequence names in the FASTA file:\nWe can see each input genomes, assembled and annotated by us, appears once.",
    "crumbs": [
      "Slides",
      "Pan-genome analysis",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Panaroo</span>"
    ]
  },
  {
    "objectID": "materials/24-panaroo.html#sec-panaroo",
    "href": "materials/24-panaroo.html#sec-panaroo",
    "title": "26  Panaroo",
    "section": "",
    "text": "mamba activate panaroo\n\n# create output directory\nmkdir results/panaroo\n\n# run panaroo\npanaroo \\\n  --input results/assemblebac/bakta/*.gff3 \\\n  --out_dir results/panaroo \\\n  --clean-mode strict \\\n  --alignment core \\\n  --core_threshold 0.98 \\\n  --remove-invalid-genes \\\n  --threads 8\n\n\n--input - all the input annotation files, in the Panaroo-compatible GFF3 format. Notice how we used the * wildcard to match all the files in the folder.\n--out_dir - the output directory we want to save the results into.\n--clean-mode - determines the stringency of Panaroo in including genes within its pan-genome graph for gene clustering and core gene identification. The available modes are ‘strict’, ‘moderate’, and ‘sensitive’. These modes balance eliminating probable contaminants against preserving valid annotations like infrequent plasmids. In our case we used ‘strict’ mode, as we are interested in building a core gene alignment for phylogenetics, so including rare plasmids is less important for our downstream task.\n--alignment - whether we want to produce an alignment of core genes or all genes (pangenome alignment). In our case we want to only consider the core genes, to build a phylogeny.\n--core_threshold - the fraction of input genomes where a gene has to be found to be considered a “core gene”. In our case we’ve set this to a very high value, to ensure most of our samples have the gene.\n--remove-invalid-genes - this is recommended to remove annotations that are incompatible with the annotation format expected by Panaroo.\n--threads - how many CPUs we want to use for parallel computations.\n\n\n\nls results/panaroo\naligned_gene_sequences/               core_alignment_header.embl        gene_presence_absence_roary.csv\nalignment_entropy.csv                 core_gene_alignment.aln           pan_genome_reference.fa\ncombined_DNA_CDS.fasta                core_gene_alignment_filtered.aln  pre_filt_graph.gml\ncombined_protein_CDS.fasta            final_graph.gml                   struct_presence_absence.Rtab\ncombined_protein_cdhit_out.txt        gene_data.csv                     summary_statistics.txt\ncombined_protein_cdhit_out.txt.clstr  gene_presence_absence.Rtab\ncore_alignment_filtered_header.embl   gene_presence_absence.csv\n\nhead results/panaroo/core_gene_alignment_filtered.aln\n&gt;ERX3876945_ERR3864892_T1\natgcaacaatcagacgtcattagtgctgccaaaaaatatatggaatctattcatcaaaat\ngattatacaggccatgatattgcgcatgtatatcgtgtcactgctttagctaaatcaatc\ngctgaaaatgaaggtgttaatgatactttagtcattgaactcgcatgtttgcttcatgat\naccgttgacgaaaaagttgtagatgctaacaaacaatatgttgaattgaagtcattttta\ntcttctttatcactatcaaccgaagatcaagagcacattttatttattattaataatatg\nagctatcgcaatggcaaaaatgatcatgtcactttatctttagaaggtcaaattgtcagg\ngatgcagatcgtcttgatgctataggcgctataggtgttgcacgaacatttcaatttgca\nggacactttggtgaaccaatgtggacagaacatatgtcactagataagattaatgatgat\nttagttgaacagttgccaccatctgcaattaagcatttctttgaaaaattacttaagtta\n\ngrep \"&gt;\" results/panaroo/core_gene_alignment_filtered.aln\n&gt;ERX3876945_ERR3864892_T1\n&gt;ERX3876948_ERR3864895_T1\n&gt;ERX3876909_ERR3864856_T1\n&gt;ERX3876942_ERR3864889_T1\n&gt;ERX3876935_ERR3864882_T1\n&gt;ERX3876905_ERR3864852_T1\n&gt;ERX3876940_ERR3864887_T1\n&gt;ERX3876954_ERR3864901_T1\n\n... more output omitted to save space ...\n\n\n\n\n\n\n\nNotePreparing GFF files for Panaroo (click to see details)\n\n\n\n\n\nPanaroo requires GFF files in a non-standard format. They are similar to standard GFF files, but they also include the genome sequence itself at the end of the file. By default, Bakta (which we used earlier to annotate our assembled genomes) already produces files in this non-standard GFF format.\nHowever, GFF files downloaded from NCBI will not be in this non-standard format. To convert the files to the required format, the Panaroo developers provide us with a Python script that can do this conversion:\npython3 convert_refseq_to_prokka_gff.py -g annotation.gff -f genome.fna -o new.gff\n\n-g is the original GFF (for example downloaded from NCBI).\n-f is the corresponding FASTA file with the genome (also downloaded from NCBI).\n-o is the name for the output file.\n\nThis is a bit more advanced, and is included here for interested users. We already prepared all the files for constructing a phylogeny, so you don’t need to worry about this for the course.\n\n\n\n\n\n\n\n\n\nExerciseExercise 1 - Core genome alignment\n\n\n\n\n\n\nUsing Panaroo, perform a core genome alignment for your assembled sequences.\n\nActivate the software environment: mamba activate panaroo.\nFix the script we provide in scripts/02-run_panaroo.sh. See Section 26.1 if you need a hint of how to fix the code in the script.\nRun the script using bash scripts/02-run_panaroo.sh.\nWhen the analysis starts you will get several messages and progress bars print on the screen.\n\nThis analysis takes a long time to run, so you can leave it running, open a new terminal and continue to the next exercise.\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nThe fixed code for our script is:\n#!/bin/bash\n\n# create output directory\nmkdir -p results/panaroo/\n\n# Run panaroo\npanaroo \\\n  --input results/assemblebac/bakta/*.gff3 \\\n  --out_dir results/panaroo \\\n  --clean-mode strict \\\n  --alignment core \\\n  --core_threshold 0.98 \\\n  --remove-invalid-genes \\\n  --threads 8\nAs it runs, Panaroo prints several messages to the screen.\nWe specified results/panaroo as the output file for the core genome aligner Panaroo. We can see all the output files it generated:\nls results/panaroo\naligned_gene_sequences                core_alignment_header.embl        gene_presence_absence_roary.csv\nalignment_entropy.csv                 core_gene_alignment.aln           pan_genome_reference.fa\ncombined_DNA_CDS.fasta                core_gene_alignment_filtered.aln  pre_filt_graph.gml\ncombined_protein_CDS.fasta            final_graph.gml                   struct_presence_absence.Rtab\ncombined_protein_cdhit_out.txt        gene_data.csv                     summary_statistics.txt\ncombined_protein_cdhit_out.txt.clstr  gene_presence_absence.Rtab\ncore_alignment_filtered_header.embl   gene_presence_absence.csv\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExerciseExercise 2 - Tree inference\n\n\n\n\n\n\n Because Panaroo takes a long time to run, we provide pre-processed results in the folder preprocessed/panaroo, which you can use as input to IQ-TREE in this exercise.\nProduce a tree from the core genome alignment from the previous step.\n\nActivate the software environment: mamba activate iqtree.\nFix the script provided in scripts/03-run_iqtree.sh. See Section 14.3.2 if you need a hint of how to fix the code in the script.\nRun the script using bash scripts/03-run_iqtree.sh. Several messages will be printed on the screen while IQ-TREE runs.\n\n\n\n\n\n\n\nHintHint\n\n\n\n\n\n\nFor SNP-sites:\n\nThe input alignment should be the output from the panaroo program found in results/panaroo/ (or in the preprocessed folder if you are still waiting for your analysis to finish).\n\n\n\n\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nThe fixed script is:\n#!/bin/bash\n\n# create output directory\nmkdir -p results/snp-sites/\nmkdir -p results/iqtree/\n\n# extract variable sites\nsnp-sites results/panaroo/core_gene_alignment_filtered.aln &gt; results/snp-sites/core_gene_alignment_snps.aln\n\n# count invariant sites\nsnp-sites -C results/panaroo/core_gene_alignment_filtered.aln &gt; results/snp-sites/constant_sites.txt\n\n# Run iqtree\niqtree \\\n  -fconst $(cat results/snp-sites/constant_sites.txt) \\\n  -s results/snp-sites/core_gene_alignment_snps.aln \\\n  --prefix results/iqtree/School_Staph \\\n  -nt AUTO \\\n  -ntmax 8 \\\n  -mem 8G \\\n  -m MFP \\\n  -bb 1000\n\nWe extract the variant sites and count of invariant sites using SNP-sites.\nAs input to both snp-sites steps, we use the core_gene_alignment_snps.aln alignment produced by Panaroo in the previous step of our analysis.\nThe number of constant sites was specified with $(cat results/snp-sites/constant_sites.txt) to directly add the contents of the constant_sites.txt file, without having to open the file to obtain these numbers.\nWe use as prefix for our output files “School_Staph” (since we are using the data from the schools Staph paper), so all the output file names will be named as such.\nWe automatically detect the number of threads/CPUs for parallel computation.\n\nAfter the analysis runs we get several output files in our directory:\nls results/iqtree/\nSchool_Staph.bionj  School_Staph.ckp.gz  School_Staph.iqtree  \nSchool_Staph.log    School_Staph.mldist  School_Staph.treefile\nThe main file of interest is School_Staph.treefile, which contains our tree in the standard Newick format.",
    "crumbs": [
      "Slides",
      "Pan-genome analysis",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Panaroo</span>"
    ]
  },
  {
    "objectID": "materials/24-panaroo.html#summary",
    "href": "materials/24-panaroo.html#summary",
    "title": "26  Panaroo",
    "section": "26.2 Summary",
    "text": "26.2 Summary\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nPanaroo can be used to generate a core gene alignment.\nIt can use as input GFF files generated by an annotation software such as Bakta.\nIQ-Tree can be used for tree inference, based on the output from Panaroo’s core gene alignment.",
    "crumbs": [
      "Slides",
      "Pan-genome analysis",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Panaroo</span>"
    ]
  },
  {
    "objectID": "materials/25-pathogenwatch.html",
    "href": "materials/25-pathogenwatch.html",
    "title": "27  Pathogenwatch",
    "section": "",
    "text": "27.1 Pathogenwatch\nPathogenwatch is a web-based platform for common genomic surveillance analysis tasks, including:\nPathogenwatch is designed to be user-friendly, supporting the analysis of over 100 species, including Staphylococcus aureus, which is our organism of focus. In this chapter, we will cover the basics of loading genomes and creating collections for analysis on this platform. The details of the Pathogenwatch analysis will then be covered in following chapters.\nIn order to use this platform you will first need to create an account (or sign-in through your existing Google, Facebook or Twitter).",
    "crumbs": [
      "Slides",
      "Pan-genome analysis",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Pathogenwatch</span>"
    ]
  },
  {
    "objectID": "materials/25-pathogenwatch.html#pathogenwatch",
    "href": "materials/25-pathogenwatch.html#pathogenwatch",
    "title": "27  Pathogenwatch",
    "section": "",
    "text": "Identifying strains for pathogens of concern.\nCluster sequences using phylogenetic analysis.\nIdentifying the presence of antibiotic resistance genes.",
    "crumbs": [
      "Slides",
      "Pan-genome analysis",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Pathogenwatch</span>"
    ]
  },
  {
    "objectID": "materials/25-pathogenwatch.html#uploading-fasta-files",
    "href": "materials/25-pathogenwatch.html#uploading-fasta-files",
    "title": "27  Pathogenwatch",
    "section": "27.2 Uploading FASTA files",
    "text": "27.2 Uploading FASTA files\nOnce you have logged in to Pathogenwatch, you can load the FASTA files with the sequences you want to analyse. In our case, we will load the assemblies we have provided in the preprocessed directory which were built using assembleBAC.\n\nClick the Upload link in the top-right corner of the page:\n\n\n\n\n\n\n\nFigure 27.1\n\n\n\n\nClick in the Upload FASTA(s) button, in the “Single Genome FASTAs” section:\n\n\n\n\n\n\n\nFigure 27.2\n\n\n\n\nIf your internet connection is slow and/or unstable, you can tick “Compress files” and “Upload files individually”.Click the + button on the bottom-right corner to upload the sequences:\n\n\n\n\n\n\n\nFigure 27.3\n\n\n\n\nThis will open a file browser, where you can select the FASTA files from your local machine. Go to the preprocessed/assemblebac/assemblies folder where you have the results from your earlier genome assembly analysis. You can upload several files at once by clicking and selecting several FASTA files while holding the Ctrl key. Click Open on the dialogue window after you have selected all of your FASTA files.\n\n\n\n\n\n\n\nFigure 27.4\n\n\n\n\nA new page will open showing the progress of the samples being uploaded and processed.\n\n\n\n\n\n\n\nFigure 27.5\n\n\n\n\nClick in the VIEW GENOMES button, which will take you to a tabular view of your samples:\n\n\n\n\n\n\n\nFigure 27.6\n\n\n\nPathogenwatch performs the following major analyses useful for genomic surveillance: sequence typing (ST), antimicrobial resistance (AMR) analysis, phylogenetics, as well as reporting general statistics about your samples (such as genome completeness, which we also assessed with checkM2). We will detail several of these analyses in the coming chapters, but here is a brief description of each column:\n\nName - the names of the uploaded samples.\nOrganism - the species that was detected for our samples, in this case Staphylococcus aureus.\nType and Typing schema - the sequence type assigned to each sample, based on MLST analysis\nCountry and Date - the country and date of collection, respectively; only shown if we provided that information as metadata.\nAccess - whether these samples are private or public; in this case, because they were uploaded by us, they are private (only we can see them).\n\n\n\n\n\n\n\nNoteMetadata\n\n\n\nIf you have metadata files associated with your sequenced samples, you can upload those files following these instructions. Make sure all metadata files are in CSV format, with five recommended columns named ‘latitude’, ‘longitude’, ‘year’, ‘month’, and ‘day’. You can also use the template provided by Pathogenwatch on the upload page, to help you prepare your metadata files before the analysis.\nHaving this type of information is highly recommended, as it will allow you to visualise your samples on a map, which is useful if you want to match particular strains to the geographic locations where outbreaks occur.",
    "crumbs": [
      "Slides",
      "Pan-genome analysis",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Pathogenwatch</span>"
    ]
  },
  {
    "objectID": "materials/25-pathogenwatch.html#collections",
    "href": "materials/25-pathogenwatch.html#collections",
    "title": "27  Pathogenwatch",
    "section": "27.3 Collections",
    "text": "27.3 Collections\nA useful feature of Pathogenwatch is to group our samples into collections. This allows us to manage and analyse samples in batches of our choice. The same sample can exist in different collections. For example you might create a collection with only the genomes you sequenced recently, another collection with all the genomes you ever sequenced in your facility, or even a collection that includes your samples together with public samples available online (if you want to compare them with each other).\nTo create a collection from your sequences, check the box next to the “Name” header to select all of the uploaded genomes. Then, from the top-right of the table, click Selected Genomes –&gt; Create Collection:\n\n\n\n\n\n\nFigure 27.7\n\n\n\nIn the next window give a name and description to your collection:\n\n\n\n\n\n\nFigure 27.8\n\n\n\nIt is highly recommended to provide details for your collection:\n\nTitle - give your collection a title that is meaningful to you, for example: “WBG 2023”.\nDescription - give a brief description of your samples, for example: “Culture-based sequencing of Staphylococcus aureus. Samples were collected from school children in Cambridgeshire in 2018.”\nIf your data come from a published study, provide a DOI of the study, for example: “10.1099/mgen.0.000993”.\n\nFinally, click Create Now button to create your collection. You will be shown a table and map, with the samples you just added to the collection:\n\n\n\n\n\n\nFigure 27.9\n\n\n\nThis table contains several columns:\n\nPurple  download button - download the assembled genome in FASTA format. This is the same file that you just uploaded, so it’s not very useful in our case. It can be useful if you want to download the public sequences available from within Pathogenwatch.\nGreen  download button - download the gene annotation performed by Pathogenwatch in GFF format. Note that our assembly script already performed gene annotation using Bakta, so this feature is also not so useful for us. But again, if you were using public sequences from Pathogenwatch, you could download their GFF files.\nNAME - your sample name.\nST and PROFILE - these columns refer to the “sequence type” (ST) assigned to each of our samples.- ST and PROFILE - these columns refer to the “sequence type” (ST) assigned to each of our samples.\nINC TYPES - identification of plasmids relevant for Staphylococcal species (“inc” stands for “incompatibility”, refering to plasmid incompatibility groups). Inc plasmids often carry antibiotic resistance and virulence genes, making them of particular relevance for public health (e.g. Foley et al 2021).\n\nAlong with the typing, we can also look at the drug susceptibility profiles of the samples by clicking on the Typing button on the left-hand side of the screen and changing it to Antibiotics:\n\n\n\n\n\n\nFigure 27.10\n\n\n\nYou will see a table containing the drugs that Pathogenwatch is able to identify resistance to using genetic variants identified in the genomes we uploaded. Resistance to a drug is shown by a red circle and we can see that the majority of our genomes are resistant to penicillin:\n\n\n\n\n\n\nFigure 27.11\n\n\n\nWe will add some of this information to our phylogenetic tree in the next section.\n\n\n\n\n\n\nExerciseExercise 1 - Downloading data from Pathogenwatch\n\n\n\n\n\n\nFor the next step, visualising our phylogeny, you will need to download the results of the lineage typing and antibiotic susceptibility from Pathogenwatch:\n\nDownload the Typing table\nDownload the AMR profile.\nTwo CSV files will be downloaded. Rename the appropriate files to the following as this will help with the next exercise:\n\nwbg-2023-typing.csv\nwbg-2023-amr-profile.csv\n\n\nNow, move these two files into the S_aureus analysis directory.\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\n\nClick on the download icon in the top right-hand corner and select Typing table:\n\n\n\nClick on the download icon again and select AMR profile:\n\n\n\nTwo files were downloaded (the longer names will likely be slightly different) to our Downloads directory:\n\npathogenwatch-saureus-pi6kp4oqdawi-wbg-2023-typing.csv\npathogenwatch-saureus-pi6kp4oqdawi-wbg-2023-amr-profile\n\nWe renamed the files on the command line (you could do this in the File Explorer too):\n\nmv pathogenwatch-saureus-pi6kp4oqdawi-wbg-2023-typing.csv wbg-2023-typing.csv\nmv pathogenwatch-saureus-pi6kp4oqdawi-wbg-2023-amr-profile wbg-2023-amr-profile.csv\n\nWe moved the files from the Downloads (or Desktop, depending on your browser settings) directory to our S_aureus directory. The following command assumes we were in the S_aureus directory to start with:\n\n# if your browser downloads to \"Downloads\" folder:\nmv ~/Downloads/wbg-2023* .\n# if your browser downloads to the \"Desktop\" folder:\nmv ~/Desktop/wbg-2023* .\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExerciseExercise 2 - Preparing data for Microreact\n\n\n\n\n\n\nNow that we have analysed our genomes with Pathogenwatch and downloaded the typing and AMR profiles, we need to merge this metadata with the existing information we have for the 30 S. aureus genomes. We could do this with Excel. Alternatively, we provide a Python script called merge_staph_data.py in the scripts directory.\n\nMake sure you are in the base software environment (where we have the Pandas library for Python).\nRun merge_staph_data.py to create the final metadata file we need for Microreact. Look at the help documentation of this script to find out how to specify inputs and outputs to this script.\n\nNote: If you have not managed to run the Pathogenwatch analysis, we provide the output files from the previous exercise in preprocessed/pathogenwatch that you can use instead.\n\n\n\n\n\n\nHintHint\n\n\n\n\n\n\nDepending on how they are written, most Python scripts will print the available options if you use the help flag (--help or -h):\npython scripts/merge_staph_data.py -h\n\n\n\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\n\nWe activated the base software environment with mamba activate base\nWe ran the merge_staph_data.py script to create a TSV file called staph_metadata.tsv in your analysis directory:\npython scripts/merge_staph_data.py -s sample_info.csv -t wbg-2023-typing.csv -a wbg-2023-amr-profile.csv",
    "crumbs": [
      "Slides",
      "Pan-genome analysis",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Pathogenwatch</span>"
    ]
  },
  {
    "objectID": "materials/25-pathogenwatch.html#summary",
    "href": "materials/25-pathogenwatch.html#summary",
    "title": "27  Pathogenwatch",
    "section": "27.4 Summary",
    "text": "27.4 Summary\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nPathogenwatch is a web-based platform designed for genomic surveillance of bacterial pathogens. It assists in the analysis and interpretation of genomic data to monitor disease outbreaks and track pathogen evolution.\nYou can upload genome assemblies in FASTA format and accompanying metadata as CSV files.\nAssemblies can be organized into collections, making it simpler to manage and analyze multiple samples together.\nPathogenwatch’s interface offers an intuitive user experience designed for users with varying levels of expertise, providing results such as biotype/serogroup, strain classification, antimicrobial resistance (AMR) and phylogenetic placement.\nDownloading and combining the output files from Pathogenwatch as well as any metadata can be of use to annotate phylogenetic trees.",
    "crumbs": [
      "Slides",
      "Pan-genome analysis",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Pathogenwatch</span>"
    ]
  },
  {
    "objectID": "materials/26-tree_visualization.html",
    "href": "materials/26-tree_visualization.html",
    "title": "28  Visualising phylogenies 2",
    "section": "",
    "text": "28.1 Summary\nIn the Visualising phylogenies chapter we used Microreact to visualize the phylogenetic tree of Namibian TB isolates. We’re going to do the same with the S.aureus phylogeny and metadata but use some of the other features in Microreact we didn’t use before.",
    "crumbs": [
      "Slides",
      "Pan-genome analysis",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Visualising phylogenies 2</span>"
    ]
  },
  {
    "objectID": "materials/26-tree_visualization.html#summary",
    "href": "materials/26-tree_visualization.html#summary",
    "title": "28  Visualising phylogenies 2",
    "section": "",
    "text": "TipKey Points\n\n\n\n\nMicroreact is a user-friendly and flexible tool for tree visualisation.\nSeveral analysis results can be plotted alongside our tree, providing a richer view of our data.",
    "crumbs": [
      "Slides",
      "Pan-genome analysis",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Visualising phylogenies 2</span>"
    ]
  },
  {
    "objectID": "materials/27-intro_pneumo.html",
    "href": "materials/27-intro_pneumo.html",
    "title": "29  Introduction to Streptococcus pneumoniae",
    "section": "",
    "text": "29.1 Streptococcus pneumoniae\nStreptococcus pneumoniae (the pneumococcus) is a gram-positive human commensal that also causes a significant disease burden with pneumococcal-related diseases such as pneumonia and meningitis responsible for up 500,000 deaths in children &lt;5 years old each year (Wahl 2018). The primary pneumococcal virulence factor is the polysaccharide capsule that surrounds the cell. Over one hundred different polysaccharide capsules (serotypes) have been identified to date and the conjugate vaccines that are routinely administered in vaccination programmes around the world typically target the 10-13 serotypes most prevalent in invasive pneumococcal disease (IPD) (Ganaie 2020).\nThe Global Pneumococcal Sequencing (GPS) project was set up to help understand the global picture of pneumococcal evolution during vaccine introductions using whole-genome sequencing (Lo 2019). By the end of 2019, the GPS project had sequenced more than 26,000 pneumococcal genomes from more than 50 countries. As well as investigating the pre- and post-vaccine pneumococcal population structure, 13,454 GPS genomes were combined with an additional 7,000 published pneumococcal genomes to identify clusters of sequences defined as Global Pneumococcal Sequence Clusters (GPSCs) (Gladstone 2019). The study identified 621 GPSCs and 35 GPSCs contained more than 100 isolates, accounting for the majority of genomes included in the dataset. These clusters are increasingly being used as the standard method of lineage assignment in pneumococcus. Tools now exist to allow new genomes to be assigned to existing clusters or else form the basis of novel clusters.\nUptake and incorporation of DNA from the environment into the pneumococcal chromosome via transformation and homologous recombination has been shown to contribute more to nucleotide variation than mutation in this species. As well as being biologically important, recombination obscures the true phylogenetic signal of vertical descent and needs to be accounted for when inferring pneumococcal phylogenies.",
    "crumbs": [
      "Slides",
      "Recombination",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Introduction to *Streptococcus pneumoniae*</span>"
    ]
  },
  {
    "objectID": "materials/27-intro_pneumo.html#dataset",
    "href": "materials/27-intro_pneumo.html#dataset",
    "title": "29  Introduction to Streptococcus pneumoniae",
    "section": "29.2 Dataset",
    "text": "29.2 Dataset\nWe have extracted sequence data for 50 pneumococcal genomes from a study that performed the first detailed characterisation of serotype 1 pneumococci collected from disease and carriage in an endemic region in West Africa (Chaguza 2022).",
    "crumbs": [
      "Slides",
      "Recombination",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Introduction to *Streptococcus pneumoniae*</span>"
    ]
  },
  {
    "objectID": "materials/27-intro_pneumo.html#reference",
    "href": "materials/27-intro_pneumo.html#reference",
    "title": "29  Introduction to Streptococcus pneumoniae",
    "section": "29.3 Reference",
    "text": "29.3 Reference\nFor the analysis of our S. pneumoniae dataset, we’re going to map the sequence data to the same reference used in the Chaguza paper, PNI0373 (CP001845). This is a serotype 1 reference and is closely related to the other samples in the dataset.",
    "crumbs": [
      "Slides",
      "Recombination",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Introduction to *Streptococcus pneumoniae*</span>"
    ]
  },
  {
    "objectID": "materials/27-intro_pneumo.html#summary",
    "href": "materials/27-intro_pneumo.html#summary",
    "title": "29  Introduction to Streptococcus pneumoniae",
    "section": "29.4 Summary",
    "text": "29.4 Summary\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nAlthough being a human commensal, S. pneumoniae is also responsible for significant diseases such as pneumonia.\nLarge genome sequencing projects revealed a high diversity in this species and allowed the classification of isolates into related clusters.\nA large contributor to the diversity seen in this species is the aquision of new DNA via recombination (horizontal gene transfer).\nRecombination can distort phylogenetic signals and is therefore crucial to take into account before building a tree.\n\n\n\n\nReferences\nWahl B, O’Brien KL, Greenbaum A, Majumder A, Liu L, Chu Y, Lukšić I, Nair H, McAllister DA, Campbell H, Rudan I, Black R, Knoll MD. Burden of Streptococcus pneumoniae and Haemophilus influenzae type b disease in children in the era of conjugate vaccines: global, regional, and national estimates for 2000-15. Lancet Glob Health. 2018. DOI\nGanaie F, Saad JS, McGee L, van Tonder AJ, Bentley SD, Lo SW, Gladstone RA, Turner P, Keenan JD, Breiman RF, Nahm MH. A New Pneumococcal Capsule Type, 10D, is the 100th Serotype and Has a Large cps Fragment from an Oral Streptococcus. mBio. 2020. DOI\nLo SW, Gladstone RA, van Tonder AJ, Lees JA, du Plessis M, Benisty R, Givon-Lavi N, Hawkins PA, Cornick JE, Kwambana-Adams B, Law PY, Ho PL, Antonio M, Everett DB, Dagan R, von Gottberg A, Klugman KP, McGee L, Breiman RF, Bentley SD; Global Pneumococcal Sequencing Consortium. Pneumococcal lineages associated with serotype replacement and antibiotic resistance in childhood invasive pneumococcal disease in the post-PCV13 era: an international whole-genome sequencing study. Lancet Infect Dis. 2019. DOI\nGladstone RA, Lo SW, Lees JA, Croucher NJ, van Tonder AJ, Corander J, Page AJ, Marttinen P, Bentley LJ, Ochoa TJ, Ho PL, du Plessis M, Cornick JE, Kwambana-Adams B, Benisty R, Nzenze SA, Madhi SA, Hawkins PA, Everett DB, Antonio M, Dagan R, Klugman KP, von Gottberg A, McGee L, Breiman RF, Bentley SD; Global Pneumococcal Sequencing Consortium. International genomic definition of pneumococcal lineages, to contextualise disease, antibiotic resistance and vaccine impact. EBioMedicine. 2019.DOI\nChaguza C, Ebruke C, Senghore M, Lo SW, Tientcheu PE, Gladstone RA, Tonkin-Hill G, Cornick JE, Yang M, Worwui A, McGee L, Breiman RF, Klugman KP, Kadioglu A, Everett DB, Mackenzie G, Croucher NJ, Roca A, Kwambana-Adams BA, Antonio M, Bentley SD. Comparative Genomics of Disease and Carriage Serotype 1 Pneumococci. Genome Biol Evol. 2022. DOI",
    "crumbs": [
      "Slides",
      "Recombination",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Introduction to *Streptococcus pneumoniae*</span>"
    ]
  },
  {
    "objectID": "materials/28-run_bactmap.html",
    "href": "materials/28-run_bactmap.html",
    "title": "30  Run bactmap",
    "section": "",
    "text": "30.1 Summary",
    "crumbs": [
      "Slides",
      "Recombination",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Run bactmap</span>"
    ]
  },
  {
    "objectID": "materials/28-run_bactmap.html#summary",
    "href": "materials/28-run_bactmap.html#summary",
    "title": "30  Run bactmap",
    "section": "",
    "text": "TipKey Points\n\n\n\n\nObtaining genomes for a species such as pneumococcus can be done using reference-based alignment.\nWe can use the same workflows and scripts covered so far:\n\navantonder/bacQC to perform sequence quality control on the raw sequencing reads.\nnf-core/bactmap for generating consensus genome sequences based on mapping reads to a reference genome.\nUse a custom script to loop through our samples and run seqtk comp, which estimates the fraction of missing data in our genomes.",
    "crumbs": [
      "Slides",
      "Recombination",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Run bactmap</span>"
    ]
  },
  {
    "objectID": "materials/29-recombination.html",
    "href": "materials/29-recombination.html",
    "title": "31  Introduction to recombination",
    "section": "",
    "text": "31.1 Removing recombination\nRecombination in bacteria is characterized by DNA transfer from one organism or strain (the donor) to another organism/strain (the recipient) or the uptake of exogenous DNA from the surrounding environment. Broadly, there are three different types of bacterial recombination:\nThe sequences transferred via recombination can influence genome-wide measures of sequence simularity more than vertically-inherited point mutations that are the signal of shared common ancestor. Thus, identifying recombinant regions and accounting for their potentially different phylogenetic history is crucial when examining the evolutionary history of bacteria. In practice, what this means is that we remove (mask) previously identified recombinant regions in our multiple sequence alignments, before we proceed with phylogenetic tree inference.\nThe two most commonly used tools to do this are Gubbins and ClonalFrameML. It’s important to note that Gubbins cannot be used on the core gene alignment produced by tools like roary or panaroo. Instead, Gubbins requires a whole-genome alignment as input, in order to analyse the spatial distribution of base substitutions. For this reason, the finer-scale phylogenetic structure of trees generated using a core gene alignment may be less accurate. If we want to properly account for recombination in this instance, typically we would perform clustering on our initial tree, then map sequence data for the samples within a cluster to a suitable reference before running our recombination removal tool of choice.",
    "crumbs": [
      "Slides",
      "Recombination",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Introduction to recombination</span>"
    ]
  },
  {
    "objectID": "materials/29-recombination.html#removing-recombination",
    "href": "materials/29-recombination.html#removing-recombination",
    "title": "31  Introduction to recombination",
    "section": "",
    "text": "Transformation: the uptake of exogenous DNA from the environment\nTransduction: virus-mediated (phage) transfer of DNA between bacteria\nConjugation: the transfer of DNA from one bacterium to another via cell-to-cell contact",
    "crumbs": [
      "Slides",
      "Recombination",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Introduction to recombination</span>"
    ]
  },
  {
    "objectID": "materials/29-recombination.html#gubbins",
    "href": "materials/29-recombination.html#gubbins",
    "title": "31  Introduction to recombination",
    "section": "31.2 Gubbins",
    "text": "31.2 Gubbins\nGubbins (Genealogies Unbiased By recomBinations In Nucleotide Sequences) is an algorithm that iteratively identifies loci containing elevated densities of base substitutions, while concurrently constructing a phylogeny based on the putative point mutations outside of these regions. We’re going to use Gubbins to identify the recombinant regions in the alignment we generated using bactmap.",
    "crumbs": [
      "Slides",
      "Recombination",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Introduction to recombination</span>"
    ]
  },
  {
    "objectID": "materials/29-recombination.html#running-gubbins",
    "href": "materials/29-recombination.html#running-gubbins",
    "title": "31  Introduction to recombination",
    "section": "31.3 Running Gubbins",
    "text": "31.3 Running Gubbins\nWe’ll start by activating the gubbins software environment:\nmamba activate gubbins\nTo run Gubbins on the aligned_pseudogenomes.fas file, the following commands can be used:\n# create output directory\nmkdir -p results/gubbins/\n\n# run gubbins\nrun_gubbins.py --prefix sero1 --tree-builder iqtree results/bactmap/pseudogenomes/aligned_pseudogenomes.fas\n\n# move gubbins outputs to results directory\nmv sero1.* results/gubbins/\nThe options we used are:\n\n--prefix - prefix to use for the Gubbins output files.\n--tree-builder - Gubbins can be run with different phylogenetic software including IQ-TREE, FastTree and RAxML.\n\nAs it runs, Gubbins prints several messages to the screen.\nWe moved the output files to results/gubbins/ where we can see all the output files it generated:\nls results/gubbins\nsero1.final_tree.tre                     sero1.per_branch_statistics.csv     sero1.summary_of_snp_distribution.vcf\nsero1.branch_base_reconstruction.embl    sero1.log                           sero1.recombination_predictions.embl\nsero1.filtered_polymorphic_sites.fasta   sero1.node_labelled.final_tree.tre  sero1.recombination_predictions.gff\nsero1.filtered_polymorphic_sites.phylip",
    "crumbs": [
      "Slides",
      "Recombination",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Introduction to recombination</span>"
    ]
  },
  {
    "objectID": "materials/29-recombination.html#masking-recombinant-regions",
    "href": "materials/29-recombination.html#masking-recombinant-regions",
    "title": "31  Introduction to recombination",
    "section": "31.4 Masking recombinant regions",
    "text": "31.4 Masking recombinant regions\nIn a similar way to how we masked the TB alignment to remove certain regions of the reference genome from downstream analyses, the next step is to mask the recombinant regions in our aligned_pseudogenomes.fas file, so these do not influence our phylogenetic tree inference. Instead of using remove_blocks_from_aln.py, we will use mask_gubbins_aln.py (included with Gubbins):\nmask_gubbins_aln.py --aln results/bactmap/pseudogenomes/aligned_pseudogenomes.fas --gff results/gubbins/sero1.recombination_predictions.gff --out results/gubbins/aligned_pseudogenomes_masked.fas\nThe options we used are:\n\n--aln - the input alignment, in this case the alignment created by bactmap.\n--gff - the GFF file containing the coordinates for the recombinant regions identified by Gubbins.\n--out - the masked alignment.\n\nThe masked final alignment will be saved to the results/gubbins/ directory.",
    "crumbs": [
      "Slides",
      "Recombination",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Introduction to recombination</span>"
    ]
  },
  {
    "objectID": "materials/29-recombination.html#visualizing-recombinant-regions",
    "href": "materials/29-recombination.html#visualizing-recombinant-regions",
    "title": "31  Introduction to recombination",
    "section": "31.5 Visualizing recombinant regions",
    "text": "31.5 Visualizing recombinant regions\nThe outputs from Gubbins can be visualised by running a R script included as part of Gubbins:\nplot_gubbins.R -t results/gubbins/sero1.final_tree.tre -r results/gubbins/sero1.recombination_predictions.gff -a resources/reference/GCF_000299015.1_ASM29901v1_genomic.gff -o results/gubbins/sero1.recombination.png\nThe options we used are:\n\n-t - the recombination-free phylogenetic tree created by Gubbins.\n-r - the GFF file containing the coordinates for the recombinant regions identified by Gubbins.\n-a - the GFF file for the reference genome containing the coordinates for the coding regions.\n-o - the figure showing the recombinant regions identified by Gubbins.\n\n\n\n\nRecombinant regions\n\n\nThe panel on the left shows the maximum-likelihood phylogeny built from the clonal frame of serotype isolates. The scale below shows the length of branches in base substitutions. The tree is coloured according to the classification of isolates, each of which corresponds to a row in the panel on the right. Each column in this panel is a base in the reference annotation, the annotation of which is shown at the top of the figure. The panel shows the distribution of inferred recombination events, which are coloured blue if they are unique to a single isolate, or red, if they are shared by multiple isolates through common ancestry.\n\n\n\n\n\n\nExerciseExercise 1 - Run Gubbins\n\n\n\n\n\n\nUsing Gubbins, create a recombination-masked alignment.\n\nActivate the software environment: mamba activate gubbins.\nRun the script we provide in scripts using bash scripts/03-run_gubbins.sh.\nWhen the analysis starts you will get several messages and progress bars print on the screen.\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nWe ran the script using bash scripts/03-run_gubbins.sh. The script prints a message while it’s running:\n--- Gubbins 3.3.1 ---\n\nCroucher N. J., Page A. J., Connor T. R., Delaney A. J., Keane J. A., Bentley S. D., Parkhill J., Harris S.R. \"Rapid phylogenetic analysis of large samples of recombinant bacterial whole genome sequences using Gubbins\". Nucleic Acids Res. 2015 Feb 18;43(3):e15. doi: 10.1093/nar/gku1196.\n\nChecking dependencies and input files...\n\nChecking input alignment file...\n\nFiltering input alignment...\n...\nIn the results/gubbins directory we can see the following files:\naligned_pseudogenomes_masked.fas         sero1.final_tree.tre                sero1.recombination.png\nsero1.branch_base_reconstruction.embl    sero1.log                           sero1.recombination_predictions.embl\nsero1.filtered_polymorphic_sites.fasta   sero1.node_labelled.final_tree.tre  sero1.recombination_predictions.gff\nsero1.filtered_polymorphic_sites.phylip  sero1.per_branch_statistics.csv     sero1.summary_of_snp_distribution.vcf\nAlong with the Gubbins outputs the script also created the masked alignment file (aligned_pseudogenomes_masked.fas) and a figure showing the location of the recombinant regions in the reference genome (sero1.recombination.png).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExerciseExercise 2 - Build post-Gubbins phylogeny\n\n\n\n\n\n\nNow that we have created a recombination-masked alignment, we can extract the variant sites and count of constant sites and use these to build a recombination-free phylogenetic tree with IQ-TREE.\n\nActivate the software environment: mamba activate iqtree.\nFix the script provided in scripts/04-run_iqtree.sh. See Section 14.3.2 if you need a hint of how to fix the code in the script.\nRun the script using bash scripts/04-run_iqtree.sh. Several messages will be printed on the screen while IQ-TREE runs.\n\n\n\n\n\n\n\nHintHint\n\n\n\n\n\n\nFor SNP-sites:\n\nThe input alignment should be the output from the gubbins program found in results/gubbins/ (or in the preprocessed folder if you are still waiting for your analysis to finish).\n\n\n\n\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nThe fixed script is:\n#!/bin/bash\n\n# create output directory\nmkdir -p results/snp-sites/\nmkdir -p results/iqtree/\n\n# extract variable sites\nsnp-sites preprocessed/gubbins/aligned_pseudogenomes_masked.fas &gt; results/snp-sites/aligned_pseudogenomes_masked_snps.fas\n\n# count invariant sites\nsnp-sites -C preprocessed/gubbins/aligned_pseudogenomes_masked.fas &gt; results/snp-sites/constant_sites.txt\n\n# FIX!!\n# Run iqtree\niqtree \\\n  -fconst $(cat results/snp-sites/constant_sites.txt) \\\n  -s results/snp-sites/aligned_pseudogenomes_masked_snps.fas \\\n  --prefix results/iqtree/sero1 \\\n  -nt AUTO \\\n  -ntmax 8 \\\n  -mem 8G \\\n  -m MFP \\\n  -bb 1000\n\nWe extract the variant sites and count of invariant sites using SNP-sites.\nAs input to both snp-sites steps, we use the aligned_pseudogenomes_masked_snps.fas file produced in the previous exercise.\nThe next step runs IQ-tree:\n\nWe specify the number of constant sites, also generated from the previous exercise. We can use $(cat results/snp-sites/constant_sites.txt) to directly add the contents of constant_sites.txt without having to open the file to obtain these numbers.\nWe use as prefix for our output files “sero1” (since we are using the data from the Chaguza serotype 1 paper), so all the output file names will be named as such.\n\nWe automatically detect the number of threads/CPUs for parallel computation.\n\nAfter the analysis runs we get several output files in our directory:\nls results/iqtree/\nsero1.bionj  sero1.ckp.gz  sero1.iqtree  \nsero1.log    sero1.mldist  sero1.treefile\nThe main file of interest is sero1.treefile, which contains our tree in the standard Newick format.",
    "crumbs": [
      "Slides",
      "Recombination",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Introduction to recombination</span>"
    ]
  },
  {
    "objectID": "materials/29-recombination.html#summary",
    "href": "materials/29-recombination.html#summary",
    "title": "31  Introduction to recombination",
    "section": "31.6 Summary",
    "text": "31.6 Summary\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nIn bacteria, recombination refers to the process of genetic material exchange between organisms, through processes such as transformation, transduction and conjugation.\nThe Gubbins software can be to produce recombination-free alignments. It requires as input a multiple sequence alignment from whole genomes.\nAfter identifying recombinant regions, these are masked from the alignment (i.e. converted to ’N’s).\nThe output from Gubbins can then be used as input to SNP-sites and IQ-tree as demonstrated before.",
    "crumbs": [
      "Slides",
      "Recombination",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Introduction to recombination</span>"
    ]
  },
  {
    "objectID": "materials/30-poppunk.html",
    "href": "materials/30-poppunk.html",
    "title": "32  Typing bacteria using PopPUNK",
    "section": "",
    "text": "32.1 Newer methods for strain typing\nAlthough Multilocus Sequence Typing (MLST) has been highly effective, newer methods leveraging whole-genome sequencing (WGS) are beginning to replace it due to their enhanced resolution and comprehensiveness. WGS provides a complete picture of the bacterial genome, allowing for the identification of single nucleotide polymorphisms (SNPs) across the entire genome rather than just a limited set of housekeeping genes. This approach offers a finer scale of differentiation between strains, which is crucial for detailed epidemiological investigations and understanding transmission dynamics. Additionally, WGS facilitates the detection of horizontal gene transfer, virulence factors, and antibiotic resistance genes, providing a more holistic view of bacterial pathogenicity and evolution. Tools and platforms such as core genome MLST (cgMLST) and SNP-based phylogenetic analysis have been developed to utilize WGS data, offering higher resolution and more accurate strain typing. As sequencing technologies become faster and more cost-effective, WGS is increasingly becoming the gold standard for bacterial strain typing, enabling precise and comprehensive bacterial surveillance and research.",
    "crumbs": [
      "Slides",
      "Recombination",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Typing bacteria using PopPUNK</span>"
    ]
  },
  {
    "objectID": "materials/30-poppunk.html#poppunk",
    "href": "materials/30-poppunk.html#poppunk",
    "title": "32  Typing bacteria using PopPUNK",
    "section": "32.2 PopPUNK",
    "text": "32.2 PopPUNK\nPopPUNK (Population Partitioning Using Nucleotide K-mers) is a bioinformatics tool designed for the rapid and scalable analysis of bacterial population structure using whole-genome sequencing data. It works by comparing genomes based on k-mer frequencies, where k-mers are short, fixed-length sequences of nucleotides. PopPUNK creates a distance matrix by calculating the genetic distances between all pairs of genomes, capturing both core and accessory genome variations. This matrix is then used to cluster genomes into distinct populations or clades using a combination of hierarchical and density-based clustering algorithms. PopPUNK can handle large datasets efficiently, providing insights into the evolutionary relationships, population dynamics, and epidemiological patterns of bacterial species. Its ability to integrate core and accessory genome data makes it a powerful tool for understanding microbial diversity and for tracking the spread of bacterial strains in public health contexts.\n\n32.2.1 Global Pneumococcal Sequence Clusters (GPSCs)\nPopPUNK has been instrumental in assigning Global Pneumococcal Sequence Clusters (GPSCs), thereby enhancing our understanding of the worldwide population structure of Streptococcus pneumoniae. By analyzing extensive collections of whole-genome sequences from pneumococcal isolates collected globally, PopPUNK clusters these genomes into distinct lineages based on k-mer frequency comparisons. This high-resolution clustering allows for the identification of major lineages and sub-lineages, providing insights into the genetic diversity and evolutionary relationships among pneumococcal strains across different geographic regions. The tool has been used to trace the spread of specific lineages, monitor the impact of pneumococcal conjugate vaccines (PCVs) on the population structure, and identify emerging strains that may pose new public health threats. By integrating global genomic data, PopPUNK facilitates a comprehensive and detailed understanding of pneumococcal epidemiology, which is crucial for devising effective vaccination strategies and managing antibiotic resistance on a global scale.\n\n\n\n621 Global Pneumococcal Sequence Clusters (GPSCs) assigned with PopPUNK (Gladstone 2006)",
    "crumbs": [
      "Slides",
      "Recombination",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Typing bacteria using PopPUNK</span>"
    ]
  },
  {
    "objectID": "materials/30-poppunk.html#running-poppunk",
    "href": "materials/30-poppunk.html#running-poppunk",
    "title": "32  Typing bacteria using PopPUNK",
    "section": "32.3 Running PopPUNK",
    "text": "32.3 Running PopPUNK\n\nCreating assembly sheet\nFirst we need to create a tab-delimited file (called assemblies.txt) containing the samples we want to assign GPSCs to and where the assemblies are located:\n\n\n\n\n\nThis file can be created in a standard spreadsheet software such as Excel, as long as you save the file in tab-delimited format.\n\n\nRunning PopPUNK\nBefore running the poppunk command, we start by activating the respective software environment:\nmamba activate poppunk\nTo run PopPUNK on our assemblies, the following commands can be used:\n# create output directory\nmkdir -p results/poppunk\n\n# run PopPUNK\npoppunk_assign --db GPS_v8_ref --external-clustering GPS_v8_external_clusters.csv --query assemblies.txt --output results/poppunk --threads 8\nThe options we used are:\n\n--db - GPSC reference database.\n--external-clustering - GPSC designations.\n--query - a 2-column tab-delimited file containing sample names and their assembly paths.\n--output - the output directory for the results.\n--threads 8 - specifies how many CPUs to use.\n\n\n\n\n\n\n\nExerciseExercise 1 - Run PopPUNK\n\n\n\n\n\n\nUsing PopPUNK, assign your Pneumococcal genomes to GPSCs.\n\nMake sure you have created an assemblies.txt file with your assembly files (see above).\nActivate the software environment: mamba activate poppunk.\nRun the script we provide in scripts using bash scripts/05-run_poppunk.sh.\nWhen the analysis starts you will get several messages print on the screen.\nWere any of the assemblies not assigned to an existing GPSC?\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nWe ran the script using bash scripts/05-run_poppunk.sh. The script prints a message while it’s running:\nPopPUNK: assign\n    (with backend: sketchlib v2.1.4\n     sketchlib: /rds/user/ajv37/hpc-work/micromamba/envs/poppunk/lib/python3.10/site-packages/pp_sketchlib.cpython-310-x86_64-linux-gnu.so)\nMode: Assigning clusters of query sequences\n\n\nGraph-tools OpenMP parallelisation enabled: with 8 threads\nSketching 5 genomes using 5 thread(s)\nProgress (CPU): 5 / 5\nWriting sketches to file\nLoading previously refined model\nCompleted model loading\nWARNING: versions of input databases sketches are different, results may not be compatible\nCalculating distances using 8 thread(s)\nProgress (CPU): 100.0%\nLoading network from /home/ajv37/rds/rds-pathogen-CvaldwrLQm4/databases/poppunk/GPS_v8_ref/GPS_v8_ref_graph.gt\nNetwork loaded: 2946 samples\n\nDone\nIn the results/poppunk directory we can see the following files:\npoppunk_clusters.csv  poppunk.dists.npy  poppunk.dists.pkl  poppunk_external_clusters.csv  poppunk.h5  poppunk_unword_clusters.csv\nWe can check the results of the clustering by opening the poppunk_clusters.csv file in the results/poppunk directory:\nTaxon,Cluster\nERX1501203_ERR1430825_T1,2\nERX1501202_ERR1430824_T1,2\nERX1265396_ERR1192012_T1,2\nERX1265488_ERR1192104_T1,2\nERX1501204_ERR1430826_T1,2\nWe can see that all the genomes were assigned to GPSC2.",
    "crumbs": [
      "Slides",
      "Recombination",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Typing bacteria using PopPUNK</span>"
    ]
  },
  {
    "objectID": "materials/30-poppunk.html#summary",
    "href": "materials/30-poppunk.html#summary",
    "title": "32  Typing bacteria using PopPUNK",
    "section": "32.4 Summary",
    "text": "32.4 Summary\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nThere are whole genome based alternatives to MLST such as cgMLST and PopPUNK\nPopPUNK can be used to assign existing GPSCs to new genomes.\n\n\n\n\nReferences\nGladstone RA, Lo SW, Lees JA, Croucher NJ, van Tonder AJ, Corander J, Page AJ, Marttinen P, Bentley LJ, Ochoa TJ, Ho PL, du Plessis M, Cornick JE, Kwambana-Adams B, Benisty R, Nzenze SA, Madhi SA, Hawkins PA, Everett DB, Antonio M, Dagan R, Klugman KP, von Gottberg A, McGee L, Breiman RF, Bentley SD; Global Pneumococcal Sequencing Consortium. International genomic definition of pneumococcal lineages, to contextualise disease, antibiotic resistance and vaccine impact. EBioMedicine 2019. DOI",
    "crumbs": [
      "Slides",
      "Recombination",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Typing bacteria using PopPUNK</span>"
    ]
  },
  {
    "objectID": "materials/31-pathogenwatch.html",
    "href": "materials/31-pathogenwatch.html",
    "title": "33  Pathogenwatch 2",
    "section": "",
    "text": "TipLearning Objectives\n\n\n\n\nAnalyse S. penumoniae genomes on Pathogenwatch and extract relevant information to use as metadata to annotate phylogenetic trees.\n\n\n\n\n\n\n\n\n\nExerciseExercise 1 - Analysing Pneumococcal genomes with Pathogenwatch\n\n\n\n\n\n\nWhilst you can upload FASTQ files to Pathogenwatch, it’s quicker if we work with already assembled genomes. We’ve provided pre-processed results for the S. pneumoniae data generated with assembleBAC in the preprocessed/ directory which you can use to upload to Pathogenwatch in this exercise.\n\nUpload the assembled S. pneumoniae genomes to Pathogenwatch.\nOnce Pathogenwatch has finished processing the genomes, save the results to a collection called Chaguza Serotype 1.\nDownload the Typing and AMR profile tables to the S_pneumoniae directory.\nRename the tables to chaguza-serotype-1-typing.csv and chaguza-serotype-1-amr-profile.csv respectively.\nMerge the two tables with sample_info.csv by running the merge_pneumo_data.py script in the scripts directory. Make sure you are on the base software environment.\n\nNote: If you have not managed to run the Pathogenwatch analysis, we provide the output files from the previous steps in preprocessed/pathogenwatch that you can use instead.\n\n\n\n\n\n\n\n\nHintHint\n\n\n\n\n\n\nRefer back to Pathogenwatch if you need a reminder on how to perform these tasks.\n\n\n\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\n\nWe opened Pathogenwatch in our web browser and logged in. We then clicked on UPLOAD, the Upload FASTA(s) button in the “Single Genome FASTAs” section and the + button before navigating to the preprocessed/assemblebac/assemblies directory. We then selected all the assembly files and clicked Open on the dialogue window.\nWe waited for Pathogenwatch to finish processing the genomes, clicked the VIEW GENOMES button, then saved the results to a collection called Chaguza Serotype 1 by clicking Selected Genomes –&gt; Create Collection and adding Chaguza Serotype 1 to the Title box. Finally, we clicked Create Now button to create our collection.\n\n\n\nWe clicked on the download icon in the top right-hand corner and selected Typing table.\nWe clicked on the download icon in the top right-hand corner and selected AMR profile.\nWe renamed the files to chaguza-serotype-1-typing.csv and chaguza-serotype-1-amr-profile.csv respectively and moved them to the S_pneumoniae directory.\nWe went back to the base software environment with mamba activate base\nWe ran the merge_pneumo_data.py script to create a TSV file called pneumo_metadata.tsv in your analysis directory:\n\npython scripts/merge_pneumo_data.py -s sample_info.csv -t chaguza-serotype-1-typing.csv -a chaguza-serotype-1-amr-profile.csv",
    "crumbs": [
      "Slides",
      "Recombination",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Pathogenwatch 2</span>"
    ]
  },
  {
    "objectID": "materials/32-tree_visualization.html",
    "href": "materials/32-tree_visualization.html",
    "title": "34  Visualising phylogenies with ggtree",
    "section": "",
    "text": "34.1 Plotting phylogenetic trees with R\nAlthough trees can be visualised with software such as FigTree or Microreact, these tools can be limited in their customisation options. In particular, to produce publication-ready figures, where you may want to produce trees that fit a certain size, resolution, font size, etc.\nAn alternative for tree visualisation is the R package ggtree, which provides many functions to customise your trees. It can produce trees in a range of layouts, as well as display additional information (such as metadata) alongside the tree. This includes basic configuration such as colouring the branches and tips of the tree, to more advanced displays such as adding heatmaps and barplots aligned with the tree branches.\nThe package supports common tree formats (such as Newick, produced by IQ-Tree), making it easy to integrate into your R analysis workflows. In the following practical/exercise, we will demonstrate the basic usage of this package, to visualise the tree and MLST typing we produced for S. pneumoniae. However, the package developer has written a book, which offers an extensive manual to using this package: Data Integration, Manipulation and Visualization of Phylogenetic Trees.",
    "crumbs": [
      "Slides",
      "Recombination",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Visualising phylogenies with ggtree</span>"
    ]
  },
  {
    "objectID": "materials/32-tree_visualization.html#plotting-phylogenetic-trees-with-r",
    "href": "materials/32-tree_visualization.html#plotting-phylogenetic-trees-with-r",
    "title": "34  Visualising phylogenies with ggtree",
    "section": "",
    "text": "ExerciseExercise 1 - Plot the Pneumococcal tree with ggtree\n\n\n\n\n\n\n\nOpen the script 06-plot_phylogeny.R in the scripts directory in RStudio.\nRun the script line-by-line to generate the annotated phylogenetic tree.\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nThe resulting tree should look similar to this:\n\n\n\nSerotype 1 phylogenetic tree generated with ggtree",
    "crumbs": [
      "Slides",
      "Recombination",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Visualising phylogenies with ggtree</span>"
    ]
  },
  {
    "objectID": "materials/32-tree_visualization.html#summary",
    "href": "materials/32-tree_visualization.html#summary",
    "title": "34  Visualising phylogenies with ggtree",
    "section": "34.2 Summary",
    "text": "34.2 Summary\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nggtree is a highly customisable R package for generating publication quality images of phylogenetic trees.",
    "crumbs": [
      "Slides",
      "Recombination",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Visualising phylogenies with ggtree</span>"
    ]
  },
  {
    "objectID": "materials/34-reverse_vaccinology.html",
    "href": "materials/34-reverse_vaccinology.html",
    "title": "35  Identifying vaccine candidates with reverse vaccinology",
    "section": "",
    "text": "35.1 The Traditional vs. Reverse Vaccinology Approach\nTo understand the innovation of reverse vaccinology, it’s helpful to compare it to the conventional method.",
    "crumbs": [
      "Slides",
      "Vaccines",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Identifying vaccine candidates with reverse vaccinology</span>"
    ]
  },
  {
    "objectID": "materials/34-reverse_vaccinology.html#the-traditional-vs.-reverse-vaccinology-approach",
    "href": "materials/34-reverse_vaccinology.html#the-traditional-vs.-reverse-vaccinology-approach",
    "title": "35  Identifying vaccine candidates with reverse vaccinology",
    "section": "",
    "text": "Traditional Vaccinology (Forward): This classic method involves growing large quantities of a pathogen in the lab, inactivating it, and then breaking it down to isolate its constituent parts (like surface proteins or polysaccharides). These isolated components are then tested, often through a long process of trial and error, to see if they can induce a protective immune response. This approach is limited to pathogens that can be cultured in the lab and often only identifies the most abundant components, potentially missing other effective antigens.\nReverse Vaccinology (Backward): This modern approach begins with the complete genomic sequence of the pathogen. Using computational tools (a process known as in silico analysis), the entire genome is scanned to predict which genes code for proteins that are likely to be good vaccine candidates. This allows for a comprehensive and rational selection of targets before any lab work on the pathogen itself even begins.",
    "crumbs": [
      "Slides",
      "Vaccines",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Identifying vaccine candidates with reverse vaccinology</span>"
    ]
  },
  {
    "objectID": "materials/34-reverse_vaccinology.html#the-reverse-vaccinology-workflow",
    "href": "materials/34-reverse_vaccinology.html#the-reverse-vaccinology-workflow",
    "title": "35  Identifying vaccine candidates with reverse vaccinology",
    "section": "35.2 The Reverse Vaccinology Workflow",
    "text": "35.2 The Reverse Vaccinology Workflow\nThe process can be broken down into five key steps, moving from the digital sequence to a tangible vaccine candidate.\n\nGenome Sequencing and In Silico Analysis The process starts by sequencing the entire genome of the target bacterium. Bioinformatics software then analyzes this genetic information to create a list of potential antigens. The algorithms prioritize genes that code for proteins with desirable characteristics, such as:\n\nSurface Location: Proteins that are secreted or located on the pathogen’s outer surface are ideal targets because they are easily accessible to the host’s immune system.\nVirulence Factors: Proteins critical for the bacterium’s survival or ability to cause disease are excellent candidates.\nBroad Conservation: The protein should be present in most or all strains of the pathogen to ensure the vaccine provides broad protection.\nNo Human Homology: The selected proteins must be dissimilar to human proteins to prevent the immune system from attacking the body’s own cells (autoimmunity).\n\nCloning and Expression The genes that were flagged as promising candidates during the in silico screening are then synthesized and cloned into a laboratory expression system, typically E. coli. This turns the bacteria into tiny factories that produce large quantities of the selected proteins for testing.\nImmunological Screening The purified proteins are used to immunize lab animals, such as mice. Scientists then analyze the animals’ immune response by:\n\nMeasuring antibody levels in their blood serum.\nTesting whether these antibodies can effectively kill the target bacterium in a lab setting (e.g., through a serum bactericidal assay).\n\nCandidate Validation Proteins that elicit a strong, protective immune response in the animal models are identified as the top vaccine candidates. These candidates undergo further testing to confirm their potential before moving into human clinical trials.\n\nFor the purposes of this training course, we will focus on the first step: Genome sequencing and in silico analysis.",
    "crumbs": [
      "Slides",
      "Vaccines",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Identifying vaccine candidates with reverse vaccinology</span>"
    ]
  },
  {
    "objectID": "materials/34-reverse_vaccinology.html#case-study-the-meningococcus-b-menb-vaccine",
    "href": "materials/34-reverse_vaccinology.html#case-study-the-meningococcus-b-menb-vaccine",
    "title": "35  Identifying vaccine candidates with reverse vaccinology",
    "section": "35.3 Case study: the Meningococcus B (MenB) vaccine",
    "text": "35.3 Case study: the Meningococcus B (MenB) vaccine\nThe most prominent success story for reverse vaccinology is the development of the vaccine against Neisseria meningitidis serogroup B (MenB), a leading cause of bacterial meningitis.\nFor decades, a MenB vaccine remained elusive. Traditional vaccine strategies often target the polysaccharide capsule surrounding the bacteria. However, the MenB capsule is chemically identical to a molecule found on human nerve cells, so using it in a vaccine could trigger a dangerous autoimmune reaction.\nBy applying reverse vaccinology, researchers sequenced the MenB genome and identified hundreds of potential protein antigens. After extensive screening, a few key proteins were selected that induced a robust and protective immune response. These proteins formed the basis of the Bexsero (4CMenB) vaccine, which is now used worldwide to prevent MenB disease. This breakthrough would have been incredibly difficult, if not impossible, using conventional methods alone.",
    "crumbs": [
      "Slides",
      "Vaccines",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Identifying vaccine candidates with reverse vaccinology</span>"
    ]
  },
  {
    "objectID": "materials/34-reverse_vaccinology.html#running-a-reverse-vaccinology-workflow",
    "href": "materials/34-reverse_vaccinology.html#running-a-reverse-vaccinology-workflow",
    "title": "35  Identifying vaccine candidates with reverse vaccinology",
    "section": "35.4 Running a reverse vaccinology workflow",
    "text": "35.4 Running a reverse vaccinology workflow\nThe purpose of this exercise is to familiarize you with the types of analyses involved in a reverse vaccinology workflow. We will not be running a complete analysis here, but rather focusing on the key steps. The end point of this exercise will be a list of potential vaccine candidates. We have provided a mamba environment with all the necessary software pre-installed. Before we start, please activate the environment:\nmamba activate reverse-vaccinology\nNow navigate to the N_meningitidis directory within the course materials.\n\n35.4.1 Genome annotation\nThe first step is to identify all the potential genes and translate them into protein sequences. We have used Bakta for this (you should already be familiar with Bakta as assembleBAC runs Bakta as part of the annotation step) and have provided the protein sequences that Bakta predicted. These are in files with the suffix .faa located within the data/faa folder.\n\n\n35.4.2 Subcellular location prediction\nNow, we need to predict where each protein resides in the cell. We are interested in Outer Membrane and Secreted proteins. We will use PSORTb, a bioinformatics tool that predicts the subcellular localization of bacterial proteins based on their amino acid sequences. It uses a combination of machine learning algorithms and curated databases to classify proteins into different cellular compartments, such as cytoplasm, inner membrane, periplasm, outer membrane, and extracellular space. The command for running PSORTb is as follows:\n./psortb_app -i data/faa/ERX029793.faa -r results/psortb/ERX029793 -n\nThe options used are:\n\n-i - the input file containing amino acid sequences in FASTA format.\n-r - the output directory where results will be saved.\n-n - specifies that the input sequences are from a Gram-negative bacterium.\n\n\n\n\n\n\n\nExerciseExercise 1 - Run PSORTb on Neisseria meningitidis amino acid sequences\n\n\n\n\n\n\nTo run PSORTb on the .faa files produced by Bakta, we have provided a script called 01-run_psortb.sh, located in the scripts folder. As PSORTb can take some time to run, we will only be running it on two of the faa files. For the downstream analyses, we will use pre-computed results for all samples.\n\nOpen the script, which you will notice is composed of two sections:\n\n#### Settings #### where we define some variables for input and output files names. If you were running this script on your own data, you may want to edit the directories in this section.\n#### Analysis #### this is where PSORTb is run on each amino acid file. You should not change the code in this section.\n\nRun the script with bash scripts/01-run_psortb.sh. If the script is running successfully it should print a message on the screen as the samples are processed.\nOnce the analysis finishes open the ERX109731_locations.csv file in the results/psortb/locations folder. How many proteins were predicted to be located in the Outer Membrane? Hint you can use grep to count the number of lines containing “OuterMembrane”.\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nWe opened the script 01-run_psortb.sh and these are the settings we used:\n\naa_dir=\"data/faa\" - the name of the directory with .faa files in it.\noutdir=\"results/psortb\" - the name of the directory where we want to save our results.\n\nWe then ran the script using bash scripts/01-run_psortb.sh. The script prints a message while it’s running:\nSaving results to /rds/project/rds-PzYD5LltalA/Teaching/bacterial_genomics/N_meningitidis/results/psortb/ERX029793/20251030144956_psortb_gramneg.txt\nSuccessfully parsed 1954 proteins.\nResults saved to: ERX029793_locations.csv\nPSORTb creates a text file for each set of amino acids containing the protein ID and its predicted location (e.g., “OuterMembrane”, “Cytoplasmic”, “Secreted”). Once PSORTb has finished running, the script extracts the predicted locations and saves them in a csv file for easier analysis.\nTo count the number of proteins predicted to be located in the Outer Membrane in the ERX029793_locations.csv file, we can use the following command:\ngrep -c \"OuterMembrane\" results/psortb/locations/ERX029793_locations.csv\nThis tells us that there are 43 proteins predicted to be located in the Outer Membrane.\n\n\n\n\n\n\n\n\n\n\n35.4.3 Conservation analysis\nTo find proteins that are present in most of our strains, we can cluster all proteins from all genomes using CD-HIT. We have provided faa files for all samples in the data/faa folder. CD-HIT will group similar proteins together based on a specified sequence identity threshold. For this exercise, we will use a threshold of 95% identity, meaning that proteins that are at least 95% identical will be grouped into the same cluster. This helps us identify conserved proteins across different strains of Neisseria meningitidis. To run CD-HIT, we can use the following command:\n# run cd-hit to cluster proteins at 95% identity\ncd-hit -i data/faa/all_proteins.faa -o results/cd-hit/conserved_clusters.txt -c 0.95 -d 0\nThe options we used are:\n\n-i - the input file containing all amino acid sequences in FASTA format.\n-o - the output file where clustered results will be saved.\n-c - the sequence identity threshold (0.95 for 95% identity).\n-d - controls the length of the description in the output file (0 means full length).\n\n\n\n\n\n\n\nExerciseExercise 2 - Run CD-HIT on Neisseria meningitidis amino acid sequences\n\n\n\n\n\n\nTo run CD-HIT on the .faa files produced by Bakta, we have provided a script called 02-run_cd-hit.sh, located in the scripts folder.\n\nOpen the script, which you will notice is composed of two sections:\n\n#### Settings #### where we define some variables for input and output files names. If you were running this script on your own data, you may want to edit the directories in this section.\n#### Analysis #### this is where CD-HIT is run on each amino acid file. You should not change the code in this section.\n\nRun the script with bash scripts/02-run_cd-hit.sh. If the script is running successfully it should print a message on the screen as the samples are processed.\nHow many clusters were identified by CD-HIT? Hint: You can find this information in the output printed to the screen while the script is running.\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nWe opened the script 02-run_cd-hit.sh and these are the settings we used:\n\naa_dir=\"data/faa\"\" - the name of the directory with .faa files in it.\noutdir=\"results/cd-hit\" - the name of the directory where we want to save our results.\n\nWe then ran the script using bash scripts/03-run_cd-hit.sh. The script prints a message while it’s running:\n================================================================\nProgram: CD-HIT, V4.8.1 (+OpenMP), Apr 24 2025, 22:00:32\nCommand: cd-hit -i results/cd-hit/all_proteins.faa -o\n         results/cd-hit/conserved_clusters.txt -c 0.95 -d 0\n\nStarted: Thu Oct 30 15:31:39 2025\n================================================================\n                            Output\n----------------------------------------------------------------\ntotal seq: 133420\nCD-HIT creates a text file containing clusters of similar proteins. Each cluster represents a group of proteins that are highly similar to each other, indicating they may be the same protein found in different strains. Like the PSORTb script, once CD-HIT has finished running, the script extracts the clustering information and saves it in a csv file for easier analysis.\nCD-HIT identified a total of 5,667 clusters from the 133,420 input protein sequences.\n\n\n\n\n\n\n\n\n\n\n35.4.4 Functional annotation and human homology check\nFor function and safety, we will use DIAMOND, an ultra-fast alternative to BLAST. We will use it to compare our protein sequences against two databases:\n\nSwiss-Prot: A high-quality, manually curated database for functional annotation.\nHuman Proteome: For checking against human proteins to avoid autoimmunity.\n\nThe first step will give us the likely function of each protein, while the second step will help us filter out any proteins that are too similar to human proteins and should therefore be avoided as vaccine candidates. The diamond commands we will use are as follows:\n# Search against Swiss-Prot for functional annotation\ndiamond blastp -d databases/swissprot.dmnd -q results/cd-hit/all_proteins.faa -o results/homology_searches/function.tsv --outfmt 6 qseqid stitle evalue \n\n# Search against Human Proteome for homology check\ndiamond blastp -d databases/human_proteome.dmnd -q results/cd-hit/all_proteins.faa -o results/homology_searches/human_homology.tsv --outfmt 6 qseqid stitle evalue pident \nThe options we used are:\n\n-d - the database to search against (Swiss-Prot or Human Proteome).\n-q - the input file containing amino acid sequences in FASTA format.\n-o - the output file where results will be saved.\n--outfmt 6 - specifies the output format (tabular format with specific fields).\n\n\n\n\n\n\n\nExerciseExercise 3 - Run DIAMOND on Neisseria meningitidis amino acid sequences\n\n\n\n\n\n\nWe have provided a script to run DIAMOND on all the protein sequences against the two databases. The script is called 03-run_diamond.sh and is located in the scripts folder.\n\nOpen the script, which you will notice is composed of two sections:\n\n#### Settings #### where we define some variables for input and output files names. If you were running this script on your own data, you may want to edit the directories in this section.\n#### Analysis #### this is where DIAMOND is run on each amino acid file. You should not change the code in this section.\n\nRun the script with bash scripts/03-run_diamond.sh. If the script is running successfully it should print a message on the screen as the samples are processed.\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nWe opened the script 03-run_diamond.sh and these are the settings we used:\n\naa_dir=\"results/cd-hit\"\" - the name of the directory with .faa files in it.\noutdir=\"results/homology_searches\" - the name of the directory where we want to save our results.\nswissprot_db=\"databases/swissprot.dmnd\" - path to Swiss-Prot database\nhuman_db=\"databases/human_proteome.dmnd\" - path to Human Proteome database\n\nWe then ran the script using bash scripts/03-run_diamond.sh. The script prints a message while it’s running:\ndiamond v2.1.13.167 (C) Max Planck Society for the Advancement of Science, Benjamin Buchfink, University of Tuebingen\nDocumentation, support and updates available at http://www.diamondsearch.org\nPlease cite: http://dx.doi.org/10.1038/s41592-021-01101-x Nature Methods (2021)\n\n#CPU threads: 76\nScoring parameters: (Matrix=BLOSUM62 Lambda=0.267 K=0.041 Penalties=11/1)\nThe script will produce two sets of results: one for the Swiss-Prot database (function.tsv) and another for the human proteome (human_homology.tsv). Each result file contains information about the best match for each protein, including the protein’s function (from Swiss-Prot) and any significant similarity to human proteins.\n\n\n\n\n\n\n\n\n\n\n35.4.5 Compiling vaccine candidates\nNow that we have all the necessary data, we can compile a list of potential vaccine candidates by integrating the results from PSORTb, CD-HIT, and DIAMOND. We have provided a script called combine_results.py that does just that. This script takes the outputs from the previous analyses and combines them into a single CSV file called vaccine_candidates.csv. This file contains all the relevant information for each protein, including its predicted location, conservation across strains, functional annotation, and human homology status. To run the script, simply execute the following command in your terminal:\npython scripts/combine_results.py --locations \"preprocessed/psortb/locations/*.csv\" --conservation results/cd-hit/conserved_clusters_conservation.csv --function results/homology_searches/function.tsv --human results/homology_searches/human_homology.tsv --output vaccine_candidates.csv\nOpen the vaccine_candidates.csv file to explore the compiled data. The four proteins included in the MenB vaccine are fHbp, NHBA, NadA, PorA. Are they present in this list? You may need to search by their gene names or their product descriptions. Unfortunately, due to the small number of genomes we are using in this exercise, it is unlikely that the four vaccine components will be present in the final list.\nFor the purposes of this training course, we selected a small number of Neisseria meningitidis genomes at random. In a real reverse vaccinology study, you would typically analyze a much larger dataset to ensure that the identified vaccine candidates are broadly conserved across diverse strains of the pathogen.",
    "crumbs": [
      "Slides",
      "Vaccines",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Identifying vaccine candidates with reverse vaccinology</span>"
    ]
  },
  {
    "objectID": "materials/34-reverse_vaccinology.html#summary",
    "href": "materials/34-reverse_vaccinology.html#summary",
    "title": "35  Identifying vaccine candidates with reverse vaccinology",
    "section": "35.5 Summary",
    "text": "35.5 Summary\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nReverse vaccinology is a genome-based approach to identify potential vaccine candidates by analyzing the genetic makeup of pathogens.\nThe workflow involves genome sequencing, subcellular location prediction, conservation analysis, functional annotation, and human homology checks.\nBioinformatics tools like PSORTb, CD-HIT, and DIAMOND play crucial roles in the reverse vaccinology process.\nThe MenB vaccine is a successful example of reverse vaccinology, demonstrating its potential to revolutionize vaccine development.",
    "crumbs": [
      "Slides",
      "Vaccines",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Identifying vaccine candidates with reverse vaccinology</span>"
    ]
  },
  {
    "objectID": "materials/34-pan-genome-vaccinology.html",
    "href": "materials/34-pan-genome-vaccinology.html",
    "title": "36  Pan-Genome Analysis for Vaccine Development",
    "section": "",
    "text": "36.1 Pan-Genome Analysis for Vaccine Development\nWhile reverse vaccinology is powerful for a single genome, its true potential is unlocked when applied to the pan-genome of a bacterial species. The pan-genome represents the entire set of genes found across all strains of a given species. It provides a complete picture of the genetic diversity and is crucial for designing vaccines with broad coverage.\nAs a reminder, the pan-genome is composed of three main parts:\nThis approach allows for two distinct vaccine development strategies.",
    "crumbs": [
      "Slides",
      "Vaccines",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Pan-Genome Analysis for Vaccine Development</span>"
    ]
  },
  {
    "objectID": "materials/34-pan-genome-vaccinology.html#pan-genome-analysis-for-vaccine-development",
    "href": "materials/34-pan-genome-vaccinology.html#pan-genome-analysis-for-vaccine-development",
    "title": "36  Pan-Genome Analysis for Vaccine Development",
    "section": "",
    "text": "Core Genome: Genes shared by all strains. These are typically essential for basic survival and are excellent targets for a broad-spectrum vaccine that aims to protect against the entire species.\nAccessory Genome (or Dispensable Genome): Genes present in some, but not all, strains. This part of the genome often contains virulence factors, antibiotic resistance genes, and genes for adapting to specific environments. These are ideal for a targeted vaccine aimed at preventing disease caused by particularly virulent lineages.\nUnique Genes: Genes specific to a single strain.\n\n\n\n36.1.1 Strategy 1: Targeting the Core Genome for a Broad-Spectrum Vaccine\nThe goal here is to identify conserved proteins that are present in every strain of the pathogen, ensuring that the vaccine will be effective regardless of which strain a person is infected with.\n\nCore genome workflow\n\nGenome Collection and Annotation:\n\nInput: Dozens to hundreds of high-quality genome sequences from diverse strains of the target bacterial species.\nProcess: Each genome is annotated (using a tool like Bakta) to identify all its protein-coding genes.\n\nPan-Genome Analysis:\n\nTool: A specialized pan-genome tool like Roary or Panaroo is used.\nProcess: The software compares all the annotated genes from every genome and clusters them into orthologous groups (gene families). It then categorizes each gene family as “core” (present in &gt;99% of strains) or “accessory.”\n\nCore Genome Candidate Selection (In Silico):\n\nThe list of core genes is filtered using the same reverse vaccinology criteria:\n\nPredicted Location: The protein must be on the Outer Membrane or Secreted to be accessible to the immune system (predicted with PSORTb or SignalP).\nSafety Screen: The protein must have no significant homology to human proteins to avoid autoimmunity (checked with DIAMOND or BLASTp against the human proteome).\nVirulence Potential: Proteins with functions related to adhesion, invasion, or nutrient acquisition are prioritized.\n\n\nDownstream Validation:\n\nThe handful of promising core proteins are then produced in the lab and tested immunologically, following the standard reverse vaccinology pipeline.\n\n\n\n\nPan-Genome Analysis\nWe can use Panaroo to calculate the pan-genome of our set of Neisseria meningitidis genomes and identify the core genes. We can then parse the Panaroo output to extract the amino acid sequences of the core genes which can be used as input for the reverse vaccinology workflow described in the previous chapter.\n\n\n\n\n\n\nExerciseExercise 1 - Run panaroo on Neisseria meningitidis annotation files\n\n\n\n\n\n\nTo run panaroo on the .gff3 files produced by Bakta, we have provided a script called 04-run_panaroo.sh, located in the scripts folder.\n\nActivate the software environment: mamba activate panaroo.\nRun the script using bash scripts/04-run_panaroo.sh.\nWhen the analysis starts you will get several messages and progress bars print on the screen.\nCheck the output files generated by Panaroo in the results/panaroo folder. How many core gene sequences were extracted?\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nAs it runs, Panaroo prints several messages to the screen.\nWe specified results/panaroo as the output directory for the pan-genome analysis tool Panaroo. We can see all the output files it generated:\nls results/panaroo\naligned_gene_sequences                core_alignment_header.embl        gene_presence_absence_roary.csv\nalignment_entropy.csv                 core_gene_alignment.aln           pan_genome_reference.fa\ncombined_DNA_CDS.fasta                core_genome_protein_sequences.fa  core_gene_alignment_filtered.aln  \ncombined_protein_CDS.fasta            final_graph.gml                   struct_presence_absence.Rtab\ncombined_protein_cdhit_out.txt        gene_data.csv                     summary_statistics.txt\ncombined_protein_cdhit_out.txt.clstr  gene_presence_absence.Rtab        pre_filt_graph.gml\ncore_alignment_filtered_header.embl   gene_presence_absence.csv\nTo count the number of core genes, we can look at the core_genome_protein_sequences.fa file, which contains the sequences of all core genes identified by Panaroo.\ngrep -c \"&gt;\" results/panaroo/core_genome_protein_sequences.fa\nThis tells us there are 1,425 core genes identified across the Neisseria meningitidis genomes we analyzed.\n\n\n\n\n\n\n\n\n\n\nReverse Vaccinology Workflow on Core Genes\nNow that we have identified the core genes using Panaroo, we can proceed with the reverse vaccinology workflow as described in the previous section. The core gene sequences extracted from Panaroo will serve as the input for subcellular localization prediction, homology searches, and functional annotation to identify potential vaccine candidates.\n\n\n\n\n\n\nExerciseExercise 2 - Run the reverse vaccinology workflow on core gene sequences identified by Panaroo\n\n\n\n\n\n\nWe have provided a script to run the reverse vaccinology workflow on the core gene sequences extracted from Panaroo. The script is called 05-run_reverse_vaccinology_core.sh and is located in the scripts folder.\n\nActivate the software environment: mamba activate reverse-vaccinology.\nRun the script using bash scripts/05-run_reverse_vaccinology_core.sh.\nWhen the analysis starts you will get several messages print on the screen.\nCheck the output files generated by the script in the results/reverse_vaccinology_core folder. How many potential vaccine candidates were identified?\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nAs it runs, the script prints several messages to the screen.\nWe specified results/reverse_vaccinology_core as the output directory for the reverse vaccinology workflow script. We can see all the output files it generated:\nls results/reverse_vaccinology_core\ncore_vaccine_candidates.csv  function.tsv  human_homology.tsv  panaroo  panaroo_locations.csv\nTo count the number of potential vaccine candidates, we can look at the core_vaccine_candidates.csv file, which contains the genes identified as potential vaccine targets by our workflow.\nwc -l results/reverse_vaccinology_core/core_vaccine_candidates.csv\nThis tells us there are 24 lines in the core_vaccine_candidates.csv so if we exclude the header there are 23 potential vaccine candidates.\n\n\n\n\n\n\n\n\n\n\n\n36.1.2 Strategy 2: Targeting the Accessory Genome for a Virulence-Specific Vaccine\nThis strategy focuses on identifying genes that are not essential for the bacterium’s survival but are strongly associated with its ability to cause severe disease. A vaccine targeting these proteins would not necessarily prevent colonization but would prevent the development of invasive disease.\n\nAccessory Genome Workflow\n\nGenome Collection with Metadata:\n\nInput: This requires a well-curated set of genomes where each strain is labeled with clinical metadata (e.g., “invasive disease” vs. “asymptomatic carriage”).\n\nPan-Genome Analysis:\n\nThe process is the same as above, using panaroo or a similar tool to generate a complete pan-genome and classify genes as core or accessory.\n\nGenome-Wide Association Study (GWAS):\n\nTool: A bacterial GWAS tool like Scoary or Pyseer is used.\nProcess: The tool takes the pan-genome output (a presence/absence matrix of all accessory genes) and the clinical metadata. It then performs statistical tests to find which accessory genes are significantly more common in the “invasive disease” group compared to the “carriage” group.\n\nVirulent Gene Candidate Selection:\n\nThe list of disease-associated accessory genes is then filtered using the standard reverse vaccinology criteria (subcellular location, human homology).\n\nDownstream Validation:\n\nThe top candidates are validated in the lab. A successful vaccine would generate antibodies that neutralize these specific virulence factors, effectively disarming the pathogen.\n\n\n\n\nGenome-Wide Association Study (GWAS) on Panaroo output\nWe can use pyseer to perform a GWAS on the accessory genome of our Neisseria meningitidis genomes to identify genes associated with invasive disease. We will use the gene presence/absence matrix generated by Panaroo and a phenotype file indicating which strains are associated with invasive disease.\nThe first step is to estimate the population structure of our genomes (we do this to filter out the noise from shared ancestry and find the specific genes that are truly linked to our trait). We will do this using a pairwise distance matrix produced using mash:\n# activate the accessory-vaccinology environment\nmamba activate accessory-vaccinology\n\n# create output directory for mash results\nmkdir -p results/mash\n\n# create mash sketch from genome assemblies\nmash sketch -s 10000 -o results/mash/mash_sketch data/genomes/*.fa\n\n# compute pairwise distances\nmash dist results/mash/mash_sketch.msh results/mash/mash_sketch.msh | square_mash &gt; results/mash/mash.tsv\n\n# clean up the strain names in the mash distance file\nsed -i 's/_contigs//g' results/mash/mash.tsv\nLet’s perform an MDS (multi-dimensional scaling) on these distances and look at a scree plot to choose the number of dimensions (a measure of our population structure) to retain:\nscree_plot_pyseer results/mash/mash.tsv --output results/mash/scree_plot.png\nWe got the following scree plot:\n\n\n\nScree plot of MDS on mash distances\n\n\nThere is a drop after about 9 dimensions, so we will use this many. This is subjective, and you may choose to include many more. This is a sensitivity/specificity tradeoff – choosing more components is more likely to reduce false positives from population structure, at the expense of power. Using more components will also slightly increase computation time.\nWe can now run the analysis on the gene_presence_absence.Rtab file produced by Panaroo, using the phenotype file resources/gwas/disease.pheno, which indicates which strains are associated with invasive disease:\n# create output directory for pyseer results\nmkdir -p results/pyseer\n\npyseer --phenotypes resources/gwas/disease.pheno \\\n       --pres results/panaroo/gene_presence_absence.Rtab \\\n       --distances results/mash/mash.tsv \\\n       --save-m results/pyseer/mash_mds \\\n       --max-dimensions 9 \\\n       &gt; results/pyseer/disease_COGs.txt\nThe options we used are:\n\n--phenotypes: the phenotype file indicating which strains are associated with invasive disease.\n--pres: the gene presence/absence matrix from Panaroo.\n--distances: the pairwise distance matrix from mash.\n--save-m: save the MDS components to a file for later use.\n--max-dimensions: the number of MDS dimensions to include as covariates in the model.\n\nThe output file results/pyseer/disease_COGs.txt contains the results of the pyseer GWAS analysis. We can filter this file to identify genes that are significantly associated with invasive disease (e.g., using a p-value threshold of 0.05 after Bonferroni correction). We can use an awk command to filter the results:\nawk -F'\\t' 'NR == 1 || ($4 &lt; 0.05 && $17 !~ /bad-chisq/ && $17 !~ /high-bse/)' results/pyseer/disease_COGs.txt &gt; results/pyseer/significant_hits.tsv\nThe resulting file results/pyseer/significant_hits.tsv contains the 9 genes that are significantly associated with invasive disease, which can be further analyzed to identify potential vaccine candidates:\nvariant af  filter-pvalue   lrt-pvalue\ngroup_2118  8.09E-01    4.94E-01    4.26E-02\ngroup_1010  5.88E-01    1.33E-01    3.63E-02\ngroup_1113  5.29E-01    7.62E-01    4.06E-02\ngroup_762   5.15E-01    4.80E-01    3.58E-02\ngroup_185   4.56E-01    5.86E-02    2.54E-03\ngroup_1006  4.12E-01    3.56E-01    1.90E-02\ngroup_266   3.53E-01    1.36E-03    4.93E-02\ngroup_983   3.38E-01    1.45E-01    2.47E-02\ngroup_2141  3.09E-01    5.12E-01    2.97E-02\n\n\n\n\n\n\nExerciseExercise 3 - Run the reverse vaccinology workflow on genes associated with invasive disease\n\n\n\n\n\n\nWe have provided a script to run the reverse vaccinology workflow on the genes identified as being significantly associated with invasive disease by pyseer. The script is called 06-run_acccessory_vaccinology_core.sh and is located in the scripts folder.\n\nActivate the software environment: mamba activate reverse-vaccinology.\nRun the script using bash scripts/06-run_accessory_vaccinology.sh.\nWhen the analysis starts you will get several messages print on the screen.\nCheck the output files generated by the script in the results/reverse_vaccinology_accessory folder. How many potential vaccine candidates remain out of the original list?\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nAs it runs, the script prints several messages to the screen.\nWe specified results/reverse_vaccinology_accessory as the output directory for the reverse vaccinology workflow script. We can see all the output files it generated:\nls results/reverse_vaccinology_accessory\naccessory  accessory_locations.csv  accessory_vaccine_candidates.csv  function.tsv  human_homology.tsv\nTo count the number of potential vaccine candidates, we can look at the accessory_vaccine_candidates.csv file, which contains the genes identified as potential vaccine targets by our workflow.\nwc -l results/reverse_vaccinology_accessory/accessory_vaccine_candidates.csv\nThis tells us there is only one line in the accessory_vaccine_candidates.csv so if we exclude the header there are no potential vaccine candidates. Why might this be? To answer this, we can look at the accessory_locations.csv file to see where the genes identified by pyseer are predicted to be located:\ncat results/reverse_vaccinology_accessory/accessory_locations.csv\nProtein_ID,Predicted_Location\ngroup_762,Unknown\ngroup_2141,Unknown\ngroup_1010,CytoplasmicMembrane\ngroup_266,Unknown\ngroup_1113,Unknown\ngroup_983,Unknown\ngroup_2118,Unknown\ngroup_185,Cytoplasmic\ngroup_1006,Unknown\nFrom this we can see that none of the genes identified by pyseer are predicted to be located on the outer membrane or secreted, which is why none of them were identified as potential vaccine candidates.",
    "crumbs": [
      "Slides",
      "Vaccines",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Pan-Genome Analysis for Vaccine Development</span>"
    ]
  },
  {
    "objectID": "materials/34-pan-genome-vaccinology.html#summary",
    "href": "materials/34-pan-genome-vaccinology.html#summary",
    "title": "36  Pan-Genome Analysis for Vaccine Development",
    "section": "36.2 Summary",
    "text": "36.2 Summary\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nPan-genome analysis enhances reverse vaccinology by considering the full genetic diversity of a bacterial species.\nTargeting the core genome allows for the development of broad-spectrum vaccines effective against all strains.\nTargeting the accessory genome enables the design of vaccines that specifically prevent invasive disease by neutralizing virulence factors.\nBioinformatics tools like Roary, Panaroo, Scoary, and Pyseer are essential for pan-genome analysis and GWAS in bacterial vaccine development.",
    "crumbs": [
      "Slides",
      "Vaccines",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Pan-Genome Analysis for Vaccine Development</span>"
    ]
  },
  {
    "objectID": "materials/33-intro_amr.html",
    "href": "materials/33-intro_amr.html",
    "title": "37  Introduction to Antimicrobial resistance",
    "section": "",
    "text": "37.1 Antimicrobial Resistance (AMR) analysis\nAntimicrobial resistance (AMR) is a phenomenon where microorganisms, such as bacteria, evolve in a way that reduces the effectiveness of antimicrobial drugs, including antibiotics. This occurs due to the overuse and misuse of these drugs, which exerts selective pressure on the microorganisms. As a result, bacteria may develop or acquire genetic changes that enable them to survive exposure to antimicrobial agents, making the drugs less effective or entirely ineffective. AMR poses a significant global health threat, as it can lead to infections that are challenging to treat, potentially causing increased morbidity and mortality. Efforts to combat AMR include responsible antibiotic use, developing new drugs, and enhancing infection prevention and control measures.\nAccording to the WHO, antimicrobial resistance (AMR) has evolved into a global concern for public health. This stems from various harmful bacterial strains developing resistance to antimicrobial medications, including antibiotics. As part of our analysis, we will now focus on identifying AMR patterns connected to our S. pneumoniae isolates.\nNumerous software tools have been created to predict the presence of genes linked to AMR in genome sequences. Estimating the function of a gene or protein solely from its sequence is complex, leading to varying outcomes across different software tools. It is advisable to employ multiple tools and compare their findings, thus increasing our confidence in identifying which antimicrobial drugs might be more effective for treating patients infected with the strains we’re studying.",
    "crumbs": [
      "Slides",
      "Antimicrobial Resistance",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Introduction to Antimicrobial resistance</span>"
    ]
  },
  {
    "objectID": "materials/33-intro_amr.html#antimicrobial-resistance-amr-analysis",
    "href": "materials/33-intro_amr.html#antimicrobial-resistance-amr-analysis",
    "title": "37  Introduction to Antimicrobial resistance",
    "section": "",
    "text": "WarningDo not use reference-based pseudogenomes for AMR analysis\n\n\n\nGenome consensus sequences, obtained using reference-based alignment, are a fast method to obtain the mutations in a large number of isolates. However, these pseudogenomes are biased to the reference genome used. For example, if a particular sequence is missing from the reference genome, it will not be present in the pseudogenome.\nThe bias created by using a reference genome is tolerable for phylogenetic applications. However, when our goal is to find antimicrobial resistance factors, we need to have as much as possible a complete genome for each isolate. Therefore, for AMR scans we should use a pipeline such as avantonder/assembleBAC for de novo genome assembly.",
    "crumbs": [
      "Slides",
      "Antimicrobial Resistance",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Introduction to Antimicrobial resistance</span>"
    ]
  },
  {
    "objectID": "materials/33-intro_amr.html#summary",
    "href": "materials/33-intro_amr.html#summary",
    "title": "37  Introduction to Antimicrobial resistance",
    "section": "37.2 Summary",
    "text": "37.2 Summary\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nAMR poses significant global public health threats by diminishing the effectiveness of antibiotics, making it challenging to treat infectious diseases effectively.\nAMR software aims to identify specific genes or mutations known to confer resistance to antimicrobial agents. These tools compare input genetic sequences to known resistance genes or patterns in their associated databases.\nAMR prediction can result in false results (either false positives or false negatives). One way to overcome this limitation is to compare the results from multiple tools and, whenever possible, complement it with validation assays in the lab.",
    "crumbs": [
      "Slides",
      "Antimicrobial Resistance",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Introduction to Antimicrobial resistance</span>"
    ]
  },
  {
    "objectID": "materials/34-command_line_amr.html",
    "href": "materials/34-command_line_amr.html",
    "title": "38  Command-line AMR prediction",
    "section": "",
    "text": "38.1 Funcscan workflow\nHere, we introduce an automated workflow called nf-core/funcscan (Figure 38.1), which uses Nextflow to manage all the software and analysis steps.\nThis pipeline uses five different AMR screening tools:\nSee Course Software for a more detailed description of each tool.\nAlong with the outputs produced by the above tools, the pipeline produces a TSV file, which contains a summary of the results from all the AMR tools used:\nThis is convenient, as we can obtain the results from multiple approaches in one step.",
    "crumbs": [
      "Slides",
      "Antimicrobial Resistance",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Command-line AMR prediction</span>"
    ]
  },
  {
    "objectID": "materials/34-command_line_amr.html#sec-funcscan",
    "href": "materials/34-command_line_amr.html#sec-funcscan",
    "title": "38  Command-line AMR prediction",
    "section": "",
    "text": "ABRicate\nAMRFinderPlus (NCBI Antimicrobial Resistance Gene Finder)\nfARGene (Fragmented Antibiotic Resistance Gene idENntifiEr)\nRGI (Resistance Gene Identifier)\nDeepARG\n\n\n\n\nhamronization_combined_report.tsv - produced by a software called hAMRonization",
    "crumbs": [
      "Slides",
      "Antimicrobial Resistance",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Command-line AMR prediction</span>"
    ]
  },
  {
    "objectID": "materials/34-command_line_amr.html#running-nf-corefuncscan",
    "href": "materials/34-command_line_amr.html#running-nf-corefuncscan",
    "title": "38  Command-line AMR prediction",
    "section": "38.2 Running nf-core/funcscan",
    "text": "38.2 Running nf-core/funcscan\n\n\n\n\n\n\nFigure 38.1: Overview of the nf-core/funcscan workflow. In our case we will run the “Antimicrobial Resistance Genes (ARGs)” analysis, shown in yellow. Image source: https://nf-co.re/funcscan/1.1.2\n\n\n\nWe are going to use the assemblies we generated for S. pneumoniae using the assembleBAC pipeline as input for funcscan and these are located in preprocessed/assemblebac/assemblies\nThe funcscan pipeline requires us to prepare a samplesheet CSV file with information about the samples we want to analyse. Two columns are required:\n\nsample –&gt; a sample name of our choice (we will use the same name that we used for the assembly).\nfasta –&gt; the path to the FASTA file corresponding to that sample.\n\nYou can create this file using a spreadsheet software such as Excel, making sure to save the file as a CSV. To to get you started in creating this file, we can save a list of our assembly file names into a file:\nls preprocessed/assemblebac/assemblies/*.fa | head -n 5 &gt; samplesheet_funcscan.csv\nIn this case, we only list the first five files (head -n 5), to save time when running the pipeline. In your own data, you should get all the files.\nWe then open this file in Excel and edit it further to have the two columns examplined above. Here is our final samplesheet:\nsample,fasta\nERX1265396_ERR1192012_T1,preprocessed/assemblebac/assemblies/ERX1265396_ERR1192012_T1_contigs.fa\nERX1265488_ERR1192104_T1,preprocessed/assemblebac/assemblies/ERX1265488_ERR1192104_T1_contigs.fa\nERX1501202_ERR1430824_T1,preprocessed/assemblebac/assemblies/ERX1501202_ERR1430824_T1_contigs.fa\nERX1501203_ERR1430825_T1,preprocessed/assemblebac/assemblies/ERX1501203_ERR1430825_T1_contigs.fa\nERX1501204_ERR1430826_T1,preprocessed/assemblebac/assemblies/ERX1501204_ERR1430826_T1_contigs.fa\nOnce we have the samplesheet ready, we can run the nf-core/funcscan workflow using the following commands:\n# activate the environment\nmamba activate nextflow\n\n# create output directory\nmkdir -p results/funcscan\n\n# run the pipeline\nnextflow run nf-core/funcscan \\\n  -r \"2.1.0\" \\\n  -profile singularity \\\n  --input SAMPLESHEET \\\n  --outdir OUTPUT_DIRECTORY \\\n  --run_arg_screening \\\n  --arg_rgi_db databases/card/ \\\n  --arg_skip_deeparg\nThe options we used are:\n\n-profile singularity - indicates we want to use the Singularity program to manage all the software required by the pipeline (another option is to use docker). See Data & Setup for details about their installation.\n--input - the samplesheet with the input files, as explained above.\n--outdir - the output directory for the results.\n--run_arg_screening - indicates we want to run the “antimicrobial resistance gene screening tools”. There are also options to run antimicrobial peptide and biosynthetic gene cluster screening (see documentation).\n--arg_rgi_db databases/card/ - indicates we want to use a pre-downloaded version of the CARD database. This is optional, and the workflow will download this automatically if you don’t include this option. However, if you run this workflow regularly, it is a good idea to pre-download the database as it saves time and bandwidth.\n--arg_skip_deeparg - this skips a step in the analysis which uses the software DeepARG. We did this simply because this software takes a very long time to run. But in a real analysis you may want to leave this option on.\n\n\n\n\n\n\n\nExerciseExercise 1 - Running funcscan\n\n\n\n\n\n\nYour next task is to run the funcscan pipeline on your data. In the folder scripts (within your analysis directory) you will find a script named 07-run_funcscan.sh. This script contains the code to run funcscan.\n\nEdit this script, adjusting it to fit your input files and the name of your output directory.\nActivate the nextflow software environment.\nRun the script using bash scripts/07-run_funcscan.sh.\n\nWhile the pipeline runs, you will get a progress printed on the screen, and then a message once it finishes.\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nThe fixed script is:\n#!/bin/bash\n\n# create output directory\nmkdir results/funcscan\n\n# run the pipeline\nnextflow run nf-core/funcscan \\\n  -r \"2.1.0\" \\\n  -profile singularity \\\n  --input samplesheet_funcscan.csv \\\n  --outdir results/funcscan \\\n  --run_arg_screening \\\n  --arg_rgi_db databases/card/ \\\n  --arg_skip_deeparg\nWe ran the script as instructed using:\nbash scripts/07-run_funcscan.sh\nWhile it was running it printed a message on the screen:\n* Software dependencies\n  https://github.com/nf-core/funcscan/blob/master/CITATIONS.md\n------------------------------------------------------\nexecutor &gt;  slurm (1371)\n[cf/4d7565] process &gt; NFCORE_FUNCSCAN:FUNCSCAN:INPUT_CHECK:SAMPLESHEET_CHECK (samplesheet_funcscan.csv)   [100%] 1 of 1 ✔\n[-        ] process &gt; NFCORE_FUNCSCAN:FUNCSCAN:GUNZIP_FASTA_PREP                                          -\n[f5/5767a9] process &gt; NFCORE_FUNCSCAN:FUNCSCAN:BIOAWK (ERX1501218_ERR1430840_T1)                          [100%] 50 of 50 ✔\n[a9/164c9c] process &gt; NFCORE_FUNCSCAN:FUNCSCAN:ARG:AMRFINDERPLUS_UPDATE (update)                          [100%] 1 of 1, cached: 1 ✔\n[8f/6d023e] process &gt; NFCORE_FUNCSCAN:FUNCSCAN:ARG:AMRFINDERPLUS_RUN (ERX1501260_ERR1430882_T1)           [100%] 50 of 50 ✔\n[6a/c9f0ce] process &gt; NFCORE_FUNCSCAN:FUNCSCAN:ARG:HAMRONIZATION_AMRFINDERPLUS (ERX1501260_ERR1430882_T1) [100%] 50 of 50 ✔\n[da/67f664] process &gt; NFCORE_FUNCSCAN:FUNCSCAN:ARG:FARGENE (ERX1501226_ERR1430848_T1)                     [100%] 500 of 500 ✔\n[79/73f7b3] process &gt; NFCORE_FUNCSCAN:FUNCSCAN:ARG:HAMRONIZATION_FARGENE (ERX1501226_ERR1430848_T1)       [ 99%] 519 of 520\n[0f/d47de8] process &gt; NFCORE_FUNCSCAN:FUNCSCAN:ARG:RGI_MAIN (ERX1501263_ERR1430885_T1)                    [100%] 50 of 50 ✔\n[21/06df72] process &gt; NFCORE_FUNCSCAN:FUNCSCAN:ARG:HAMRONIZATION_RGI (ERX1501263_ERR1430885_T1)           [100%] 50 of 50 ✔\n[eb/b67eca] process &gt; NFCORE_FUNCSCAN:FUNCSCAN:ARG:ABRICATE_RUN (ERX1501218_ERR1430840_T1)                [100%] 50 of 50 ✔\n[d6/e5fdd4] process &gt; NFCORE_FUNCSCAN:FUNCSCAN:ARG:HAMRONIZATION_ABRICATE (ERX1501218_ERR1430840_T1)      [100%] 50 of 50 ✔\n[-        ] process &gt; NFCORE_FUNCSCAN:FUNCSCAN:ARG:HAMRONIZATION_SUMMARIZE                                -\n[-        ] process &gt; NFCORE_FUNCSCAN:FUNCSCAN:CUSTOM_DUMPSOFTWAREVERSIONS                                -\n[-        ] process &gt; NFCORE_FUNCSCAN:FUNCSCAN:MULTIQC                                                    -\n\n\n\n\n\n\n\n\n\n38.2.1 funcscan results\nAfter running the pipeline, we can look at the output directory in results/funcscan (if your pipeline finished running), or you can also use the preprocessed/funcscan results. There are various directories containing output files:\n\n\n\n\n\n\n\nDirectory\nDescription\n\n\n\n\narg\nContains the results of running the ARG (Antibiotic Resistance Genes) sub-workflow\n\n\nreports\nContains the hamronization_combined_report.tsv file\n\n\nmultiqc\nContains a html file containing summaries of the various outputs\n\n\npipeline_info\nContains information about the pipeline run\n\n\n\n\n\n38.2.2 The hamronization_combined_report.tsv report\nThe main output of interest from this pipeline is the hamronization_combined_report.tsv file, which contains a summary of the results from all the AMR tools used (make sure to use the version in preprocessed/funcscan/reports). You can open this file using any standard spreadsheet software such as Excel (Figure 38.2).\nThis file is quite large, containing many columns and rows. You can find information about the column headers on the nf-core/funscan “Output” documentation page. The easiest way to query this table is to filter the table based on the column “antimicrobial_agent” to remove rows where no AMR gene was detected (Figure 38.2). This way you are left with only the results which were positive for the AMR analysis.\n\n\n\n\n\n\nFigure 38.2: To analyse the table output by hAMRonization in Excel you can go to “Data” –&gt; “Auto-filter”. Then, select the dropdown button on the “antimicrobial_agent” column and untick the box “(Blanks)”. This will only show the genes associated with resistance to antimicrobial drugs.\n\n\n\n\n\n38.2.3 Results from other tools\nYou can also look at the detailed results of each individual tool, which can be found in the directory preprocessed/funcscan/arg. This directory contains sub-directories for each of the 5 AMR tools used (in our case only 4 folders, because we skipped the DeepARG step):\nls preprocessed/funcscan/arg\nabricate  amrfinderplus  fargene  hamronization  rgi\nFor each individual tool’s output folder shown above, there is a report, which is associated with the predicted AMRs for each of our samples. In most cases, the report is in tab-delimited TSV format, which can be opened in a standard spreadsheet software such as Excel. For instance, the AMR report from Abricate for one of our samples looks like this:\nless -S preprocessed/funcscan/arg/abricate/ERX1501203_ERR1430825_T1/ERX1501203_ERR1430825_T1.txt\n#FILE   SEQUENCE    START   END STRAND  GENE    COVERAGE    COVERAGE_MAP    GAPS    %COVERAGE   %IDENTITY   DATABASE    ACCESSION   PRODUCT RESISTANCE\nERX1501203_ERR1430825_T1_contigs.fa contig00008 81930   83849   +   tet(M)  1-1920/1920 =============== 0/0 100.00  100.00  ncbi    NG_048235.1 tetracycline resistance ribosomal protection protein Tet(M) TETRACYCLINE\n\nFor this sample there was just one putative AMR gene detected by Abricate, associated with tetracycline resistance. These genes were identified based on their similarity with annotated sequences from the NCBI database. For example, the gene Tet(M) was detected in our sample, matching the NCBI accession NG_048235.1. This is annotated as as a reference for antimicrobial resistance, in this case to the drug “TETRACYCLINE”.\n\n\n\n\n\n\nNoteCommand line trick \n\n\n\nHere is a trick using standard commands to count how many times each drug was identified by funcscan:\ncat preprocessed/funcscan/reports/hamronization_summarize/hamronization_combined_report.tsv | cut -f 10 | sort | uniq -c\n\ncat prints the content of the file\ncut extracts the 10th column from the file\nsort and uniq -c are used in combination to count unique output values\n\nThe result of the above command is:\n1 antimicrobial_agent\n12 TETRACYCLINE",
    "crumbs": [
      "Slides",
      "Antimicrobial Resistance",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Command-line AMR prediction</span>"
    ]
  },
  {
    "objectID": "materials/34-command_line_amr.html#summary",
    "href": "materials/34-command_line_amr.html#summary",
    "title": "38  Command-line AMR prediction",
    "section": "38.3 Summary",
    "text": "38.3 Summary\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nThe nf-core/funcscan workflow performs AMR analysis using several software tools. It requires as input a samplesheet with sample names and their respective FASTA files.\nThe results from the several AMR tools are summarised in a single report, which can be conveniently used to filter for putative resistance to antimicrobial agents.",
    "crumbs": [
      "Slides",
      "Antimicrobial Resistance",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Command-line AMR prediction</span>"
    ]
  },
  {
    "objectID": "materials/35-funcscan_pathogenwatch.html",
    "href": "materials/35-funcscan_pathogenwatch.html",
    "title": "39  funcscan versus Pathogenwatch",
    "section": "",
    "text": "39.1 Which AMR do my isolates have?\nAt this stage you may notice that different tools will give you a different answer to this question and it is therefore recommended to compare the results across multiple tools. For example, Pathogenwatch generally detects AMR for comparatively more antimicrobial drugs (ten in this case) compared to the funcscan analysis. When we filtered the hamronization_combined_report.tsv table we found that funcscan had only identified resistance to Tetracycline (table below, showing some of the columns from the hAMRonization table):\nThe main reason for funcscan only identifying resistance to Tetracycline whilst Pathogenwatch identified resistance to for up to ten drugs is the resistance-determinant databases used for the predictions. For species such as S. pneumoniae, Pathogenwatch uses a curated database specific to the species whilst funcscan uses databases such as amrfinderplus which contain variants for all species and may not contain the variants in the Pathogenwatch database.\nIn conclusion, always be critical of the analysis of your results at this stage, comparing the output from different tools as well as considering the quality of your assemblies. Ultimately, the safest way to assess AMR is with experimental validation, by testing those strains against the relevant antimicrobial agents in the lab. However, computational analysis such as what we did can help inform these experiments and treatment decisions.",
    "crumbs": [
      "Slides",
      "Antimicrobial Resistance",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>funcscan versus Pathogenwatch</span>"
    ]
  },
  {
    "objectID": "materials/35-funcscan_pathogenwatch.html#which-amr-do-my-isolates-have",
    "href": "materials/35-funcscan_pathogenwatch.html#which-amr-do-my-isolates-have",
    "title": "39  funcscan versus Pathogenwatch",
    "section": "",
    "text": "input_file_name gene_symbol reference_accession antimicrobial_agent coverage_percentage sequence_identity\nERX1501203_ERR1430825_T1.tsv.amrfinderplus  tet(M)  WP_002414694.1  TETRACYCLINE    100 100\nERX1501204_ERR1430826_T1.tsv.amrfinderplus  tet(M)  WP_002414694.1  TETRACYCLINE    100 100\nERX1501217_ERR1430839_T1.tsv.amrfinderplus  tet(M)  WP_002414694.1  TETRACYCLINE    100 99.84\nERX1501229_ERR1430851_T1.tsv.amrfinderplus  tet(M)  WP_002414694.1  TETRACYCLINE    100 99.84\nERX1501230_ERR1430852_T1.tsv.amrfinderplus  tet(M)  WP_002414694.1  TETRACYCLINE    100 100\nERX1501238_ERR1430860_T1.tsv.amrfinderplus  tet(M)  WP_002414694.1  TETRACYCLINE    100 100\nERX1501242_ERR1430864_T1.tsv.amrfinderplus  tet(M)  WP_000691741.1  TETRACYCLINE    100 100\nERX1501243_ERR1430865_T1.tsv.amrfinderplus  tet(M)  WP_002414694.1  TETRACYCLINE    100 100\nERX1501248_ERR1430870_T1.tsv.amrfinderplus  tet(M)  WP_002414694.1  TETRACYCLINE    100 100\nERX1501250_ERR1430872_T1.tsv.amrfinderplus  tet(M)  WP_002414694.1  TETRACYCLINE    100 100\nERX1501252_ERR1430874_T1.tsv.amrfinderplus  tet(M)  WP_002414694.1  TETRACYCLINE    100 99.84\nERX1501254_ERR1430876_T1.tsv.amrfinderplus  tet(M)  WP_002414694.1  TETRACYCLINE    100 100          \n\n\n\n\n\n\n\n\nExerciseExercise 1 - AMR with Pathogenwatch\n\n\n\n\n\n\nFollowing from the Pathogenwatch exercise in Analysing Pneumococcal genomes with Pathogenwatch, open the “Chaguza Serotype 1” collection that you created and answer the following questions:\n\nOpen the antibiotics summary table.\nDo all your samples have evidence for antibiotic resistance?\nIf any samples have resistance to much fewer antibiotics compared to the others, do you think this could be related to assembly quality?\nHow do the results from Pathogenwatch compare to those from nf-core/funcscan?\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nWe can open the “Antibiotics” table from the top-left dropdown menu, as shown in the image below.\n\nWe can see that Pathogenwatch identified resistance to several antibiotics. We can see that there are mainly two distinct AMR-profiles in our samples.The first group is resistant to Tetracycline, Trimethoprim, Sulfamethoxazole and Co-Trimoxazole and susceptible to the other drugs. The second group is resistant to Fluroquinolones, Sulfamethoxazole and only intermediate resistant to Co-Trimoxazole. Additionally sample ERX1501242_ERR1430864 is only resistant to Tetracycline and sample ERX1501229_ERR1430851 is resistant to Tetracycline, Sulfamethoxazole intermediate resistant to Co-Trimoxazole.\nIf we look at the funcscan results we can see that it identified Tetracycline resistance in the same 12 samples as Pathogenwatch.",
    "crumbs": [
      "Slides",
      "Antimicrobial Resistance",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>funcscan versus Pathogenwatch</span>"
    ]
  },
  {
    "objectID": "materials/35-funcscan_pathogenwatch.html#summary",
    "href": "materials/35-funcscan_pathogenwatch.html#summary",
    "title": "39  funcscan versus Pathogenwatch",
    "section": "39.2 Summary",
    "text": "39.2 Summary\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nWhile there is overlap in the AMR results of different software, these results can be sometimes widely different.\nBesides differences in the algorithms used, one key difference is the databases used to infer the presence of antimicrobial resistance.\n\nTools designed to be more widely applicable, such as those used by nf-core/funcscan, may have less power to detect AMR.\nTools such as Pathogenwatch, which uses curated databases for specific species, may identify a higher number of AMR genes.\n\nThe best way to validate bioinformatic AMR results is with experimental validation.",
    "crumbs": [
      "Slides",
      "Antimicrobial Resistance",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>funcscan versus Pathogenwatch</span>"
    ]
  },
  {
    "objectID": "materials/36-working_ont.html",
    "href": "materials/36-working_ont.html",
    "title": "40  Working with ONT data",
    "section": "",
    "text": "40.1 Introduction to Oxford Nanopore Technologies (ONT)\nOxford Nanopore Technologies (ONT) offers a compelling alternative to Illumina’s dominant short-read sequencing platforms, particularly for applications where long-read sequencing provides a critical advantage. ONT’s sequencing technology is based on nanopore sensing, which measures changes in electrical current as DNA or RNA strands pass through protein nanopores, enabling real-time, ultra-long reads (often exceeding 100 kb). This contrasts with Illumina’s short-read (50–300 bp) sequencing-by-synthesis approach. Key advantages of ONT include its portability (with devices ranging from pocket-sized MinIONs to high-throughput PromethION systems), lower upfront costs, and the ability to directly detect base modifications (e.g., methylation) without additional processing. However, ONT has historically had higher error rates (~5–15%) compared to Illumina’s ultra-high accuracy (~99.9%), though improvements in chemistry and basecalling algorithms have narrowed this gap. Additionally, ONT’s real-time data streaming allows for adaptive sequencing, where reads of interest can be selectively targeted during a run.\nLong-read sequencing excels in applications where resolving complex genomic regions is essential, such as de novo genome assembly, structural variant detection, and resolving repetitive or highly homologous sequences (e.g., telomeres, centromeres, and transposable elements). It is also invaluable for full-length transcriptome sequencing (isoform detection) and metagenomic analyses, where short reads often fail to distinguish closely related species or genes. While Illumina remains the gold standard for high-accuracy, high-throughput applications (e.g., SNP calling, targeted sequencing, and large-scale population studies), ONT’s long reads provide a unique advantage in clinical diagnostics (e.g., identifying large pathogenic deletions or fusion genes) and field-based sequencing (e.g., outbreak surveillance in remote areas). The choice between ONT and Illumina ultimately depends on the trade-offs between read length, accuracy, cost, and the specific biological question at hand.",
    "crumbs": [
      "Slides",
      "Plasmid analysis",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Working with ONT data</span>"
    ]
  },
  {
    "objectID": "materials/36-working_ont.html#introduction-to-oxford-nanopore-technologies-ont",
    "href": "materials/36-working_ont.html#introduction-to-oxford-nanopore-technologies-ont",
    "title": "40  Working with ONT data",
    "section": "",
    "text": "Overview of Nanopore sequencing showing the highly-portable MinION device. The device contains thousands of nanopores embedded in a membrane where current is applied. As individual DNA molecules pass through these nanopores they cause changes in this current, which is detected by sensors and read by a dedicated computer program. Each DNA base causes different changes in the current, allowing the software to convert this signal into base calls.\n\n\n\n\n40.1.1 Alternative pipelines for ONT data\nThere are alternative pipelines for analysing ONT data which are designed to handle the unique characteristics of ONT data, including its long reads and higher error rates. These include the following options that can be used instead of the pipelines we have used in this course:\n\nbacQC-ONT: This pipeline is designed for quality control and taxonomic classification of ONT reads. Many of the tools are the same as those used in the bacQC pipeline, but it uses the ONT-specific QC tools nanoplot and pycoQC.\nassembleBAC-ONT: This pipeline is designed for de novo assembly and annotation of bacterial genomes from ONT reads.\nwf-bacterial-genomes: This pipeline is for mapping and small variant calling of haploid samples. It can be used instead of bactmap for ONT data.",
    "crumbs": [
      "Slides",
      "Plasmid analysis",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Working with ONT data</span>"
    ]
  },
  {
    "objectID": "materials/36-working_ont.html#summary",
    "href": "materials/36-working_ont.html#summary",
    "title": "40  Working with ONT data",
    "section": "40.2 Summary",
    "text": "40.2 Summary\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nOxford Nanopore Technologies (ONT) offers long-read sequencing capabilities, which are particularly useful for applications requiring resolution of complex genomic regions, such as de novo genome assembly and structural variant detection.\nONT’s sequencing technology is based on nanopore sensing, enabling real-time, ultra-long reads, which contrasts with Illumina’s short-read sequencing.\nONT has advantages such as portability, lower costs, and the ability to directly detect base modifications, but it has historically had higher error rates compared to Illumina.\nThere are alternative pipelines for ONT data, such as bacQC-ONT, assembleBAC-ONT, and wf-bacterial-genomes, which are designed to handle the unique characteristics of ONT data.",
    "crumbs": [
      "Slides",
      "Plasmid analysis",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Working with ONT data</span>"
    ]
  },
  {
    "objectID": "materials/37-plasmids.html",
    "href": "materials/37-plasmids.html",
    "title": "41  Identifying Plasmids",
    "section": "",
    "text": "41.1 Plasmids\nPlasmids are small, circular, extrachromosomal DNA molecules that play a crucial role in the horizontal gene transfer (HGT) of antimicrobial resistance (AMR) determinants among bacteria. Unlike chromosomal genes, plasmids can autonomously replicate and transfer between bacterial cells via conjugation, transformation, or transduction, enabling the rapid spread of resistance genes across different species and even genera. Many plasmids carry mobile genetic elements (MGEs), such as transposons and integrons, which further facilitate the acquisition and dissemination of AMR genes. This mobility allows bacteria to quickly adapt to antibiotic pressure, contributing to the global AMR crisis. Clinically relevant resistance genes, including those encoding extended-spectrum β-lactamases (ESBLs), carbapenemases (e.g., NDM, KPC), and plasmid-mediated quinolone resistance (PMQR), are frequently plasmid-borne. Because plasmids can persist in bacterial populations even in the absence of antibiotic selection, they serve as long-term reservoirs for resistance, complicating infection control and treatment strategies. Understanding plasmid epidemiology is therefore essential for tracking AMR spread and developing targeted interventions.",
    "crumbs": [
      "Slides",
      "Plasmid analysis",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Identifying Plasmids</span>"
    ]
  },
  {
    "objectID": "materials/37-plasmids.html#plasmid-identification",
    "href": "materials/37-plasmids.html#plasmid-identification",
    "title": "41  Identifying Plasmids",
    "section": "41.2 Plasmid Identification",
    "text": "41.2 Plasmid Identification\nPlasmid identification is a critical step in understanding the role of plasmids in the spread of antimicrobial resistance (AMR) and other traits among bacteria. The most commonly used tools for plasmid identification in whole-genome sequencing (WGS) data include PlasmidFinder and mlplasmids, which detect plasmid-derived sequences using curated databases of known replicons. Other popular tools like MOB-suite and Platon employ machine learning and homology-based approaches to predict plasmid contigs and reconstruct plasmid structures from assembled genomes.\n\n41.2.1 MOB-suite\nThe MOB-suite is designed to be a modular set of tools for the typing and reconstruction of plasmid sequences from WGS assemblies. It is particularly useful for identifying plasmid contigs in assembled genomes, reconstructing plasmid sequences, and predicting their potential mobility.\n\n\n41.2.2 Running MOB-suite\nWe are going to use E.coli assemblies we’ve provided for you as input for MOB-suite and these are located in E_coli/data/assemblies. These assemblies were generated from ONT data using the assembleBAC-ONT pipeline.\nFirst activate the MOB-Suite software environment:\nmamba activate mob_suite\nTo run MOB-suite on a single assembly, the following command can be used:\n# create output directory\nmkdir -p results/mobsuite/\n\n# run MOB-suite\nmob_recon --infile data/assemblies/SRX23625789.fa --outdir results/mobsuite/SRX23625789 -g databases/2019-11-NCBI-Enterobacteriacea-Chromosomes.fasta\nThe options we used are:\n\n--infile - the assembly to search for plasmids.\n--outdir - output directory for MOB-suite to save its outputs.\n-g - the path to the reference database of known plasmid sequences. This is a required parameter for MOB-suite to identify plasmids in the input assembly.\n\nAs it runs, mob_recon prints several messages to the screen.\nWe can see all the output files mob_recon generated:\nls results/mobsuite/SRX23625789\nbiomarkers.blast.txt  mge.report.txt        chromosome.fasta     plasmid_AA379.fasta  plasmid_AA619.fasta\ncontig_report.txt     mobtyper_results.txt  plasmid_AA170.fasta  plasmid_AA474.fasta  plasmid_AD548.fasta       \n\n\n\n\n\n\nExerciseExercise 1 - Running MOB-suite\n\n\n\n\n\n\nMake sure you are in the E_coli directory for this exercise.\nAbove, we have run MOB-suite on a single sample. However, we have ten samples that we need to repeat the analysis on. To do this, we’ve provided a script that runs MOB-suite on all the FASTA files for all the samples in the data/assemblies directory using a for loop.\n\nIn the folder scripts (inside your analysis directory), you’ll find a script named 01-run_mobsuite.sh.\nOpen the script, which you will notice is composed of two sections:\n\n#### Settings #### where we define some variables for input and output files names. If you were running this script on your own data, you may want to edit the directories in this section.\n#### Analysis #### this is where MOB-suite is run on each sample as detailed in Section 41.2.2. You should not change the code in this section.\n\nActivate the software environment: mamba activate mob_suite\nRun the script with bash scripts/01-run_mobsuite.sh. If the script is running successfully it should print a message on the screen as the samples are processed.\nHow many different plasmids were identified in sample SRX23625854?\nWhich contig(s) were identified as chromosome for sample SRX23625854?\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nWe opened the script 01-run_mobsuite.sh and these are the settings we used:\n\nfasta_dir=\"data/assemblies\" - the name of the directory with FASTA files in it.\noutdir=\"results/mobsuite\" - the name of the directory where we want to save our results.\ndatabase=\"databases/mob_suite/2019-11-NCBI-Enterobacteriacea-Chromosomes.fasta\" - the name of the directory with the plasmid database in it.\n\nWe then ran the script using bash scripts/01-run_mobsuite.sh. The script prints a message while it’s running:\nProcessing SRX23625789\n2025-06-26 12:37:29,646 mob_suite.mob_recon INFO: MOB-recon version 3.1.9  [in /rds/user/ajv37/hpc-work/micromamba/envs/mob_suite/lib/python3.11/site-packages/mob_suite/mob_recon.py:984]\n2025-06-26 12:37:29,729 mob_suite.mob_recon INFO: SUCCESS: Found program blastn at /rds/user/ajv37/hpc-work/micromamba/envs/mob_suite/bin/blastn [in /rds/user/ajv37/hpc-work/micromamba/envs/mob_suite/lib/python3.11/site-packages/mob_suite/utils.py:597]\n2025-06-26 12:37:29,765 mob_suite.mob_recon INFO: SUCCESS: Found program makeblastdb at /rds/user/ajv37/hpc-work/micromamba/envs/mob_suite/bin/makeblastdb [in /rds/user/ajv37/hpc-work/micromamba/envs/mob_suite/lib/python3.11/site-packages/mob_suite/utils.py:597]\n2025-06-26 12:37:29,803 mob_suite.mob_recon INFO: SUCCESS: Found program tblastn at /rds/user/ajv37/hpc-work/micromamba/envs/mob_suite/bin/tblastn [in /rds/user/ajv37/hpc-work/micromamba/envs/mob_suite/lib/python3.11/site-packages/mob_suite/utils.py:597]\n2025-06-26 12:37:29,804 mob_suite.mob_recon INFO: Processing fasta file data/assemblies//SRX23625789.fa [in /rds/user/ajv37/hpc-work/micromamba/envs/mob_suite/lib/python3.11/site-packages/mob_suite/mob_recon.py:1011]\n2025-06-26 12:37:29,804 mob_suite.mob_recon INFO: Analysis directory results/mobsuite/SRX23625789 [in /rds/user/ajv37/hpc-work/micromamba/envs/mob_suite/lib/python3.11/site-packages/mob_suite/mob_recon.py:1012]\n2025-06-26 12:37:31,995 mob_suite.mob_recon INFO: Writing cleaned header input fasta file from data/assemblies//SRX23625789.fa to results/mobsuite/SRX23625789/__tmp/fixed.input.fasta [in /rds/user/ajv37/hpc-work/micromamba/envs/mob_suite/lib/python3.11/site-packages/mob_suite/mob_recon.py:1107]\n...\nThere are a few different output files generated by MOB-suite for each sample. The most useful one to help answer the questions is contig_report.txt. We can use less to look at the number of plasmids identified in the sample and which contig(s) were identified as the chromosome.\nless -S results/mobsuite/SRX23625854/contig_report.txt\nsample_id       molecule_type   primary_cluster_id      secondary_cluster_id    contig_id       size    gc      md5     circularity_status      rep_type(s)     rep_t&gt;\nSRX23625854     chromosome      -       -       contig_1        4835838 0.5077347504196791      76eea322d7fb7ab52954ff7c3e51074c        not tested      -       -    &gt;\nSRX23625854     plasmid AA579   AI802   contig_2        2493    0.5194544725230645      55ebe5a8c69762065dc86ecdc0b026ab        not tested      -       -       -    &gt;\nSRX23625854     plasmid AA579   AI802   contig_3        1131    0.5004420866489832      9ad46c349e5d7f1e38a7d893e35f546b        not tested      -       -       -    &gt;\nSRX23625854     chromosome      -       -       contig_4        733     0.5061391541609823      67e9da01dbdfa64d30f64bd093426d7d        not tested      -       -    &gt;\nSRX23625854     plasmid AA170   AH818   contig_5        120329  0.5314512710984052      2182898861b4a6f8bb457cd120c69577        not tested      IncFIA,IncFIA   00013&gt;\nSRX23625854     plasmid AA474   AI621   contig_6        94110   0.5004781638508129      ff6338308f3bdf00df017328dafacd98        not tested      IncI1/B/O       EU418&gt;\nSRX23625854     chromosome      -       -       contig_7        8214    0.4398587776966155      0c46e1e37a8dc1ac259f845a1346a585        not tested      ColRNAI_rep_c&gt;\nWe can see that MOB-suite identified 3 plasmids (AA579, AA170, AA474) in this sample, and the contigs identified as the chromosome were contig_1, contig_4, and contig_7.",
    "crumbs": [
      "Slides",
      "Plasmid analysis",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Identifying Plasmids</span>"
    ]
  },
  {
    "objectID": "materials/37-plasmids.html#plasmid-clustering",
    "href": "materials/37-plasmids.html#plasmid-clustering",
    "title": "41  Identifying Plasmids",
    "section": "41.3 Plasmid clustering",
    "text": "41.3 Plasmid clustering\n\n41.3.1 Pling\nPling is a software workflow for plasmid analysis using rearrangement distances, specifically the Double Cut and Join Indel (DCJ-Indel) distance. By intelligently combining containment distance (shared content as fraction of the smaller) and DCJ-indel distance (“how far apart evolutionarily” in a structural sense), and by preventing shared mobile elements from clouding the issue, it infers clusters of related plasmids.\n\n\n41.3.2 Running Pling\nTo run Pling, we need to provide it with the plasmid sequences that were identified by MOB-suite. These need to be copied from the results/mobsuite directory to a new directory called results/pling. Now we can run Pling on the plasmid sequences we identified with MOB-suite:\n# activate the pling software environment\nmamba activate pling\n\n# create pling output directory\nmkdir -p results/pling/\n\n# copy plasmid sequences to pling directory\ncp results/mobsuite/*/*_plasmid_*.fasta results/pling/\n\n# create the input file for pling\nls -d -1 results/pling/*.fasta &gt; input.txt\n\n# run pling\npling input.txt results/pling/output align\nThe options we used are:\n\ninput.txt - the plasmid FASTA files to cluster.\nresults/pling/output - output directory for pling to save its outputs.\nalign - integerisation method: “align” for alignment.\n\nAs it runs, pling prints several messages to the screen.\nWe can see all the output files pling generated:\nls results/pling/\nall_plasmids_distances.tsv  batches  containment  dcj_thresh_4_graph  unimogs       \n\n\n\n\n\n\nExerciseExercise 2 - Running Pling\n\n\n\n\n\n\nYour next task is to run Pling on your data.\nIn the folder scripts (within your analysis directory) you will find a script named 02-run_pling.sh. This script contains the code to run Pling.\n\nEdit this script, adjusting it to fit your input files and the name of your output directory.\nActivate the pling software environment.\nRun the script using bash scripts/02-run_pling.sh.\n\nWhile the pipeline runs, you will get a progress printed on the screen, and then a message once it finishes.\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nThe fixed script is:\n#!/bin/bash\n\n# create output directory\nmkdir -p results/pling/\n\n# copy plasmid sequences to Pling directory\ncp results/mobsuite/*/*_plasmid_*.fasta results/pling/\n\n# create the input file for Pling\nls -d -1 results/pling/*.fasta &gt; input.txt\n\n# run Pling\npling input.txt results/pling/output align\nWe ran the script as instructed using:\nbash scripts/02-run_pling.sh\nWhile it was running it printed a message on the screen:\nBatching...\n\nCompleted batching.\n\nAligning, integerising, and building containment network...\n\nCompleted distance calculations and clustering.\n\n\n\n\n\n\n\n\n\n\n41.3.3 Pling results\nNow that Pling has run we can look at the results. The file we’ll have a look at is index.html: go to the File Explorer application , navigate to results/pling/output/dcj_thresh_4_graph/visualisations/communities/ and double click on index.html.\nThis will open the file in your web browser:\n\nYou can click on any of the communities on the list to be taken to a visualisation of that community’s containment network. Click on the first link on this page (View community_0 (14 nodes, 59 edges)). Pling defines broad plasmid communities by building a containment network. Each node is a plasmid, and its colour denotes which subcommunity it’s assigned to. There are edges between every pair of plasmids that have a containment distance less than or equal to 0.5, and the edges are labelled by both containment distance (first number) and DCJ-Indel distance (second number). The layout may be a bit different, as it is regenerated each time you view the community.\n\n\n\n\n\n\n\nExerciseExercise 3 - Examine Pling output\n\n\n\n\n\n\nOpen the index.html file in the containment directory of your Pling results and try to answer the following questions:\n\nHow many plasmid communities were identified?\nHow many plasmids are in the largest community?\n\nWe can also add the plasmid clustering results to a phylogenetic tree of our samples in R using the ggtree package. This will allow us to identify which plasmids are “the same” and spot horizontal gene transfer (HGT) events.\n\nGenerate a ‘quick and dirty’ phylogenetic tree of our samples using mashtree. Run the script 03-run_mashtree.sh in the scripts directory. This will generate a tree in the results/mashtree directory.\nOpen the script 04-plot_pling.R in the scripts directory in RStudio.\nRun the script line-by-line to generate the annotated phylogenetic tree.\nDo you see any HGT events in the tree?\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nWe opened the index.html file in the communities directory of our Pling results and found that:\n\nHow many plasmid communities were identified?: 14 communities were identified.\nHow many plasmids are in the largest community?: The largest community (community_0) has 14 plasmids.\n\nWe built a phylogenetic tree using mashtree. We then opened the script 04-plot_pling.R in RStudio and ran it line-by-line. The script generated a phylogenetic tree of our samples and added metadata strips for plasmid “type” and community to spot HGT events. We can see that samples SRX23626042 and SRX23625928 have versions of the AA170 plasmid from the same community, indicating potential horizontal gene transfer (HGT) events.",
    "crumbs": [
      "Slides",
      "Plasmid analysis",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Identifying Plasmids</span>"
    ]
  },
  {
    "objectID": "materials/37-plasmids.html#summary",
    "href": "materials/37-plasmids.html#summary",
    "title": "41  Identifying Plasmids",
    "section": "41.4 Summary",
    "text": "41.4 Summary\n\n\n\n\n\n\nTipKey Points\n\n\n\n\nPlasmids are small, circular DNA molecules that can carry antimicrobial resistance (AMR) genes and facilitate horizontal gene transfer (HGT) among bacteria.\nMOB-suite is a tool for identifying plasmid contigs in whole-genome sequencing (WGS) assemblies, reconstructing plasmid sequences, and predicting their potential mobility.\nPling is a software workflow for plasmid clustering using rearrangement distances, specifically the Double Cut and Join Indel (DCJ-Indel) distance, to infer clusters of related plasmids.",
    "crumbs": [
      "Slides",
      "Plasmid analysis",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Identifying Plasmids</span>"
    ]
  },
  {
    "objectID": "materials/38-outbreak.html",
    "href": "materials/38-outbreak.html",
    "title": "OUTBREAK ALERT!",
    "section": "",
    "text": "Analyse the data\nWe’ve been contacted by the World Health Organization who are dealing with an outbreak of Acute Watery Diarrhoea in an undisclosed country. They need some urgent bioinformatics analysis doing on isolates collected as part of the outbreak and they’ve provided the raw data for us to analyse.\nThe data for this exercise can be found in the outbreak directory. There are two versions of the exercise, using Illumina or ONT data (your choice).",
    "crumbs": [
      "Slides",
      "Outbreak!",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>OUTBREAK ALERT!</span>"
    ]
  },
  {
    "objectID": "materials/38-outbreak.html#analyse-the-data",
    "href": "materials/38-outbreak.html#analyse-the-data",
    "title": "OUTBREAK ALERT!",
    "section": "Outbreak alert - trainer solutions",
    "text": "Perform QC on the sequence data to identify the causative agent and check that the data is good enough for further analysis.\nHave a look at the “Know Your Bug” flowchart and decide what the best analysis approach is.\nUse what you have learned this week to analyse the isolates so as to be able to provide some useful information to the WHO so they can deal with the outbreak effectively.\nHave a think about what key information the WHO will want to know.\nFeel free to consult your colleagues if you need some help.\n\n\n\n\n\n\n\nTipLong-running pipelines\n\n\n\nOne of the pipelines that we expect you to run takes a long time to complete. You do not need to wait for it to complete - once you are happy that your pipeline is running successfully, rather than wait, ask the trainers where to find preprocessed data to continue with your analysis.\n\n\n\n\n\n\nOutbreak alert - trainer solutions\nBelow we have quick solutions for the outbreak exercise. These are guidelines, the participants are welcome to explore other approaches.\nSome general notes:\n\nPreprocessed data: Some of these pipeline steps take quite a while to run (especially de-novo assembly). To avoid having the participants waiting for too long, there is a hidden folder with preprocessed results in outbreak/illumina/.preprocessed and outbreak/ont/.preprocessed. Participants can be told about this folder once their first pipeline is running successfully, so they can advance with the rest of the exercise.\nShell scripts: Encourage participants to write their commands in a shell script for reproducibility and documentation.\n\n\nIlluminaONT\n\n\n\nQC\nRun avantonder/bacQC (link to section).\nStart by creating the samplesheet using the helper script:\npython scripts/fastq_dir_to_samplesheet.py data/reads/ --read1_extension \"_1.fastq.gz\" --read2_extension \"_2.fastq.gz\" samplesheet.csv\nThen run bacQC, with command below. Participants will need to “guess” a genome size, as they won’t know what it is. The genome size is not critical for the analysis.\nnextflow run avantonder/bacQC \\\n  -r \"1.12.0\" \\\n  -resume -profile \"singularity\" \\\n  --input \"samplesheet.csv\" \\\n  --outdir \"results/bacqc\" \\\n  --kraken2db \"databases/k2_standard_08gb_20240605\" \\\n  --kronadb databases/krona/taxonomy.tab \\\n  --genome_size \"3200000\"\nFrom the QC participants should notice:\n\nIsolate02 has very few reads, which might lead to poor assemblies - they can choose whether to proceed with it or not\nThe Kraken barplot reveals that we are dealing with a Vibrio cholerae outbreak.\nHowever, isolate08 is Vibrio parahaemolyticus. The GC content was also slightly different for this isolate.\n\nagain, they can choose to proceed with this sample or not - however proceeding is nice to use as an outgroup for phylogeny later on.\n\n\n\n\nAssembly\nRun avantonder/assembleBAC (link to section):\nnextflow run avantonder/assembleBAC \\\n  -r \"v2.0.2\" \\\n  -resume -profile \"singularity\" \\\n  --input \"samplesheet.csv\" \\\n  --outdir \"results/assemblebac\" \\\n  --baktadb \"databases/bakta_light_20240119\" \\\n  --genome_size \"3.2M\" \\\n  --checkm2db \"databases/checkm2_v2_20210323/uniref100.KO.1.dmnd\"\n\n\n\n\n\n\nWarningPreprocessed data\n\n\n\nassembleBAC takes a long time to run (up to 1h). Make sure to point the participants to the .preprocessed hidden folder.\n\n\nFrom the assemblies participants should notice:\n\nMultiQC report:\n\nIf they proceeded with isolate02, they will notice very poor assembly, essentially unusable.\nOther samples are all comparable in quality, some more fragmented than others, but not massively different\n\nThe checkm2_report.tsv (in the report output folder):\n\nVery high assembly completeness for all samples (except isolate02).\nIsolate08 is again a bit different: slightly lower GC content (matches what was seen with bacQC) and higher predicted genome size at ~5Mb and higher number of predicted genes.\nAs further QC they can compare these numbers with the ones on reference databases, for example on KEGG: V. parahaemolyticus; V. cholerae\n\n\n\n\nCore genome alignemnt\nCreate a script to run Panaroo (link to section). This takes a while to run, but we provide preprocessed files if they want to proceed from there once their command is working and running.\nmamba activate panaroo\n\n# create output directory\nmkdir results/panaroo\n\n# ensure isolate02 is not being used - they might do this in a more \"manual\" way\ngffs=$(ls results/assemblebac/bakta/*.gff3 | grep -v \"isolate02\")\n\n# run panaroo\npanaroo \\\n  --input $gffs \\\n  --out_dir results/panaroo \\\n  --clean-mode strict \\\n  --alignment core \\\n  --core_threshold 0.98 \\\n  --remove-invalid-genes \\\n  --threads 8\n\n\n\n\n\n\nTipDiscussing use of an outgroup\n\n\n\nWe’re including V. parahaemolyticus to use as an outgroup in our phylogeny. However, it’s worth noting that by doing so we may be reducing the number of core genes in the alignment, as these species may be sufficiently diverged to alter the Panaroo clustering. One possibility would be to lower the --core_threshold to include a few more genes.\nThere is no clear “right” answer, but this could be a good discussion to have with the participants.\n\n\n\n\n\n\nQC\nRun avantonder/bacQC-ONT (takes ~20 minutes).\nStart by creating the samplesheet:\nsample,fastq\nsample1,data/fastq_pass/barcode01/ERR10146532.fastq.gz\nsample2,data/fastq_pass/barcode06/ERR10146521.fastq.gz\nsample3,data/fastq_pass/barcode02/ERR10146551.fastq.gz\nsample4,data/fastq_pass/barcode09/ERR10146531.fastq.gz\nsample5,data/fastq_pass/barcode05/ERR10146520.fastq.gz\nThen run bacQC-ONT, with the command below. Participants will need to “guess” a genome size, as they won’t know what it is. The genome size is not critical for the analysis.\nnextflow run avantonder/bacQC-ONT \\\n  -r \"v2.0.2\" \\\n  -profile singularity \\\n  --input samplesheet.csv \\\n  --summary_file sequencing_summary.txt \\\n  --genome_size 4000000 \\\n  --kraken2db databases/k2_standard_08gb_20240605/ \\\n  --kronadb databases/krona/taxonomy.tab \\\n  --outdir results/bacqc-ont\n\n\nAssembly\nRun avantonder/assembleBAC-ONT (takes ~2h - point participants to hidden .preprocessed folder):\nnextflow run avantonder/assembleBAC-ONT \\\n  -r \"v2.0.3\" \\\n  -profile singularity \\\n  --input samplesheet.csv \\\n  --genome_size 4M \\\n  --medaka_model r941_min_fast_g507 \\\n  --outdir results/assemblebac \\\n  --baktadb databases/bakta_light_20240119/ \\\n  --checkm2db databases/checkm2_v2_20210323/uniref100.KO.1.dmnd\n\n\n\n\n\n\nWarningPreprocessed data\n\n\n\nassembleBAC-ONT takes a long time to run (up to 2h). Make sure to point the participants to the .preprocessed hidden folder.\n\n\n\n\nCore genome alignemnt\nCreate a script to run Panaroo (link to section). This takes a while to run, but we provide preprocessed files if they want to proceed from there once their command is working and running.\nmamba activate panaroo\n\n# create output directory\nmkdir results/panaroo\n\n# run panaroo\npanaroo \\\n  --input results/assemblebac/bakta/*.gff3 \\\n  --out_dir results/panaroo \\\n  --clean-mode strict \\\n  --alignment core \\\n  --core_threshold 0.98 \\\n  --remove-invalid-genes \\\n  --threads 8\n\n\n\n\n\n\nPhylogeny\nOnce participants have the core alignment, they can build their trees by writing a script to run IQ-tree (link to section).\n# create output directory\nmkdir -p results/snp-sites/\nmkdir -p results/iqtree/\n\n# extract variable sites\nsnp-sites results/panaroo/core_gene_alignment_filtered.aln &gt; results/snp-sites/core_gene_alignment_snps.aln\n\n# count invariant sites\nsnp-sites -C results/panaroo/core_gene_alignment_filtered.aln &gt; results/snp-sites/constant_sites.txt\n\n# Run iqtree\niqtree \\\n  -fconst $(cat results/snp-sites/constant_sites.txt) \\\n  -s results/snp-sites/core_gene_alignment_snps.aln \\\n  --prefix results/iqtree/vibrio_outbreak \\\n  -nt AUTO \\\n  -ntmax 8 \\\n  -mem 8G \\\n  -m MFP \\\n  -bb 1000\nParticipants can visualise the phylogeny in Microreact, FigTree or R/ggtree - their choice. Encourage participants to include metadata from other analyses (e.g. MLST and AMR).\n\n\nMLST\nAlthough assembleBAC runs MLST, participants may opt to run this separately as well (link to section).\nThey can use the mlst --list command to see which schemes are available for Vibrio. There are actually two schemes available, named vcholerae and vcholerae_2. They can run both:\nmamba activate mlst\n\nmkdir results/mlst\n\n# exclude isolate02 and 08\nsamples=$(ls results/assemblebac/assemblies/*.fa | grep -v \"isolate02\\|isolate08\")\n\nmlst --scheme vcholerae $samples &gt; results/mlst/mlst_typing_scheme1.tsv\nmlst --scheme vcholerae_2 $samples &gt; results/mlst/mlst_typing_scheme2.tsv\nParticipants should notice that most isolates fall under profile 69, except isolate09 and isolate10. This should match what they see in the phylogeny.\n\n\nAMR\nParticipants can upload their FASTA assemblies to Pathogenwatch, which implements its own AMR analysis method (link to section).\nIn addition, they can use the funcscan workflow (link to section). First we need to create the funcscan samplesheet with two columns: “sample” and “fasta”.\nThis can be done manually, but we provide some code here as a (slightly convoluted) way to do this with the command line. We do not expect you to share this with the participants, it’s here for trainers’ convenience, if you want to copy/paste to run the workflow quicker.\n# get sample names into temporary file - excluding isolate02 and isolate08\nbasename -a results/assemblebac/assemblies/*.fa | sed 's/_T1_contigs.fa//' | grep -v \"isolate02\\|isolate08\" &gt; temp_samples\n# get FASTA files into temporary file - excluding isolate02 and isolate08\nls results/assemblebac/assemblies/*.fa | grep -v \"isolate02\\|isolate08\" &gt; temp_fastas\n# create samplesheet header\necho \"sample,fasta\" &gt; samplesheet_funcscan.csv\n# paste the temporary files and append to samplesheet\npaste -d \",\" temp_samples temp_fastas &gt;&gt; samplesheet_funcscan.csv\n# remove temporary files\nrm temp_samples temp_fastas\nThe following can be used in a shell script to run the workflow:\nnextflow run nf-core/funcscan \\\n  -r \"2.1.0\" \\\n  -resume -profile \"singularity\" \\\n  --input \"samplesheet_funcscan.csv\" \\\n  --outdir \"results/funcscan\" \\\n  --run_arg_screening \\\n  --arg_rgi_db databases/card/ \\\n  --arg_skip_deeparg",
    "crumbs": [
      "Slides",
      "Outbreak!",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>OUTBREAK ALERT!</span>"
    ]
  },
  {
    "objectID": "materials/38-outbreak.html#outbreak-alert---trainer-solutions",
    "href": "materials/38-outbreak.html#outbreak-alert---trainer-solutions",
    "title": "OUTBREAK ALERT!",
    "section": "QC",
    "text": "Below we have quick solutions for the outbreak exercise. These are guidelines, the participants are welcome to explore other approaches.\nSome general notes:\n\nPreprocessed data: Some of these pipeline steps take quite a while to run (especially de-novo assembly). To avoid having the participants waiting for too long, there is a hidden folder with preprocessed results in outbreak/illumina/.preprocessed and outbreak/ont/.preprocessed. Participants can be told about this folder once their first pipeline is running successfully, so they can advance with the rest of the exercise.\nShell scripts: Encourage participants to write their commands in a shell script for reproducibility and documentation.\n\n\nIlluminaONT\n\n\n\nQC\nRun avantonder/bacQC (link to section).\nStart by creating the samplesheet using the helper script:\npython scripts/fastq_dir_to_samplesheet.py data/reads/ --read1_extension \"_1.fastq.gz\" --read2_extension \"_2.fastq.gz\" samplesheet.csv\nThen run bacQC, with command below. Participants will need to “guess” a genome size, as they won’t know what it is. The genome size is not critical for the analysis.\nnextflow run avantonder/bacQC \\\n  -r \"1.12.0\" \\\n  -resume -profile \"singularity\" \\\n  --input \"samplesheet.csv\" \\\n  --outdir \"results/bacqc\" \\\n  --kraken2db \"databases/k2_standard_08gb_20240605\" \\\n  --kronadb databases/krona/taxonomy.tab \\\n  --genome_size \"3200000\"\nFrom the QC participants should notice:\n\nIsolate02 has very few reads, which might lead to poor assemblies - they can choose whether to proceed with it or not\nThe Kraken barplot reveals that we are dealing with a Vibrio cholerae outbreak.\nHowever, isolate08 is Vibrio parahaemolyticus. The GC content was also slightly different for this isolate.\n\nagain, they can choose to proceed with this sample or not - however proceeding is nice to use as an outgroup for phylogeny later on.\n\n\n\n\nAssembly\nRun avantonder/assembleBAC (link to section):\nnextflow run avantonder/assembleBAC \\\n  -r \"v2.0.2\" \\\n  -resume -profile \"singularity\" \\\n  --input \"samplesheet.csv\" \\\n  --outdir \"results/assemblebac\" \\\n  --baktadb \"databases/bakta_light_20240119\" \\\n  --genome_size \"3.2M\" \\\n  --checkm2db \"databases/checkm2_v2_20210323/uniref100.KO.1.dmnd\"\n\n\n\n\n\n\nWarningPreprocessed data\n\n\n\nassembleBAC takes a long time to run (up to 1h). Make sure to point the participants to the .preprocessed hidden folder.\n\n\nFrom the assemblies participants should notice:\n\nMultiQC report:\n\nIf they proceeded with isolate02, they will notice very poor assembly, essentially unusable.\nOther samples are all comparable in quality, some more fragmented than others, but not massively different\n\nThe checkm2_report.tsv (in the report output folder):\n\nVery high assembly completeness for all samples (except isolate02).\nIsolate08 is again a bit different: slightly lower GC content (matches what was seen with bacQC) and higher predicted genome size at ~5Mb and higher number of predicted genes.\nAs further QC they can compare these numbers with the ones on reference databases, for example on KEGG: V. parahaemolyticus; V. cholerae\n\n\n\n\nCore genome alignemnt\nCreate a script to run Panaroo (link to section). This takes a while to run, but we provide preprocessed files if they want to proceed from there once their command is working and running.\nmamba activate panaroo\n\n# create output directory\nmkdir results/panaroo\n\n# ensure isolate02 is not being used - they might do this in a more \"manual\" way\ngffs=$(ls results/assemblebac/bakta/*.gff3 | grep -v \"isolate02\")\n\n# run panaroo\npanaroo \\\n  --input $gffs \\\n  --out_dir results/panaroo \\\n  --clean-mode strict \\\n  --alignment core \\\n  --core_threshold 0.98 \\\n  --remove-invalid-genes \\\n  --threads 8\n\n\n\n\n\n\nTipDiscussing use of an outgroup\n\n\n\nWe’re including V. parahaemolyticus to use as an outgroup in our phylogeny. However, it’s worth noting that by doing so we may be reducing the number of core genes in the alignment, as these species may be sufficiently diverged to alter the Panaroo clustering. One possibility would be to lower the --core_threshold to include a few more genes.\nThere is no clear “right” answer, but this could be a good discussion to have with the participants.\n\n\n\n\n\n\nQC\nRun avantonder/bacQC-ONT (takes ~20 minutes).\nStart by creating the samplesheet:\nsample,fastq\nsample1,data/fastq_pass/barcode01/ERR10146532.fastq.gz\nsample2,data/fastq_pass/barcode06/ERR10146521.fastq.gz\nsample3,data/fastq_pass/barcode02/ERR10146551.fastq.gz\nsample4,data/fastq_pass/barcode09/ERR10146531.fastq.gz\nsample5,data/fastq_pass/barcode05/ERR10146520.fastq.gz\nThen run bacQC-ONT, with the command below. Participants will need to “guess” a genome size, as they won’t know what it is. The genome size is not critical for the analysis.\nnextflow run avantonder/bacQC-ONT \\\n  -r \"v2.0.2\" \\\n  -profile singularity \\\n  --input samplesheet.csv \\\n  --summary_file sequencing_summary.txt \\\n  --genome_size 4000000 \\\n  --kraken2db databases/k2_standard_08gb_20240605/ \\\n  --kronadb databases/krona/taxonomy.tab \\\n  --outdir results/bacqc-ont\n\n\nAssembly\nRun avantonder/assembleBAC-ONT (takes ~2h - point participants to hidden .preprocessed folder):\nnextflow run avantonder/assembleBAC-ONT \\\n  -r \"v2.0.3\" \\\n  -profile singularity \\\n  --input samplesheet.csv \\\n  --genome_size 4M \\\n  --medaka_model r941_min_fast_g507 \\\n  --outdir results/assemblebac \\\n  --baktadb databases/bakta_light_20240119/ \\\n  --checkm2db databases/checkm2_v2_20210323/uniref100.KO.1.dmnd\n\n\n\n\n\n\nWarningPreprocessed data\n\n\n\nassembleBAC-ONT takes a long time to run (up to 2h). Make sure to point the participants to the .preprocessed hidden folder.\n\n\n\n\nCore genome alignemnt\nCreate a script to run Panaroo (link to section). This takes a while to run, but we provide preprocessed files if they want to proceed from there once their command is working and running.\nmamba activate panaroo\n\n# create output directory\nmkdir results/panaroo\n\n# run panaroo\npanaroo \\\n  --input results/assemblebac/bakta/*.gff3 \\\n  --out_dir results/panaroo \\\n  --clean-mode strict \\\n  --alignment core \\\n  --core_threshold 0.98 \\\n  --remove-invalid-genes \\\n  --threads 8",
    "crumbs": [
      "Slides",
      "Outbreak!",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>OUTBREAK ALERT!</span>"
    ]
  },
  {
    "objectID": "materials/38-outbreak.html#qc",
    "href": "materials/38-outbreak.html#qc",
    "title": "OUTBREAK ALERT!",
    "section": "",
    "text": "Run avantonder/bacQC (link to section).\nStart by creating the samplesheet using the helper script:\npython scripts/fastq_dir_to_samplesheet.py data/reads/ --read1_extension \"_1.fastq.gz\" --read2_extension \"_2.fastq.gz\" samplesheet.csv\nThen run bacQC, with command below. Participants will need to “guess” a genome size, as they won’t know what it is. The genome size is not critical for the analysis.\nnextflow run avantonder/bacQC \\\n  -r \"1.12.0\" \\\n  -resume -profile \"singularity\" \\\n  --input \"samplesheet.csv\" \\\n  --outdir \"results/bacqc\" \\\n  --kraken2db \"databases/k2_standard_08gb_20240605\" \\\n  --kronadb databases/krona/taxonomy.tab \\\n  --genome_size \"3200000\"\nFrom the QC participants should notice:\n\nIsolate02 has very few reads, which might lead to poor assemblies - they can choose whether to proceed with it or not\nThe Kraken barplot reveals that we are dealing with a Vibrio cholerae outbreak.\nHowever, isolate08 is Vibrio parahaemolyticus. The GC content was also slightly different for this isolate.\n\nagain, they can choose to proceed with this sample or not - however proceeding is nice to use as an outgroup for phylogeny later on.",
    "crumbs": [
      "Slides",
      "Outbreak!",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>OUTBREAK ALERT!</span>"
    ]
  },
  {
    "objectID": "materials/38-outbreak.html#assembly",
    "href": "materials/38-outbreak.html#assembly",
    "title": "OUTBREAK ALERT!",
    "section": "Assembly",
    "text": "Assembly\nRun avantonder/assembleBAC (link to section):\nnextflow run avantonder/assembleBAC \\\n  -r \"v2.0.2\" \\\n  -resume -profile \"singularity\" \\\n  --input \"samplesheet.csv\" \\\n  --outdir \"results/assemblebac\" \\\n  --baktadb \"databases/bakta_light_20240119\" \\\n  --genome_size \"3.2M\" \\\n  --checkm2db \"databases/checkm2_v2_20210323/uniref100.KO.1.dmnd\"\n\n\n\n\n\n\nWarningPreprocessed data\n\n\n\nassembleBAC takes a long time to run (up to 1h). Make sure to point the participants to the .preprocessed hidden folder.\n\n\nFrom the assemblies participants should notice:\n\nMultiQC report:\n\nIf they proceeded with isolate02, they will notice very poor assembly, essentially unusable.\nOther samples are all comparable in quality, some more fragmented than others, but not massively different\n\nThe checkm2_report.tsv (in the report output folder):\n\nVery high assembly completeness for all samples (except isolate02).\nIsolate08 is again a bit different: slightly lower GC content (matches what was seen with bacQC) and higher predicted genome size at ~5Mb and higher number of predicted genes.\nAs further QC they can compare these numbers with the ones on reference databases, for example on KEGG: V. parahaemolyticus; V. cholerae",
    "crumbs": [
      "Slides",
      "Outbreak!",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>OUTBREAK ALERT!</span>"
    ]
  },
  {
    "objectID": "materials/38-outbreak.html#core-genome-alignemnt",
    "href": "materials/38-outbreak.html#core-genome-alignemnt",
    "title": "OUTBREAK ALERT!",
    "section": "Core genome alignemnt",
    "text": "Core genome alignemnt\nCreate a script to run Panaroo (link to section). This takes a while to run, but we provide preprocessed files if they want to proceed from there once their command is working and running.\nmamba activate panaroo\n\n# create output directory\nmkdir results/panaroo\n\n# ensure isolate02 is not being used - they might do this in a more \"manual\" way\ngffs=$(ls results/assemblebac/bakta/*.gff3 | grep -v \"isolate02\")\n\n# run panaroo\npanaroo \\\n  --input $gffs \\\n  --out_dir results/panaroo \\\n  --clean-mode strict \\\n  --alignment core \\\n  --core_threshold 0.98 \\\n  --remove-invalid-genes \\\n  --threads 8\n\n\n\n\n\n\nTipDiscussing use of an outgroup\n\n\n\nWe’re including V. parahaemolyticus to use as an outgroup in our phylogeny. However, it’s worth noting that by doing so we may be reducing the number of core genes in the alignment, as these species may be sufficiently diverged to alter the Panaroo clustering. One possibility would be to lower the --core_threshold to include a few more genes.\nThere is no clear “right” answer, but this could be a good discussion to have with the participants.",
    "crumbs": [
      "Slides",
      "Outbreak!",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>OUTBREAK ALERT!</span>"
    ]
  },
  {
    "objectID": "materials/38-outbreak.html#qc-1",
    "href": "materials/38-outbreak.html#qc-1",
    "title": "OUTBREAK ALERT!",
    "section": "QC",
    "text": "QC\nRun avantonder/bacQC-ONT (takes ~20 minutes).\nStart by creating the samplesheet:\nsample,fastq\nsample1,data/fastq_pass/barcode01/ERR10146532.fastq.gz\nsample2,data/fastq_pass/barcode06/ERR10146521.fastq.gz\nsample3,data/fastq_pass/barcode02/ERR10146551.fastq.gz\nsample4,data/fastq_pass/barcode09/ERR10146531.fastq.gz\nsample5,data/fastq_pass/barcode05/ERR10146520.fastq.gz\nThen run bacQC-ONT, with the command below. Participants will need to “guess” a genome size, as they won’t know what it is. The genome size is not critical for the analysis.\nnextflow run avantonder/bacQC-ONT \\\n  -r \"v2.0.2\" \\\n  -profile singularity \\\n  --input samplesheet.csv \\\n  --summary_file sequencing_summary.txt \\\n  --genome_size 4000000 \\\n  --kraken2db databases/k2_standard_08gb_20240605/ \\\n  --kronadb databases/krona/taxonomy.tab \\\n  --outdir results/bacqc-ont",
    "crumbs": [
      "Slides",
      "Outbreak!",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>OUTBREAK ALERT!</span>"
    ]
  },
  {
    "objectID": "materials/38-outbreak.html#assembly-1",
    "href": "materials/38-outbreak.html#assembly-1",
    "title": "OUTBREAK ALERT!",
    "section": "Assembly",
    "text": "Assembly\nRun avantonder/assembleBAC-ONT (takes ~2h - point participants to hidden .preprocessed folder):\nnextflow run avantonder/assembleBAC-ONT \\\n  -r \"v2.0.3\" \\\n  -profile singularity \\\n  --input samplesheet.csv \\\n  --genome_size 4M \\\n  --medaka_model r941_min_fast_g507 \\\n  --outdir results/assemblebac \\\n  --baktadb databases/bakta_light_20240119/ \\\n  --checkm2db databases/checkm2_v2_20210323/uniref100.KO.1.dmnd\n\n\n\n\n\n\nWarningPreprocessed data\n\n\n\nassembleBAC-ONT takes a long time to run (up to 2h). Make sure to point the participants to the .preprocessed hidden folder.",
    "crumbs": [
      "Slides",
      "Outbreak!",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>OUTBREAK ALERT!</span>"
    ]
  },
  {
    "objectID": "materials/38-outbreak.html#core-genome-alignemnt-1",
    "href": "materials/38-outbreak.html#core-genome-alignemnt-1",
    "title": "OUTBREAK ALERT!",
    "section": "Core genome alignemnt",
    "text": "Core genome alignemnt\nCreate a script to run Panaroo (link to section). This takes a while to run, but we provide preprocessed files if they want to proceed from there once their command is working and running.\nmamba activate panaroo\n\n# create output directory\nmkdir results/panaroo\n\n# run panaroo\npanaroo \\\n  --input results/assemblebac/bakta/*.gff3 \\\n  --out_dir results/panaroo \\\n  --clean-mode strict \\\n  --alignment core \\\n  --core_threshold 0.98 \\\n  --remove-invalid-genes \\\n  --threads 8",
    "crumbs": [
      "Slides",
      "Outbreak!",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>OUTBREAK ALERT!</span>"
    ]
  },
  {
    "objectID": "materials/38-outbreak.html#phylogeny",
    "href": "materials/38-outbreak.html#phylogeny",
    "title": "OUTBREAK ALERT!",
    "section": "Phylogeny",
    "text": "Phylogeny\nOnce participants have the core alignment, they can build their trees by writing a script to run IQ-tree (link to section).\n# create output directory\nmkdir -p results/snp-sites/\nmkdir -p results/iqtree/\n\n# extract variable sites\nsnp-sites results/panaroo/core_gene_alignment_filtered.aln &gt; results/snp-sites/core_gene_alignment_snps.aln\n\n# count invariant sites\nsnp-sites -C results/panaroo/core_gene_alignment_filtered.aln &gt; results/snp-sites/constant_sites.txt\n\n# Run iqtree\niqtree \\\n  -fconst $(cat results/snp-sites/constant_sites.txt) \\\n  -s results/snp-sites/core_gene_alignment_snps.aln \\\n  --prefix results/iqtree/vibrio_outbreak \\\n  -nt AUTO \\\n  -ntmax 8 \\\n  -mem 8G \\\n  -m MFP \\\n  -bb 1000\nParticipants can visualise the phylogeny in Microreact, FigTree or R/ggtree - their choice. Encourage participants to include metadata from other analyses (e.g. MLST and AMR).",
    "crumbs": [
      "Slides",
      "Outbreak!",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>OUTBREAK ALERT!</span>"
    ]
  },
  {
    "objectID": "materials/38-outbreak.html#mlst",
    "href": "materials/38-outbreak.html#mlst",
    "title": "OUTBREAK ALERT!",
    "section": "MLST",
    "text": "MLST\nAlthough assembleBAC runs MLST, participants may opt to run this separately as well (link to section).\nThey can use the mlst --list command to see which schemes are available for Vibrio. There are actually two schemes available, named vcholerae and vcholerae_2. They can run both:\nmamba activate mlst\n\nmkdir results/mlst\n\n# exclude isolate02 and 08\nsamples=$(ls results/assemblebac/assemblies/*.fa | grep -v \"isolate02\\|isolate08\")\n\nmlst --scheme vcholerae $samples &gt; results/mlst/mlst_typing_scheme1.tsv\nmlst --scheme vcholerae_2 $samples &gt; results/mlst/mlst_typing_scheme2.tsv\nParticipants should notice that most isolates fall under profile 69, except isolate09 and isolate10. This should match what they see in the phylogeny.",
    "crumbs": [
      "Slides",
      "Outbreak!",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>OUTBREAK ALERT!</span>"
    ]
  },
  {
    "objectID": "materials/38-outbreak.html#amr",
    "href": "materials/38-outbreak.html#amr",
    "title": "OUTBREAK ALERT!",
    "section": "AMR",
    "text": "AMR\nParticipants can upload their FASTA assemblies to Pathogenwatch, which implements its own AMR analysis method (link to section).\nIn addition, they can use the funcscan workflow (link to section). First we need to create the funcscan samplesheet with two columns: “sample” and “fasta”.\nThis can be done manually, but we provide some code here as a (slightly convoluted) way to do this with the command line. We do not expect you to share this with the participants, it’s here for trainers’ convenience, if you want to copy/paste to run the workflow quicker.\n# get sample names into temporary file - excluding isolate02 and isolate08\nbasename -a results/assemblebac/assemblies/*.fa | sed 's/_T1_contigs.fa//' | grep -v \"isolate02\\|isolate08\" &gt; temp_samples\n# get FASTA files into temporary file - excluding isolate02 and isolate08\nls results/assemblebac/assemblies/*.fa | grep -v \"isolate02\\|isolate08\" &gt; temp_fastas\n# create samplesheet header\necho \"sample,fasta\" &gt; samplesheet_funcscan.csv\n# paste the temporary files and append to samplesheet\npaste -d \",\" temp_samples temp_fastas &gt;&gt; samplesheet_funcscan.csv\n# remove temporary files\nrm temp_samples temp_fastas\nThe following can be used in a shell script to run the workflow:\nnextflow run nf-core/funcscan \\\n  -r \"2.1.0\" \\\n  -resume -profile \"singularity\" \\\n  --input \"samplesheet_funcscan.csv\" \\\n  --outdir \"results/funcscan\" \\\n  --run_arg_screening \\\n  --arg_rgi_db databases/card/ \\\n  --arg_skip_deeparg",
    "crumbs": [
      "Slides",
      "Outbreak!",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>OUTBREAK ALERT!</span>"
    ]
  },
  {
    "objectID": "materials/39-group_exercise_2.html",
    "href": "materials/39-group_exercise_2.html",
    "title": "43  Communicating your findings",
    "section": "",
    "text": "43.1 Final presentation\nIn this final exercise, you will prepare a 5 minute presentation aimed at one of the following audiences:\nParticipants will be split into groups and assigned one of the audiences. Feel free to work with any of the four datasets we’ve worked with this week (including the outbreak). Your presentation should include a brief background/introduction, a methods section (remember the Minister of Health may not care which version of TB-Profiler you used!), your results and some interpretation.\nRemember that the presentation should be tailored to your audience.",
    "crumbs": [
      "Slides",
      "Reporting",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Communicating your findings</span>"
    ]
  },
  {
    "objectID": "materials/39-group_exercise_2.html#final-presentation",
    "href": "materials/39-group_exercise_2.html#final-presentation",
    "title": "43  Communicating your findings",
    "section": "",
    "text": "Field epidemiologist\nHead of a public health lab\nMinister of Health\nConcerned citizen",
    "crumbs": [
      "Slides",
      "Reporting",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Communicating your findings</span>"
    ]
  },
  {
    "objectID": "materials/appendices/01-file_formats.html",
    "href": "materials/appendices/01-file_formats.html",
    "title": "Appendix A — Common file formats",
    "section": "",
    "text": "This page lists some common file formats used in Bioinformatics (listed alphabetically). The heading of each file links to a page with more details about each format.\nGenerally, files can be classified into two categories: text files and binary files.\n\nText files can be opened with standard text editors, and manipulated using command-line tools (such as head, less, grep, cat, etc.). However, many of the standard files listed in this page can be opened with specific software that displays their content in a more user-friendly way. For example, the NEWICK format is used to store phylogenetic trees and, although it can be opened in a text editor, it is better used with a software such as FigTree to visualise the tree as a graph.\nBinary files are often used to store data more efficiently. Typically, specific tools need to be used with those files. For example, the BAM format is used to store sequences aligned to a reference genome and can be manipulated with dedicated software such as samtools.\n\nVery often, text files may be compressed to save storage space. A common compression format used in bioinformatics is gzip with has extension .gz. Many bioinformatic tools support compressed files. For example, FASTQ files (used to store NGS sequencing data) are often compressed with format .fq.gz.\n\nBAM (“Binary Alignment Map”)\n\nBinary file.\nSame as a SAM file but compressed in binary form.\nFile extensions: .bam\n\n\n\nBED (“Browser Extensible Data”)\n\nText file.\nStores coordinates of genomic regions.\nFile extension: .bed\n\n\n\nCSV (“Comma Separated Values”)\n\nText file.\nStores tabular data in a text file. (also see TSV format)\nFile extensions: .csv\n\nThese files can be opened with spreadsheet programs (such as Microsoft Excel). They can also be created from spreadsheet programs by going to File &gt; Save As… and select “CSV (Comma delimited)” as the file format.\n\n\nFAST5\n\nBinary file. More specifically, this is a Hierarchical Data Format (HDF5) file.\nUsed by Nanopore platforms to store the called sequences (in FASTQ format) as well as the raw electrical signal data from the pore.\nFile extensions: .fast5\n\n\n\nFASTA\n\nText file.\nStores nucleotide or amino acid sequences.\nFile extensions: .fa or .fas or .fasta\n\n\n\nFASTQ\n\nText file, but often compressed with gzip.\nStores sequences and their quality scores.\nFile extensions: .fq or .fastq (compressed as .fq.gz or .fastq.gz)\n\n\n\nGFF (“General Feature Format”)\n\nText file.\nStores gene coordinates and other features.\nFile extension: .gff\n\n\n\nNEWICK\n\nText file.\nStores phylogenetic trees including nodes names and edge lengths.\nFile extensions: .tree or .treefile\n\n\n\nSAM (“Sequence Alignment Map”)\n\nText file.\nStores sequences aligned to a reference genome. (also see BAM format)\nFile extensions: .sam\n\n\n\nTSV (“Tab-Separated Values”)\n\nText file.\nStores tabular data in a text file. (also see CSV format)\nFile extensions: .tsv or .txt\n\nThese files can be opened with spreadsheet programs (such as Microsoft Excel). They can also be created from spreadsheet programs by going to File &gt; Save As… and select “Text (Tab delimited)” as the file format.\n\n\nVCF (“Variant Calling Format”)\n\nText file but often compressed with gzip.\nStores SNP/Indel variants\nFile extension: .vcf (or compressed as .vcf.gz)",
    "crumbs": [
      "Slides",
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Common file formats</span>"
    ]
  },
  {
    "objectID": "materials/appendices/02-course_software.html",
    "href": "materials/appendices/02-course_software.html",
    "title": "Appendix B — Course Software",
    "section": "",
    "text": "This page lists the tools used during this course (listed alphabetically). The heading of each file links to a page with more details about each tool.\n\nBakta\nBakta is a software tool designed for the rapid and accurate annotation of bacterial genomes. It automates the process of identifying and categorizing genes and other functional elements within bacterial DNA sequences. otation\n\n\nBCFtools\nBCFtools is a set of utilities that manipulate variant calls in the Variant Call Format (VCF) and its binary counterpart, BCF. It is widely used for filtering, querying, merging, and annotating genomic variants, making it an essential tool in bioinformatics pipelines for genomic data analysis.\n\n\nBracken\nBracken (Bayesian Reestimation of Abundance with KrakEN) is a bioinformatics tool designed to improve the accuracy of species abundance estimation in metagenomic samples. It refines the results from Kraken by using Bayesian statistics to re-estimate the abundance of species, providing more precise and reliable microbial community profiles.\n\n\nBWA\nBWA (Burrows-Wheeler Aligner) is a fast and efficient software tool for aligning short DNA sequences to a reference genome. It supports various alignment algorithms, including BWA-MEM for high-quality alignments of sequences ranging from a few base pairs to hundreds of kilobases, making it a widely used tool in next-generation sequencing analysis.\n\n\nCheckM2\nCheckM2 is an advanced bioinformatics tool used for assessing the quality and completeness of microbial genomes and metagenome-assembled genomes (MAGs). It provides accurate estimates of genome completeness and contamination by leveraging a curated set of marker genes, which helps ensure the reliability of genomic data for downstream analyses.\n\n\nfastp\nfastp is a versatile and high-performance tool designed for preprocessing high-throughput sequencing data. It performs quality control, filtering, trimming, and adapter removal with a focus on speed and accuracy, making it an essential component of modern genomic data analysis pipelines.\n\n\nfastQC\nFastQC is a widely used bioinformatics tool that provides a comprehensive overview of the quality of high-throughput sequencing data. It generates detailed reports on various quality metrics, such as per-base sequence quality, GC content, and sequence duplication levels, helping researchers identify and address potential issues in their sequencing datasets.\n\n\nfastq-scan\nfastq-scan is a lightweight and efficient tool designed to quickly summarize basic quality metrics from FASTQ files. It provides essential statistics, such as read length distribution and GC content, without the computational overhead of more comprehensive tools, making it ideal for initial quality assessments of sequencing data.\n\n\nGubbins\nGubbins (Genealogies Unbiased By recomBinations In Nucleotide Sequences) is a bioinformatics tool used to detect and account for recombination in bacterial whole-genome sequence data. By identifying recombinant regions and reconstructing the underlying phylogeny, Gubbins helps ensure accurate evolutionary analyses and inferences.\n\n\nIQ-TREE\nIQ-TREE is a fast and efficient software tool for constructing phylogenetic trees based on maximum likelihood estimation. It supports a wide range of evolutionary models and provides advanced features such as ultrafast bootstrap approximation and model selection, making it a popular choice for high-quality phylogenetic analysis.\n\n\nKraken 2\nKraken 2 is a powerful bioinformatics tool used for taxonomic classification of sequencing reads from metagenomic or genomic datasets. It rapidly assigns taxonomic labels to reads based on k-mer matches to a user-defined reference database, facilitating the characterization of microbial communities and the detection of pathogens in complex samples.\n\n\nKrona\nKrona is a set of scripts to create Krona charts from several bioinformatics tools as well as from text and XML files.\n\n\nmash\nmash is a computational tool used for fast genome and metagenome distance estimation based on k-mer similarity. It enables rapid comparison of large genomic datasets by compressing sequences into sketch files, allowing efficient retrieval of pairwise distances between genomes or metagenomes.\n\n\nmlst\nmlst facilitates the analysis of Multi-Locus Sequence Typing data by automating allele calling and sequence comparison across bacterial isolates. It generates profiles that define the sequence types (STs) of each isolate based on allele variations in designated loci, crucial for studying bacterial population dynamics and genetic diversity.\n\n\nMultiQC\nMultiQC is a versatile tool used for aggregating and visualizing quality control metrics from multiple bioinformatics analyses in a single comprehensive report. It simplifies the assessment of data quality across diverse sequencing experiments and facilitates rapid identification of potential issues or trends in large datasets.\n\n\nNextflow\nNextflow is a data-driven workflow management system designed for scalable and reproducible scientific workflows. It simplifies the deployment and execution of complex computational pipelines across different computing environments, enhancing collaboration and facilitating the integration of diverse bioinformatics tools and data sources.\n\n\npairsnp\npairsnp is a bioinformatics tool used for comparing whole-genome sequences and identifying single nucleotide polymorphisms (SNPs) between pairs of genomes. It provides a rapid and efficient method for genome-wide SNP detection, aiding in the study of genetic variation and evolutionary relationships among bacterial or viral isolates.\n\n\nPanaroo\nPanaroo is a bioinformatics tool used for pan-genome analysis, which identifies core and accessory genes across multiple genomes of related organisms. It efficiently clusters and annotates genes to reveal the genomic diversity within a species or strain collection, aiding in understanding evolutionary dynamics and functional differences among microbial populations.\n\n\nQUAST\nQUAST (Quality Assessment Tool for Genome Assemblies) is a software tool used for evaluating the quality of genome assemblies. It provides comprehensive metrics and graphical reports to assess key aspects such as contiguity, accuracy, and completeness, aiding researchers in optimizing genome assembly strategies and comparing different assembly methods.\n\n\nRasusa\nRasusa is a bioinformatics tool used for simulating sequencing reads from reference genomes with specified sequencing error profiles and sequencing depths. It enables researchers to evaluate and benchmark bioinformatics pipelines by generating synthetic datasets that mimic real-world sequencing data characteristics.\n\n\nremove_blocks_from_aln.py\nremove_blocks_from_aln.py is a Python script used for removing blocks of sequences from a multiple sequence alignment (MSA) based on specified criteria such as sequence identity or gap content. It facilitates the preprocessing of alignments to focus on conserved regions or to remove poorly aligned or ambiguous sequences, improving downstream phylogenetic or evolutionary analyses.\n\n\nSAMtools\nSAMtools is a widely used software suite for manipulating sequencing alignment files in the SAM/BAM format. It provides essential functions such as file format conversion, sorting, indexing, and variant calling, making it indispensable in genomics research and variant analysis workflows.\n\n\nseqtk\nseqtk is a versatile toolkit for processing sequences in FASTA and FASTQ formats. It includes tools for subsetting sequences, extracting specific regions, filtering by quality, and converting between different sequence formats, facilitating various bioinformatics analyses and preprocessing tasks.\n\n\nShovill\nShovill is a bioinformatics tool designed for fast and accurate de novo assembly of microbial genomes from Illumina sequencing data. It automates the assembly process, including read trimming, error correction, and scaffolding, providing comprehensive genome assemblies suitable for downstream genomic analysis and comparative genomics studies.\n\n\nSNP-sites\nSNP-sites is a software tool used for identifying and extracting variable sites (SNPs) from a multiple sequence alignment (MSA). It efficiently detects polymorphic positions across aligned sequences, essential for population genetics, phylogenetics, and evolutionary studies based on genomic data.\n\n\nTB-Profiler\nTB-Profiler is a bioinformatics tool used for analyzing Mycobacterium tuberculosis genomes to detect mutations associated with drug resistance and lineage classification. It provides a comprehensive analysis of resistance profiles and strain lineages based on genomic data, aiding in clinical diagnostics and epidemiological surveillance of tuberculosis.\n\n\nTreeTime\nTreeTime is a tool used for molecular clock phylogenetic analysis, which estimates the evolutionary timescale of genetic sequences by integrating sequence data with a phylogenetic tree. It performs ancestral reconstruction and divergence dating, providing insights into the timing of evolutionary events within microbial populations or viral lineages.",
    "crumbs": [
      "Slides",
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Course Software</span>"
    ]
  }
]